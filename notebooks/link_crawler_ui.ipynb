{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"wfHzWjmH3vM_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# pip install -r requirements.txt first"],"metadata":{"id":"sknmac1N3v_S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8LvLA27rcjJ"},"outputs":[],"source":["# File: notebooks/link_crawler_ui.ipynb - Cell 1\n","\n","# Installing necessary packages\n","# !pip install gradio pandas requests tqdm beautifulsoup4 lxml -q\n","# !pip install fireducks\n","\n","import sys\n","import os\n","import warnings\n","\n","# Suppress common warnings if desired\n","warnings.filterwarnings(\n","    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",")\n","\n","# Add the project root to the Python path\n","project_root = \"/content/drive/My Drive/WebKnoGraph\" # Explicitly set\n","\n","if project_root not in sys.path:\n","    sys.path.insert(0, project_root)\n","\n","print(f\"Project root added to sys.path: {project_root}\")\n","print(f\"Current working directory: {os.getcwd()}\")\n","print(f\"sys.path: {sys.path}\")\n","\n","# Google Colab Drive Mount\n","try:\n","    from google.colab import drive\n","    if not os.path.exists(\"/content/drive/My Drive\"):\n","        drive.mount(\"/content/drive/\")\n","        print(\"Google Drive mounted successfully.\")\n","    else:\n","        print(\"Google Drive already mounted.\")\n","except ImportError:\n","    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n","except Exception as e:\n","    print(f\"Error mounting Google Drive: {e}\")\n","\n","\n","# Import from your refactored backend and shared modules\n","import gradio as gr\n","import io\n","import fireducks.pandas as pd # Using fireducks.pandas as specified\n","from urllib.parse import urlparse\n","\n","# Specific imports for the Link Graph Extractor\n","from src.backend.config.link_crawler_config import LinkCrawlerConfig # Using the new config\n","from src.backend.data.link_graph_repository import LinkGraphStateManager # Using the new state manager\n","from src.backend.utils.strategies import VisitedUrlManager, BFSCrawlingStrategy, DFSCrawlingStrategy # Reusing strategies\n","from src.backend.utils.http import HttpClient # Reusing HttpClient\n","from src.backend.utils.link_url import LinkUrlFilter, LinkExtractorForEdges # Using the new URL components\n","from src.backend.services.link_crawler_service import EdgeCrawler # Using the new orchestrator\n","from src.shared.logging_config import ConsoleAndGradioLogger # Using the generic logger\n","\n","print(\"All modules imported successfully!\")"]},{"cell_type":"code","source":["# File: notebooks/link_crawler_ui.ipynb - Cell 2\n","\n","def run_edge_crawler_interface(initial_start_url: str, crawling_scope_path: str, saving_scope_path: str,\n","                               crawling_strategy_type: str, state_db_path_input: str,\n","                               edge_list_path_input: str, max_pages_to_crawl: int):\n","    log_stream = io.StringIO()\n","    logger = ConsoleAndGradioLogger(log_stream, logger_name=\"EdgeCrawlerLogger\") # Pass logger_name\n","\n","    try:\n","        base_domain = urlparse(initial_start_url).netloc\n","        if not base_domain: raise ValueError(\"Invalid Initial Start URL.\")\n","\n","        config = LinkCrawlerConfig( # Using new config class\n","            initial_start_url=initial_start_url,\n","            crawling_scope_path=crawling_scope_path,\n","            saving_scope_path=saving_scope_path,\n","            state_db_path=state_db_path_input,\n","            edge_list_path=edge_list_path_input,\n","            max_pages_to_crawl=max_pages_to_crawl,\n","            base_domain=base_domain\n","        )\n","        os.makedirs(os.path.dirname(config.edge_list_path), exist_ok=True) # Ensure output dir exists\n","        yield \"Initializing...\", log_stream.getvalue(), \"\"\n","\n","        state_manager = LinkGraphStateManager(config.state_db_path, config.edge_list_path, logger) # Using new state manager\n","        visited_manager = VisitedUrlManager()\n","\n","        logger.info(\"Rebuilding visited set from existing edge list CSV...\")\n","        # Load visited URLs from CSV using the state manager\n","        rebuilt_visited_urls = state_manager.load_visited_from_edges()\n","        for url in rebuilt_visited_urls:\n","            visited_manager.add(url)\n","        if rebuilt_visited_urls:\n","            logger.info(f\"Rebuilt visited set with {visited_manager.size()} URLs from CSV.\")\n","        else:\n","            logger.warning(\"No previously processed edges found. Starting with a fresh visited set.\")\n","\n","\n","        if crawling_strategy_type == 'BFS':\n","            crawling_strategy = BFSCrawlingStrategy(visited_manager, logger) # Reusing BFSCrawlingStrategy\n","        else:\n","            crawling_strategy = DFSCrawlingStrategy(visited_manager, logger) # Reusing DFSCrawlingStrategy\n","\n","        loaded_frontier = state_manager.load_frontier()\n","        unvisited_frontier = [info for info in loaded_frontier if not visited_manager.contains(info[0])]\n","\n","        if unvisited_frontier:\n","            crawling_strategy.prime_with_frontier(unvisited_frontier)\n","            logger.info(f\"Primed frontier with {len(unvisited_frontier)} URLs from DB.\")\n","        elif not visited_manager.contains(config.initial_start_url):\n","            crawling_strategy.add_links([(config.initial_start_url, 0)])\n","            logger.info(f\"Added initial URL {config.initial_start_url} to frontier.\")\n","        else:\n","            logger.info(f\"Initial URL {config.initial_start_url} already visited. No new URLs to start with from DB or initial URL.\")\n","\n","\n","        url_filter = LinkUrlFilter(config.crawling_scope_path, config.base_domain) # Using new LinkUrlFilter\n","        link_extractor = LinkExtractorForEdges(url_filter) # Using new LinkExtractorForEdges\n","        # HttpClient expects a config object that has 'user_agents', 'max_retries_request', 'request_timeout', 'min_request_delay' attributes. LinkCrawlerConfig has these.\n","        crawler = EdgeCrawler(config, crawling_strategy, state_manager, HttpClient(config, logger), link_extractor, logger) # Using new EdgeCrawler\n","\n","        final_status = \"\"\n","        for status_msg in crawler.crawl():\n","            final_status = status_msg\n","            yield status_msg, log_stream.getvalue(), \"\"\n","\n","        logger.info(\"Generating final summary from CSV file...\")\n","        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Edge List Location**: `{config.edge_list_path}`\"\n","        try:\n","            if os.path.exists(config.edge_list_path):\n","                edge_df = pd.read_csv(config.edge_list_path)\n","                num_edges = len(edge_df)\n","                num_nodes = len(pd.concat([edge_df['FROM'], edge_df['TO']]).unique())\n","                summary_md += f\"\\n- **Total Unique Pages (Nodes):** {num_nodes}\\n- **Total Links (Edges):** {num_edges}\"\n","        except Exception as e:\n","            logger.error(f\"Could not generate summary from CSV: {e}\")\n","            summary_md += \"\\n\\n**Could not generate summary from CSV file.**\"\n","\n","        yield final_status, log_stream.getvalue(), summary_md\n","\n","    except Exception as e:\n","        logger.exception(f\"A critical error occurred: {e}\")\n","        yield \"Crawl Failed!\", log_stream.getvalue(), f\"**Error:** {e}\""],"metadata":{"id":"O1gRjU7hrfMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File: notebooks/link_crawler_ui.ipynb - Cell 3\n","\n","with gr.Blocks(theme=gr.themes.Soft()) as demo:\n","    gr.Markdown(\"# üï∏Ô∏è Link Graph Extractor\")\n","    gr.Markdown(\"This tool crawls a website to produce a simple `FROM, TO` list of all hyperlinks, saved as a CSV file.\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=1):\n","            gr.Markdown(\"## Configuration\")\n","            initial_url_input = gr.Textbox(label=\"Initial Start URL\", value=LinkCrawlerConfig.initial_start_url) # Uses LinkCrawlerConfig\n","            max_pages_input = gr.Number(label=\"Maximum Pages to Process\", value=LinkCrawlerConfig.max_pages_to_crawl, minimum=1, step=100) # Uses LinkCrawlerConfig\n","            crawling_strategy_radio = gr.Radio(choices=['BFS', 'DFS'], label=\"Crawling Strategy\", value='BFS')\n","\n","            gr.Markdown(\"### üìú Scopes\")\n","            crawling_scope_path_input = gr.Textbox(label=\"Crawling Scope Path\", value=LinkCrawlerConfig.crawling_scope_path, info=\"The 'playground'. Set to '/' to explore the entire site.\") # Uses LinkCrawlerConfig\n","            saving_scope_path_input = gr.Textbox(label=\"Saving Scope Path\", value=LinkCrawlerConfig.saving_scope_path, info=\"The 'rulebook'. Only save links where FROM and TO are in this path.\") # Uses LinkCrawlerConfig\n","\n","            gr.Markdown(\"### üíæ Storage Paths\")\n","            state_db_path_input = gr.Textbox(label=\"Crawl State DB Path (SQLite)\", value=LinkCrawlerConfig.state_db_path) # Uses LinkCrawlerConfig\n","            edge_list_path_input = gr.Textbox(label=\"Output Edge List Path (CSV)\", value=LinkCrawlerConfig.edge_list_path) # Uses LinkCrawlerConfig\n","\n","            start_button = gr.Button(\"üöÄ Start Extraction\", variant=\"primary\")\n","\n","        with gr.Column(scale=2):\n","            gr.Markdown(\"## Status & Results\")\n","            status_message_output = gr.Textbox(label=\"Status Message\", interactive=False)\n","            logs_output = gr.Textbox(label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20)\n","            summary_output = gr.Markdown(\"---\")\n","\n","    start_button.click(\n","        fn=run_edge_crawler_interface,\n","        inputs=[\n","            initial_url_input,\n","            crawling_scope_path_input,\n","            saving_scope_path_input,\n","            crawling_strategy_radio,\n","            state_db_path_input,\n","            edge_list_path_input,\n","            max_pages_input\n","        ],\n","        outputs=[status_message_output, logs_output, summary_output]\n","    )"],"metadata":{"id":"nISLHL5urhAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File: notebooks/link_crawler_ui.ipynb - Cell 4\n","\n","# --- Launch the Application ---\n","if __name__ == '__main__':\n","    try:\n","        from google.colab import drive\n","        # It's better to mount once at the very start of the notebook\n","        # or main.py. If it's already mounted, no need to force_remount unless necessary.\n","        # Check if already mounted before attempting to mount again (as in Cell 1)\n","        if not os.path.exists(\"/content/drive/My Drive\"):\n","            drive.mount(\"/content/drive/\")\n","            print(\"Google Drive mounted successfully.\")\n","        else:\n","            print(\"Google Drive already mounted.\")\n","\n","        demo.launch(debug=True, share=True)\n","    except Exception as e:\n","        print(f\"Could not launch Gradio demo in this environment: {e}\")"],"metadata":{"id":"ZYIfh5m64yYU"},"execution_count":null,"outputs":[]}]}