{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Always pefrorm Runtime -> Restart session"
      ],
      "metadata": {
        "id": "RydrBsWi3GyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary packages\n",
        "!pip install pandas pyarrow duckdb gradio requests beautifulsoup4 lxml tqdm\n",
        "from urllib.parse import urlparse, parse_qs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-ts3S8AJc2E",
        "outputId": "59a9ee5a-1890-4da0-ab80-19129bffdd66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#\n",
        "# --- Imports and Setup ---\n",
        "#\n",
        "#\n",
        "\n",
        "import gradio as gr\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "import duckdb\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse, urljoin, parse_qs\n",
        "from collections import deque\n",
        "from abc import ABC, abstractmethod\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Google Colab Drive Mount ---\n",
        "#\n",
        "#\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Configuration ---\n",
        "#\n",
        "#\n",
        "\n",
        "@dataclass\n",
        "class CrawlerConfig:\n",
        "    state_db_path: str = \"/content/drive/My Drive/master_july_2025/data/crawler_state.db\"\n",
        "    parquet_path: str = \"/content/drive/My Drive/master_july_2025/data/crawled_data_parquet/\"\n",
        "    min_request_delay: float = 1.0\n",
        "    max_request_delay: float = 30.0\n",
        "    max_pages_to_crawl: int = 700\n",
        "    save_interval_pages: int = 10\n",
        "    max_retries_request: int = 3\n",
        "    max_redirects: int = 2\n",
        "    request_timeout: int = 15\n",
        "    allowed_path_segment: str = \"/section\"\n",
        "    initial_start_url: str = 'https://example-url.com/section'\n",
        "    user_agents: list[str] = field(default_factory=lambda: [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n",
        "    ])\n",
        "    base_domain: str = \"\"\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Logging and Core Interfaces ---\n",
        "#\n",
        "#\n",
        "class ILogger(ABC):\n",
        "    @abstractmethod\n",
        "    def debug(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def info(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def warning(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def error(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def exception(self, message: str): pass\n",
        "\n",
        "class GradioLogHandler(logging.Handler):\n",
        "    def __init__(self, log_output_stream: io.StringIO):\n",
        "        super().__init__()\n",
        "        self.log_output_stream = log_output_stream\n",
        "        self.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "\n",
        "    def emit(self, record):\n",
        "        log_entry = self.format(record)\n",
        "        self.log_output_stream.write(log_entry + '\\n')\n",
        "        self.log_output_stream.flush()\n",
        "\n",
        "class ConsoleAndGradioLogger(ILogger):\n",
        "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
        "        self._logger = logging.getLogger(\"CrawlerLogger\")\n",
        "        self._logger.setLevel(level)\n",
        "        if self._logger.hasHandlers():\n",
        "            self._logger.handlers.clear()\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        self._logger.addHandler(console_handler)\n",
        "        gradio_handler = GradioLogHandler(log_output_stream)\n",
        "        self._logger.addHandler(gradio_handler)\n",
        "\n",
        "    def debug(self, message: str): self._logger.debug(message)\n",
        "    def info(self, message: str): self._logger.info(message)\n",
        "    def warning(self, message: str): self._logger.warning(message)\n",
        "    def error(self, message: str): self._logger.error(message)\n",
        "    def exception(self, message: str): self._logger.exception(message)\n",
        "\n",
        "class VisitedUrlManager:\n",
        "    def __init__(self): self.visited = set()\n",
        "    def add(self, url: str): self.visited.add(url)\n",
        "    def contains(self, url: str) -> bool: return url in self.visited\n",
        "    def size(self) -> int: return len(self.visited)\n",
        "\n",
        "class CrawlingStrategy(ABC):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        self.visited = visited_manager\n",
        "        self.logger = logger\n",
        "    @abstractmethod\n",
        "    def add_links(self, links_info: list[tuple[str, int]]): pass\n",
        "    @abstractmethod\n",
        "    def get_next(self) -> tuple[str, int]: pass\n",
        "    @abstractmethod\n",
        "    def has_next(self) -> bool: pass\n",
        "    @abstractmethod\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): pass\n",
        "\n",
        "class BFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        super().__init__(visited_manager, logger)\n",
        "        self.queue = deque()\n",
        "    def add_links(self, links_info: list[tuple[str, int]]):\n",
        "        new_links = [link_info for link_info in links_info if not self.visited.contains(link_info[0])]\n",
        "        for link_url, _ in new_links: self.visited.add(link_url)\n",
        "        self.queue.extend(new_links)\n",
        "    def get_next(self) -> tuple[str, int]: return self.queue.popleft()\n",
        "    def has_next(self) -> bool: return len(self.queue) > 0\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): self.queue.extend(frontier_urls_info)\n",
        "\n",
        "class DFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        super().__init__(visited_manager, logger)\n",
        "        self.stack = []\n",
        "    def add_links(self, links_info: list[tuple[str, int]]):\n",
        "        new_links = [link_info for link_info in links_info if not self.visited.contains(link_info[0])]\n",
        "        for link_url, _ in new_links: self.visited.add(link_url)\n",
        "        self.stack.extend(new_links)\n",
        "    def get_next(self) -> tuple[str, int]: return self.stack.pop()\n",
        "    def has_next(self) -> bool: return len(self.stack) > 0\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): self.stack.extend(frontier_urls_info)\n",
        "\n",
        "\n",
        "class StateManager:\n",
        "    def __init__(self, db_path: str, logger: ILogger):\n",
        "        self.db_path = db_path\n",
        "        self.logger = logger\n",
        "        db_dir = os.path.dirname(self.db_path)\n",
        "        if db_dir: os.makedirs(db_dir, exist_ok=True)\n",
        "        self.ensure_frontier_table_exists()\n",
        "\n",
        "    def _execute_query(self, query: str, params=None, fetch=False):\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(query, params or [])\n",
        "                if fetch: return cursor.fetchall()\n",
        "                conn.commit()\n",
        "        except sqlite3.Error as e:\n",
        "            self.logger.error(f\"StateManager DB error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def ensure_frontier_table_exists(self):\n",
        "        self._execute_query(\"CREATE TABLE IF NOT EXISTS crawl_frontier (URL TEXT UNIQUE, Redirects INTEGER)\")\n",
        "        self.logger.info(\"Crawl frontier table ensured to exist.\")\n",
        "\n",
        "    def save_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
        "        self._execute_query(\"DELETE FROM crawl_frontier\")\n",
        "        if not frontier_urls_info: return\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                conn.executemany(\"INSERT OR IGNORE INTO crawl_frontier (URL, Redirects) VALUES (?, ?)\", frontier_urls_info)\n",
        "                conn.commit()\n",
        "            self.logger.info(f\"Saved {len(frontier_urls_info)} URLs to frontier.\")\n",
        "        except sqlite3.Error as e:\n",
        "            self.logger.error(f\"Error saving frontier: {e}\")\n",
        "\n",
        "    def load_frontier(self) -> list[tuple[str, int]]:\n",
        "        result = self._execute_query(\"SELECT URL, Redirects FROM crawl_frontier\", fetch=True)\n",
        "        return result or []\n",
        "\n",
        "class HttpClient:\n",
        "    def __init__(self, config: CrawlerConfig, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.session = self._create_requests_session()\n",
        "        self.current_delay = self.config.min_request_delay\n",
        "\n",
        "    def _create_requests_session(self):\n",
        "        retry = Retry(total=self.config.max_retries_request, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        session = requests.Session()\n",
        "        session.mount(\"http://\", adapter)\n",
        "        session.mount(\"https://\", adapter)\n",
        "        return session\n",
        "\n",
        "    def fetch(self, url: str) -> tuple[int, str, str | None]:\n",
        "        headers = {'User-Agent': random.choice(self.config.user_agents)}\n",
        "        try:\n",
        "            time.sleep(self.current_delay)\n",
        "            response = self.session.get(url, headers=headers, timeout=self.config.request_timeout, allow_redirects=False)\n",
        "            if 300 <= response.status_code < 400:\n",
        "                redirect_url = response.headers.get('Location')\n",
        "                return response.status_code, \"\", urljoin(url, redirect_url)\n",
        "            return response.status_code, response.text, None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.logger.error(f\"Request error for {url}: {e}\")\n",
        "            return -2, f\"Request Error: {e}\", None\n",
        "\n",
        "class UrlFilter:\n",
        "    def __init__(self, allowed_path_segment: str, base_domain: str):\n",
        "        self.allowed_path_segment = allowed_path_segment\n",
        "        self.base_domain = base_domain\n",
        "        self.file_extension_pattern = re.compile(r'\\.(jpg|jpeg|png|gif|pdf|doc|xls|zip|rar|mp3|mp4)$', re.I)\n",
        "\n",
        "    def is_valid(self, url: str) -> bool:\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            return (parsed_url.scheme in ('http', 'https') and\n",
        "                    parsed_url.netloc == self.base_domain and\n",
        "                    self.allowed_path_segment in parsed_url.path and\n",
        "                    not self.file_extension_pattern.search(parsed_url.path))\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "class LinkExtractor:\n",
        "    def __init__(self, url_filter: UrlFilter):\n",
        "        self.url_filter = url_filter\n",
        "\n",
        "    def extract_links(self, base_url: str, html_content: str) -> list[str]:\n",
        "        links = set()\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            absolute_link = urljoin(base_url, href)\n",
        "            parsed_link = urlparse(absolute_link)\n",
        "            normalized_link = parsed_link._replace(fragment=\"\").geturl()\n",
        "            if self.url_filter.is_valid(normalized_link):\n",
        "                links.add(normalized_link)\n",
        "        return list(links)\n",
        "\n",
        "class WebCrawler:\n",
        "    def __init__(self, config: CrawlerConfig, crawling_strategy: CrawlingStrategy, state_manager: StateManager,\n",
        "                 http_client: HttpClient, url_filter: UrlFilter, link_extractor: LinkExtractor, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.crawling_strategy = crawling_strategy\n",
        "        self.state_manager = state_manager\n",
        "        self.http_client = http_client\n",
        "        self.url_filter = url_filter\n",
        "        self.link_extractor = link_extractor\n",
        "        self.logger = logger\n",
        "        self.data_buffer = []\n",
        "        self.pages_crawled_session = 0\n",
        "\n",
        "    def _process_url(self, url_info: tuple[str, int]):\n",
        "        url, num_redirects = url_info\n",
        "        if num_redirects >= self.config.max_redirects:\n",
        "            self.data_buffer.append({'URL': url, 'Status_Code': 999, 'Content': \"Max redirects reached\"})\n",
        "            return\n",
        "\n",
        "        status, content, redirect_url = self.http_client.fetch(url)\n",
        "        self.logger.info(f\"Fetched {url} [{status}]\")\n",
        "        self.data_buffer.append({'URL': url, 'Status_Code': status, 'Content': content if 200 <= status < 300 else \"\"})\n",
        "\n",
        "        if 200 <= status < 300:\n",
        "            extracted = self.link_extractor.extract_links(url, content)\n",
        "            self.crawling_strategy.add_links([(link, 0) for link in extracted])\n",
        "            del content\n",
        "        elif redirect_url and self.url_filter.is_valid(redirect_url):\n",
        "            self.crawling_strategy.add_links([(redirect_url, num_redirects + 1)])\n",
        "\n",
        "    def _save_buffer_to_parquet(self) -> str | None:\n",
        "        if not self.data_buffer:\n",
        "            return None\n",
        "\n",
        "        num_records = len(self.data_buffer)\n",
        "        df = pd.DataFrame(self.data_buffer)\n",
        "        today = datetime.now().date()\n",
        "        df['crawl_date'] = today\n",
        "\n",
        "        try:\n",
        "            partition_path = os.path.join(self.config.parquet_path, f'crawl_date={today}')\n",
        "            df.to_parquet(path=self.config.parquet_path, engine='pyarrow', compression='snappy', partition_cols=['crawl_date'])\n",
        "            log_message = f\"âœ… Saved a batch of **{num_records}** pages to partition `{partition_path}`\"\n",
        "            self.logger.info(log_message)\n",
        "            self.data_buffer = []\n",
        "            gc.collect()\n",
        "            return log_message\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to save to Parquet: {e}\")\n",
        "            return None\n",
        "\n",
        "    def crawl(self):\n",
        "        pbar = tqdm(total=self.config.max_pages_to_crawl, desc=\"Crawling Progress\")\n",
        "        while self.pages_crawled_session < self.config.max_pages_to_crawl:\n",
        "            if not self.crawling_strategy.has_next():\n",
        "                self.logger.info(\"Frontier is empty. Stopping crawl.\")\n",
        "                break\n",
        "\n",
        "            url_data = self.crawling_strategy.get_next()\n",
        "            self._process_url(url_data)\n",
        "            self.pages_crawled_session += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            save_event_msg = None\n",
        "            if len(self.data_buffer) >= self.config.save_interval_pages:\n",
        "                save_event_msg = self._save_buffer_to_parquet()\n",
        "                if hasattr(self.crawling_strategy, 'queue'):\n",
        "                    self.state_manager.save_frontier(list(self.crawling_strategy.queue))\n",
        "                elif hasattr(self.crawling_strategy, 'stack'):\n",
        "                    self.state_manager.save_frontier(list(self.crawling_strategy.stack))\n",
        "\n",
        "            # THE FIX: Yield a single dictionary, which is robust.\n",
        "            yield {\n",
        "                \"status\": f\"Crawled {self.pages_crawled_session}/{self.config.max_pages_to_crawl} pages.\",\n",
        "                \"save_event\": save_event_msg\n",
        "            }\n",
        "\n",
        "        pbar.close()\n",
        "        self.logger.info(\"Crawl finished. Performing final save...\")\n",
        "        final_save_msg = self._save_buffer_to_parquet()\n",
        "        if hasattr(self.crawling_strategy, 'queue'):\n",
        "            self.state_manager.save_frontier(list(self.crawling_strategy.queue))\n",
        "        elif hasattr(self.crawling_strategy, 'stack'):\n",
        "            self.state_manager.save_frontier(list(self.crawling_strategy.stack))\n",
        "\n",
        "        # THE FIX: Also yield a dictionary here.\n",
        "        yield {\n",
        "            \"status\": f\"Crawl finished. Processed {self.pages_crawled_session} pages.\",\n",
        "            \"save_event\": final_save_msg\n",
        "        }\n",
        "\n",
        "\n",
        "def run_gradio_crawler_interface(initial_start_url: str, allowed_path_segment: str, crawling_strategy_type: str,\n",
        "                                 state_db_path_input: str, parquet_path_input: str, max_pages_to_crawl: int):\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(log_stream)\n",
        "\n",
        "    try:\n",
        "        base_domain = urlparse(initial_start_url).netloc\n",
        "        if not base_domain: raise ValueError(\"Invalid Initial Start URL.\")\n",
        "\n",
        "        config = CrawlerConfig(\n",
        "            initial_start_url=initial_start_url, allowed_path_segment=allowed_path_segment,\n",
        "            state_db_path=state_db_path_input, parquet_path=parquet_path_input,\n",
        "            max_pages_to_crawl=max_pages_to_crawl, base_domain=base_domain\n",
        "        )\n",
        "        os.makedirs(config.parquet_path, exist_ok=True)\n",
        "        yield \"Initializing...\", log_stream.getvalue(), \"### Save Events Log\\n\\n- Waiting for first save event...\", \"\"\n",
        "\n",
        "        state_manager = StateManager(config.state_db_path, logger)\n",
        "        visited_manager = VisitedUrlManager()\n",
        "\n",
        "        logger.info(\"Rebuilding visited set from existing Parquet data...\")\n",
        "        try:\n",
        "            parquet_glob_path = os.path.join(config.parquet_path, '**', '*.parquet')\n",
        "            visited_urls_df = duckdb.query(f\"SELECT DISTINCT URL FROM read_parquet('{parquet_glob_path}')\").to_df()\n",
        "            for url in visited_urls_df['URL']:\n",
        "                visited_manager.add(url)\n",
        "            logger.info(f\"Rebuilt visited set with {visited_manager.size()} URLs.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not rebuild visited set from Parquet (may be a new crawl): {e}\")\n",
        "\n",
        "        strategy_class = BFSCrawlingStrategy if crawling_strategy_type == 'BFS' else DFSCrawlingStrategy\n",
        "        crawling_strategy = strategy_class(visited_manager, logger)\n",
        "\n",
        "        loaded_frontier = state_manager.load_frontier()\n",
        "        unvisited_frontier = [info for info in loaded_frontier if not visited_manager.contains(info[0])]\n",
        "\n",
        "        if unvisited_frontier:\n",
        "            crawling_strategy.prime_with_frontier(unvisited_frontier)\n",
        "        elif not visited_manager.contains(config.initial_start_url):\n",
        "             crawling_strategy.add_links([(config.initial_start_url, 0)])\n",
        "\n",
        "        http_client = HttpClient(config, logger)\n",
        "        url_filter = UrlFilter(config.allowed_path_segment, config.base_domain)\n",
        "        link_extractor = LinkExtractor(url_filter)\n",
        "        crawler = WebCrawler(config, crawling_strategy, state_manager, http_client, url_filter, link_extractor, logger)\n",
        "\n",
        "        final_status = \"\"\n",
        "        save_events_log = [\"### Save Events Log\"]\n",
        "\n",
        "        # THE FIX: Loop receives a single 'event' dictionary.\n",
        "        for event in crawler.crawl():\n",
        "            # Safely get values from the dictionary.\n",
        "            status_msg = event.get(\"status\")\n",
        "            save_event = event.get(\"save_event\")\n",
        "\n",
        "            final_status = status_msg\n",
        "            if save_event:\n",
        "                save_events_log.append(f\"- {save_event}\")\n",
        "            yield status_msg, log_stream.getvalue(), \"\\n\".join(save_events_log), \"\"\n",
        "\n",
        "        logger.info(\"Generating final summary from Parquet data...\")\n",
        "        final_save_events = \"\\n\".join(save_events_log)\n",
        "        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Crawled Data Location**: `{config.parquet_path}`\"\n",
        "        try:\n",
        "            parquet_glob_path = os.path.join(config.parquet_path, '**', '*.parquet')\n",
        "            summary_df = duckdb.query(f\"SELECT CASE WHEN Status_Code >= 200 AND Status_Code < 300 THEN 'Success (Content Saved)' WHEN Status_Code >= 300 AND Status_Code < 400 THEN 'Redirect' ELSE 'Error / Other' END AS Category, COUNT(*) as Total FROM read_parquet('{parquet_glob_path}') GROUP BY Category ORDER BY Total DESC\").to_df()\n",
        "            total_urls = summary_df['Total'].sum()\n",
        "            summary_md += f\"\\n- **Total URLs in Parquet Dataset**: {total_urls}\\n\\n### Crawl Summary by Category\\n\\n\"\n",
        "            summary_md += summary_df.to_markdown(index=False)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Could not generate summary from Parquet: {e}\")\n",
        "            summary_md += \"\\n\\n**Could not generate summary from Parquet data.**\"\n",
        "\n",
        "        yield final_status, log_stream.getvalue(), final_save_events, summary_md\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"A critical error occurred: {e}\")\n",
        "        yield \"Crawl Failed!\", log_stream.getvalue(), \"\", f\"**Error:** {e}\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ•¸ï¸ Memory-Optimized Parquet Web Crawler\")\n",
        "    gr.Markdown(\"This crawler saves data to a partitioned Parquet dataset and uses SQLite only to manage the crawl state.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Crawler Configuration\")\n",
        "            initial_url_input = gr.Textbox(label=\"Initial Start URL\", value=CrawlerConfig.initial_start_url)\n",
        "            allowed_path_input = gr.Textbox(label=\"Allowed Path Segment\", value=CrawlerConfig.allowed_path_segment)\n",
        "            crawling_strategy_radio = gr.Radio(choices=['BFS', 'DFS'], label=\"Crawling Strategy\", value='BFS')\n",
        "            max_pages_input = gr.Number(label=\"Maximum Pages to Crawl (per session)\", value=CrawlerConfig.max_pages_to_crawl, minimum=1, step=100)\n",
        "            gr.Markdown(\"### Storage Paths\")\n",
        "            state_db_path_input = gr.Textbox(label=\"Crawl State DB Path (SQLite)\", value=CrawlerConfig.state_db_path)\n",
        "            parquet_path_input = gr.Textbox(label=\"Crawled Data Path (Parquet)\", value=CrawlerConfig.parquet_path)\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## Actions and Status\")\n",
        "            start_button = gr.Button(\"ðŸš€ Start Crawl\", variant=\"primary\")\n",
        "            status_message_output = gr.Textbox(label=\"Status Message\", interactive=False)\n",
        "            logs_output = gr.Textbox(label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20)\n",
        "            with gr.Row():\n",
        "                save_events_output = gr.Markdown(\"### Save Events Log\")\n",
        "                summary_output = gr.Markdown(\"---\")\n",
        "\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_gradio_crawler_interface,\n",
        "        inputs=[initial_url_input, allowed_path_input, crawling_strategy_radio, state_db_path_input, parquet_path_input, max_pages_input],\n",
        "        outputs=[status_message_output, logs_output, save_events_output, summary_output]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7FCmsKPKA__",
        "outputId": "26e39d87-9071-4dbc-e3d8-a9321d3919cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ad411b623d7ce67ed2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ad411b623d7ce67ed2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-15 11:49:11,951 - INFO - Crawl frontier table ensured to exist.\n",
            "INFO:CrawlerLogger:Crawl frontier table ensured to exist.\n",
            "2025-06-15 11:49:11,953 - INFO - Rebuilding visited set from existing Parquet data...\n",
            "INFO:CrawlerLogger:Rebuilding visited set from existing Parquet data...\n",
            "2025-06-15 11:49:12,003 - INFO - Rebuilt visited set with 20 URLs.\n",
            "INFO:CrawlerLogger:Rebuilt visited set with 20 URLs.\n",
            "Crawling Progress:   0%|          | 0/700 [00:00<?, ?it/s]2025-06-15 11:49:14,187 - INFO - Fetched https://adevait.com/blog/remote-work/managing-remote-teams-the-key-to-employee-engagement [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/remote-work/managing-remote-teams-the-key-to-employee-engagement [200]\n",
            "Crawling Progress:   0%|          | 1/700 [00:02<25:39,  2.20s/it]2025-06-15 11:49:16,851 - INFO - Fetched https://adevait.com/blog/startups [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups [200]\n",
            "Crawling Progress:   0%|          | 2/700 [00:04<28:45,  2.47s/it]2025-06-15 11:49:19,267 - INFO - Fetched https://adevait.com/blog/startups/reduce-product-development-costs-startup [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/reduce-product-development-costs-startup [200]\n",
            "Crawling Progress:   0%|          | 3/700 [00:07<28:25,  2.45s/it]2025-06-15 11:49:21,132 - INFO - Fetched https://adevait.com/blog/future-of-work/10-best-open-source-startups [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/future-of-work/10-best-open-source-startups [200]\n",
            "Crawling Progress:   1%|          | 4/700 [00:09<25:43,  2.22s/it]2025-06-15 11:49:23,089 - INFO - Fetched https://adevait.com/blog/workplace/build-open-source-community [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/build-open-source-community [200]\n",
            "Crawling Progress:   1%|          | 5/700 [00:11<24:38,  2.13s/it]2025-06-15 11:49:24,966 - INFO - Fetched https://adevait.com/blog/workplace/the-engine-for-collaboration-advantages-of-open-source-for-your-business [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/the-engine-for-collaboration-advantages-of-open-source-for-your-business [200]\n",
            "Crawling Progress:   1%|          | 6/700 [00:12<23:39,  2.05s/it]2025-06-15 11:49:26,958 - INFO - Fetched https://adevait.com/blog/workplace/assess-the-digital-maturity-of-your-workplace-with-these-6-questions [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/assess-the-digital-maturity-of-your-workplace-with-these-6-questions [200]\n",
            "Crawling Progress:   1%|          | 7/700 [00:14<23:20,  2.02s/it]2025-06-15 11:49:28,766 - INFO - Fetched https://adevait.com/blog/workplace/these-four-traits-can-help-you-become-a-better-digital-transformation-leader [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/these-four-traits-can-help-you-become-a-better-digital-transformation-leader [200]\n",
            "Crawling Progress:   1%|          | 8/700 [00:16<22:30,  1.95s/it]2025-06-15 11:49:30,690 - INFO - Fetched https://adevait.com/blog/workplace/game-changing-digital-transformation-frameworks [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/game-changing-digital-transformation-frameworks [200]\n",
            "Crawling Progress:   1%|â–         | 9/700 [00:18<22:23,  1.94s/it]2025-06-15 11:49:32,477 - INFO - Fetched https://adevait.com/blog/workplace/your-digital-future-depends-on-your-digital-transformation-strategy [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/your-digital-future-depends-on-your-digital-transformation-strategy [200]\n",
            "Crawling Progress:   1%|â–         | 10/700 [00:20<21:48,  1.90s/it]2025-06-15 11:49:34,609 - INFO - Fetched https://adevait.com/blog/workplace/7-soft-skills-developers-2019 [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/7-soft-skills-developers-2019 [200]\n",
            "Crawling Progress:   2%|â–         | 11/700 [00:22<22:35,  1.97s/it]2025-06-15 11:49:36,438 - INFO - Fetched https://adevait.com/blog/remote-work/future-of-work-the-evolution-of-the-modern-employees [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/remote-work/future-of-work-the-evolution-of-the-modern-employees [200]\n",
            "Crawling Progress:   2%|â–         | 12/700 [00:24<22:06,  1.93s/it]2025-06-15 11:49:38,514 - INFO - Fetched https://adevait.com/blog/startups/motivate-remote-employees-tech-startup [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/motivate-remote-employees-tech-startup [200]\n",
            "Crawling Progress:   2%|â–         | 13/700 [00:26<22:33,  1.97s/it]2025-06-15 11:49:40,455 - INFO - Fetched https://adevait.com/blog/remote-work/agile-games-distributed-teams [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/remote-work/agile-games-distributed-teams [200]\n",
            "Crawling Progress:   2%|â–         | 14/700 [00:28<22:27,  1.96s/it]2025-06-15 11:49:42,721 - INFO - Fetched https://adevait.com/blog/community/tech-podcasts-tech-enthusiasts [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/community/tech-podcasts-tech-enthusiasts [200]\n",
            "Crawling Progress:   2%|â–         | 15/700 [00:30<23:26,  2.05s/it]2025-06-15 11:49:44,893 - INFO - Fetched https://adevait.com/blog/startups/write-brief-for-software-project [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/write-brief-for-software-project [200]\n",
            "Crawling Progress:   2%|â–         | 16/700 [00:32<23:48,  2.09s/it]2025-06-15 11:49:47,346 - INFO - Fetched https://adevait.com/blog/startups/israel-engineer-view-startup-nation [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/israel-engineer-view-startup-nation [200]\n",
            "Crawling Progress:   2%|â–         | 17/700 [00:35<25:01,  2.20s/it]2025-06-15 11:49:49,490 - INFO - Fetched https://adevait.com/blog/startups/build-minimum-viable-product [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/build-minimum-viable-product [200]\n",
            "Crawling Progress:   3%|â–Ž         | 18/700 [00:37<24:50,  2.19s/it]2025-06-15 11:49:51,450 - INFO - Fetched https://adevait.com/blog/workplace/agile-software-development [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/agile-software-development [200]\n",
            "Crawling Progress:   3%|â–Ž         | 19/700 [00:39<23:59,  2.11s/it]2025-06-15 11:49:53,859 - INFO - Fetched https://adevait.com/blog/workplace/5-slack-integrations [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/5-slack-integrations [200]\n",
            "Crawling Progress:   3%|â–Ž         | 20/700 [00:41<24:58,  2.20s/it]2025-06-15 11:49:53,945 - INFO - âœ… Saved a batch of **20** pages to partition `/content/drive/My Drive/master_july_2025/data/crawled_data_parquet/crawl_date=2025-06-15`\n",
            "INFO:CrawlerLogger:âœ… Saved a batch of **20** pages to partition `/content/drive/My Drive/master_july_2025/data/crawled_data_parquet/crawl_date=2025-06-15`\n",
            "2025-06-15 11:49:54,114 - INFO - Saved 37 URLs to frontier.\n",
            "INFO:CrawlerLogger:Saved 37 URLs to frontier.\n",
            "2025-06-15 11:49:56,437 - INFO - Fetched https://adevait.com/blog/agile-work/agile-testing-a-new-era-for-agile-teams [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/agile-work/agile-testing-a-new-era-for-agile-teams [200]\n",
            "Crawling Progress:   3%|â–Ž         | 21/700 [00:44<26:12,  2.32s/it]2025-06-15 11:49:58,537 - INFO - Fetched https://adevait.com/blog/startups/good-startup-cto [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/good-startup-cto [200]\n",
            "Crawling Progress:   3%|â–Ž         | 22/700 [00:46<25:25,  2.25s/it]2025-06-15 11:50:00,925 - INFO - Fetched https://adevait.com/blog/agile-work/staff-augmentation [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/agile-work/staff-augmentation [200]\n",
            "Crawling Progress:   3%|â–Ž         | 23/700 [00:48<25:56,  2.30s/it]2025-06-15 11:50:03,795 - INFO - Fetched https://adevait.com/blog/workplace/5-tools-assess-your-engineers [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/workplace/5-tools-assess-your-engineers [200]\n",
            "Crawling Progress:   3%|â–Ž         | 24/700 [00:51<27:44,  2.46s/it]2025-06-15 11:50:05,718 - INFO - Fetched https://adevait.com/blog/startups/interviewing-developers [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/interviewing-developers [200]\n",
            "Crawling Progress:   4%|â–Ž         | 25/700 [00:53<25:53,  2.30s/it]2025-06-15 11:50:07,731 - INFO - Fetched https://adevait.com/blog/startups/building-great-engineering-team [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/building-great-engineering-team [200]\n",
            "Crawling Progress:   4%|â–Ž         | 26/700 [00:55<24:52,  2.22s/it]2025-06-15 11:50:09,586 - INFO - Fetched https://adevait.com/blog/startups/open-innovation-financial-services-finos [200]\n",
            "INFO:CrawlerLogger:Fetched https://adevait.com/blog/startups/open-innovation-financial-services-finos [200]\n",
            "Crawling Progress:   4%|â–         | 27/700 [00:57<23:55,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ad411b623d7ce67ed2.gradio.live\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3449562879>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/displayhook.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \"\"\"\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_underscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_displayhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_output_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/displayhook.py\u001b[0m in \u001b[0;36mquiet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0msio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tokenize.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(readline, encoding)\u001b[0m\n\u001b[1;32m    591\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mTokenInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misidentifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m               \u001b[0;31m# ordinary name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mTokenInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0minitial\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m:\u001b[0m                      \u001b[0;31m# continued stmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}