{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RydrBsWi3GyG"
   },
   "source": [
    "# Always pefrorm Runtime -> Restart session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-ts3S8AJc2E"
   },
   "outputs": [],
   "source": [
    "# Installing necessary packages\n",
    "!pip install pandas pyarrow duckdb gradio requests beautifulsoup4 lxml tqdm -q\n",
    "!pip install fireducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7FCmsKPKA__"
   },
   "outputs": [],
   "source": [
    "### --- Imports and Setup ---\n",
    "import gradio as gr\n",
    "import sqlite3\n",
    "import fireducks.pandas as pd  # Assuming fireducks.pandas is intended as a pandas replacement\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "from urllib.parse import (\n",
    "    urljoin,\n",
    "    urlencode,\n",
    "    urlparse,\n",
    "    parse_qs,\n",
    ")  # ADDED urlparse, parse_qs\n",
    "from collections import deque\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "### --- Google Colab Drive Mount ---\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive/\")\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "\n",
    "\n",
    "### --- Configuration ---\n",
    "@dataclass\n",
    "class CrawlerConfig:\n",
    "    state_db_path: str = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/crawler_state.db\"\n",
    "    )\n",
    "    parquet_path: str = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/crawled_data_parquet/\"\n",
    "    )\n",
    "    min_request_delay: float = 1.0\n",
    "    max_request_delay: float = 30.0\n",
    "    max_pages_to_crawl: int = 700\n",
    "    save_interval_pages: int = 10\n",
    "    max_retries_request: int = 3\n",
    "    max_redirects: int = 2\n",
    "    request_timeout: int = 15\n",
    "    allowed_path_segment: str = \"/blog/\"\n",
    "    initial_start_url: str = \"https://example.com/blog\"\n",
    "    user_agents: list[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            # Chrome - Windows, Mac, Linux\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.117 Safari/537.36\",\n",
    "            # Firefox - Windows, Mac\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12.6; rv:115.0) Gecko/20100101 Firefox/115.0\",\n",
    "            # Edge - Windows\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.132 Safari/537.36 Edg/117.0.2045.60\",\n",
    "            # Mobile Browsers\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",\n",
    "            \"Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.92 Mobile Safari/537.36\",\n",
    "            # Googlebot / SEO crawlers\n",
    "            \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\",\n",
    "            \"Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)\",\n",
    "        ]\n",
    "    )\n",
    "    # --- NEW: Whitelist of query parameters to keep ---\n",
    "    allowed_query_params: list[str] = field(\n",
    "        default_factory=lambda: [\"page\", \"p\", \"q\", \"id\", \"post\"]\n",
    "    )\n",
    "    base_domain: str = \"\"\n",
    "\n",
    "\n",
    "### --- Logging and Core Interfaces (Unchanged) ---\n",
    "class ILogger(ABC):\n",
    "    @abstractmethod\n",
    "    def debug(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def info(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def warning(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def error(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def exception(self, message: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "class GradioLogHandler(logging.Handler):\n",
    "    def __init__(self, log_output_stream: io.StringIO):\n",
    "        super().__init__()\n",
    "        self.log_output_stream = log_output_stream\n",
    "        self.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "\n",
    "    def emit(self, record):\n",
    "        log_entry = self.format(record)\n",
    "        self.log_output_stream.write(log_entry + \"\\n\")\n",
    "        self.log_output_stream.flush()\n",
    "\n",
    "\n",
    "class ConsoleAndGradioLogger(ILogger):\n",
    "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
    "        self._logger = logging.getLogger(\"CrawlerLogger\")\n",
    "        self._logger.setLevel(level)\n",
    "        if self._logger.hasHandlers():\n",
    "            self._logger.handlers.clear()\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        self._logger.addHandler(console_handler)\n",
    "        gradio_handler = GradioLogHandler(log_output_stream)\n",
    "        self._logger.addHandler(gradio_handler)\n",
    "\n",
    "    def debug(self, message: str):\n",
    "        self._logger.debug(message)\n",
    "\n",
    "    def info(self, message: str):\n",
    "        self._logger.info(message)\n",
    "\n",
    "    def warning(self, message: str):\n",
    "        self._logger.warning(message)\n",
    "\n",
    "    def error(self, message: str):\n",
    "        self._logger.error(message)\n",
    "\n",
    "    def exception(self, message: str):\n",
    "        self._logger.exception(message)\n",
    "\n",
    "\n",
    "class VisitedUrlManager:\n",
    "    def __init__(self):\n",
    "        self.visited = set()\n",
    "\n",
    "    def add(self, url: str):\n",
    "        self.visited.add(url)\n",
    "\n",
    "    def contains(self, url: str) -> bool:\n",
    "        return url in self.visited\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.visited)\n",
    "\n",
    "\n",
    "class CrawlingStrategy(ABC):\n",
    "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
    "        self.visited = visited_manager\n",
    "        self.logger = logger\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_links(self, links_info: list[tuple[str, int]]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_next(self) -> tuple[str, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def has_next(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BFSCrawlingStrategy(CrawlingStrategy):\n",
    "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
    "        super().__init__(visited_manager, logger)\n",
    "        self.queue = deque()\n",
    "\n",
    "    def add_links(self, links_info: list[tuple[str, int]]):\n",
    "        new_links = [\n",
    "            link_info\n",
    "            for link_info in links_info\n",
    "            if not self.visited.contains(link_info[0])\n",
    "        ]\n",
    "        for link_url, _ in new_links:\n",
    "            self.visited.add(link_url)\n",
    "        self.queue.extend(new_links)\n",
    "\n",
    "    def get_next(self) -> tuple[str, int]:\n",
    "        return self.queue.popleft()\n",
    "\n",
    "    def has_next(self) -> bool:\n",
    "        return len(self.queue) > 0\n",
    "\n",
    "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
    "        self.queue.extend(frontier_urls_info)\n",
    "\n",
    "\n",
    "class DFSCrawlingStrategy(CrawlingStrategy):\n",
    "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
    "        super().__init__(visited_manager, logger)\n",
    "        self.stack = []\n",
    "\n",
    "    def add_links(self, links_info: list[tuple[str, int]]):\n",
    "        new_links = [\n",
    "            link_info\n",
    "            for link_info in links_info\n",
    "            if not self.visited.contains(link_info[0])\n",
    "        ]\n",
    "        for link_url, _ in new_links:\n",
    "            self.visited.add(link_url)\n",
    "        self.stack.extend(new_links)\n",
    "\n",
    "    def get_next(self) -> tuple[str, int]:\n",
    "        return self.stack.pop()\n",
    "\n",
    "    def has_next(self) -> bool:\n",
    "        return len(self.stack) > 0\n",
    "\n",
    "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
    "        self.stack.extend(frontier_urls_info)\n",
    "\n",
    "\n",
    "class StateManager:\n",
    "    def __init__(self, db_path: str, logger: ILogger):\n",
    "        self.db_path = db_path\n",
    "        self.logger = logger\n",
    "        db_dir = os.path.dirname(self.db_path)\n",
    "        if db_dir:\n",
    "            os.makedirs(db_dir, exist_ok=True)\n",
    "        self.ensure_frontier_table_exists()\n",
    "\n",
    "    def _execute_query(self, query: str, params=None, fetch=False):\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(query, params or [])\n",
    "                if fetch:\n",
    "                    return cursor.fetchall()\n",
    "                conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"StateManager DB error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def ensure_frontier_table_exists(self):\n",
    "        self._execute_query(\n",
    "            \"CREATE TABLE IF NOT EXISTS crawl_frontier (URL TEXT UNIQUE, Redirects INTEGER)\"\n",
    "        )\n",
    "        self.logger.info(\"Crawl frontier table ensured to exist.\")\n",
    "\n",
    "    def save_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
    "        self._execute_query(\"DELETE FROM crawl_frontier\")\n",
    "        if not frontier_urls_info:\n",
    "            return\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                conn.executemany(\n",
    "                    \"INSERT OR IGNORE INTO crawl_frontier (URL, Redirects) VALUES (?, ?)\",\n",
    "                    frontier_urls_info,\n",
    "                )\n",
    "                conn.commit()\n",
    "            self.logger.info(f\"Saved {len(frontier_urls_info)} URLs to frontier.\")\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"Error saving frontier: {e}\")\n",
    "\n",
    "    def load_frontier(self) -> list[tuple[str, int]]:\n",
    "        result = self._execute_query(\n",
    "            \"SELECT URL, Redirects FROM crawl_frontier\", fetch=True\n",
    "        )\n",
    "        return result or []\n",
    "\n",
    "\n",
    "class HttpClient:\n",
    "    def __init__(self, config: CrawlerConfig, logger: ILogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.session = self._create_requests_session()\n",
    "        self.current_delay = self.config.min_request_delay\n",
    "\n",
    "    def _create_requests_session(self):\n",
    "        retry = Retry(\n",
    "            total=self.config.max_retries_request,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session = requests.Session()\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def fetch(self, url: str) -> tuple[int, str, str | None]:\n",
    "        headers = {\"User-Agent\": random.choice(self.config.user_agents)}\n",
    "        try:\n",
    "            time.sleep(self.current_delay)\n",
    "            response = self.session.get(\n",
    "                url,\n",
    "                headers=headers,\n",
    "                timeout=self.config.request_timeout,\n",
    "                allow_redirects=False,\n",
    "            )\n",
    "            if 300 <= response.status_code < 400:\n",
    "                redirect_url = response.headers.get(\"Location\")\n",
    "                return response.status_code, \"\", urljoin(url, redirect_url)\n",
    "            return response.status_code, response.text, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"Request error for {url}: {e}\")\n",
    "            return -2, f\"Request Error: {e}\", None\n",
    "\n",
    "\n",
    "class UrlFilter:\n",
    "    def __init__(self, allowed_path_segment: str, base_domain: str):\n",
    "        self.allowed_path_segment = allowed_path_segment\n",
    "        self.base_domain = base_domain\n",
    "        self.file_extension_pattern = re.compile(\n",
    "            r\"\\.(jpg|jpeg|png|gif|pdf|doc|xls|zip|rar|mp3|mp4)$\", re.I\n",
    "        )\n",
    "\n",
    "    def is_valid(self, url: str) -> bool:\n",
    "        try:\n",
    "            parsed_url = urlparse(url)\n",
    "            return (\n",
    "                parsed_url.scheme in (\"http\", \"https\")\n",
    "                and parsed_url.netloc == self.base_domain\n",
    "                and self.allowed_path_segment in parsed_url.path\n",
    "                and not self.file_extension_pattern.search(parsed_url.path)\n",
    "            )\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "### --- LinkExtractor (UPDATED with Normalization Logic) ---\n",
    "class LinkExtractor:\n",
    "    def __init__(self, url_filter: UrlFilter, allowed_params: list[str]):\n",
    "        self.url_filter = url_filter\n",
    "        self.allowed_params = allowed_params\n",
    "\n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Strips fragments and unwanted query parameters from a URL.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "\n",
    "        # If there are query parameters, filter them\n",
    "        if parsed.query:\n",
    "            query_params = parse_qs(parsed.query)\n",
    "            # Keep only the parameters that are in our whitelist\n",
    "            filtered_params = {\n",
    "                k: v for k, v in query_params.items() if k in self.allowed_params\n",
    "            }\n",
    "            # Re-encode the query string with only the allowed parameters\n",
    "            clean_query = urlencode(filtered_params, doseq=True)\n",
    "        else:\n",
    "            clean_query = \"\"\n",
    "\n",
    "        # Reconstruct the URL with the clean query and no fragment\n",
    "        normalized = parsed._replace(query=clean_query, fragment=\"\").geturl()\n",
    "        return normalized\n",
    "\n",
    "    def extract_links(self, base_url: str, html_content: str) -> list[str]:\n",
    "        links = set()\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            href = a_tag[\"href\"]\n",
    "            absolute_link = urljoin(base_url, href)\n",
    "\n",
    "            # Normalize the link before doing anything else\n",
    "            normalized_link = self.normalize_url(absolute_link)\n",
    "\n",
    "            if self.url_filter.is_valid(normalized_link):\n",
    "                links.add(normalized_link)\n",
    "        return list(links)\n",
    "\n",
    "\n",
    "### --- WebCrawler and Main UI (Unchanged, but benefits from new LinkExtractor) ---\n",
    "class WebCrawler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: CrawlerConfig,\n",
    "        crawling_strategy: CrawlingStrategy,\n",
    "        state_manager: StateManager,\n",
    "        http_client: HttpClient,\n",
    "        url_filter: UrlFilter,\n",
    "        link_extractor: LinkExtractor,\n",
    "        logger: ILogger,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.crawling_strategy = crawling_strategy\n",
    "        self.state_manager = state_manager\n",
    "        self.http_client = http_client\n",
    "        self.url_filter = url_filter\n",
    "        self.link_extractor = link_extractor\n",
    "        self.logger = logger\n",
    "        self.data_buffer = []\n",
    "        self.pages_crawled_session = 0\n",
    "\n",
    "    def _process_url(self, url_info: tuple[str, int]):\n",
    "        url, num_redirects = url_info\n",
    "        if num_redirects >= self.config.max_redirects:\n",
    "            self.data_buffer.append(\n",
    "                {\"URL\": url, \"Status_Code\": 999, \"Content\": \"Max redirects reached\"}\n",
    "            )\n",
    "            return\n",
    "\n",
    "        status, content, redirect_url = self.http_client.fetch(url)\n",
    "        self.logger.info(f\"Fetched {url} [{status}]\")\n",
    "        self.data_buffer.append(\n",
    "            {\n",
    "                \"URL\": url,\n",
    "                \"Status_Code\": status,\n",
    "                \"Content\": content if 200 <= status < 300 else \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if 200 <= status < 300:\n",
    "            extracted = self.link_extractor.extract_links(url, content)\n",
    "            self.crawling_strategy.add_links([(link, 0) for link in extracted])\n",
    "            del content\n",
    "        elif redirect_url:\n",
    "            # Also normalize redirect URLs before validation\n",
    "            normalized_redirect = self.link_extractor.normalize_url(redirect_url)\n",
    "            if self.url_filter.is_valid(normalized_redirect):\n",
    "                self.crawling_strategy.add_links(\n",
    "                    [(normalized_redirect, num_redirects + 1)]\n",
    "                )\n",
    "\n",
    "    def _save_buffer_to_parquet(self) -> str | None:\n",
    "        if not self.data_buffer:\n",
    "            return None\n",
    "        num_records = len(self.data_buffer)\n",
    "        # Using fireducks.pandas as intended by the import\n",
    "        df = pd.DataFrame(self.data_buffer)\n",
    "        today = datetime.now().date()\n",
    "        df[\"crawl_date\"] = today\n",
    "        try:\n",
    "            partition_path = os.path.join(\n",
    "                self.config.parquet_path, f\"crawl_date={today}\"\n",
    "            )\n",
    "            # Ensure the partition directory exists before writing\n",
    "            os.makedirs(partition_path, exist_ok=True)\n",
    "            # Write to the specific partition directory\n",
    "            df.to_parquet(\n",
    "                path=os.path.join(\n",
    "                    partition_path, f\"{int(time.time())}.parquet\"\n",
    "                ),  # Unique filename within partition\n",
    "                engine=\"pyarrow\",\n",
    "                compression=\"snappy\",\n",
    "                # partition_cols=[\"crawl_date\"], # Not needed when saving to a pre-created partition directory\n",
    "            )\n",
    "            log_message = f\"âœ… Saved a batch of **{num_records}** pages to partition `{partition_path}`\"\n",
    "            self.logger.info(log_message)\n",
    "            self.data_buffer = []\n",
    "            gc.collect()\n",
    "            return log_message\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to Parquet: {e}\")\n",
    "            return None\n",
    "\n",
    "    def crawl(self):\n",
    "        pbar = tqdm(total=self.config.max_pages_to_crawl, desc=\"Crawling Progress\")\n",
    "        while self.pages_crawled_session < self.config.max_pages_to_crawl:\n",
    "            if not self.crawling_strategy.has_next():\n",
    "                self.logger.info(\"Frontier is empty. Stopping crawl.\")\n",
    "                break\n",
    "\n",
    "            url_data = self.crawling_strategy.get_next()\n",
    "            self._process_url(url_data)\n",
    "            self.pages_crawled_session += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            save_event_msg = None\n",
    "            if len(self.data_buffer) >= self.config.save_interval_pages:\n",
    "                save_event_msg = self._save_buffer_to_parquet()\n",
    "                # Determine current frontier based on strategy and save it\n",
    "                current_frontier = []\n",
    "                if hasattr(self.crawling_strategy, \"queue\"):\n",
    "                    current_frontier = list(self.crawling_strategy.queue)\n",
    "                elif hasattr(self.crawling_strategy, \"stack\"):\n",
    "                    current_frontier = list(self.crawling_strategy.stack)\n",
    "                self.state_manager.save_frontier(current_frontier)\n",
    "\n",
    "            yield {\n",
    "                \"status\": f\"Crawled {self.pages_crawled_session}/{self.config.max_pages_to_crawl} pages.\",\n",
    "                \"save_event\": save_event_msg,\n",
    "            }\n",
    "\n",
    "        pbar.close()\n",
    "        self.logger.info(\"Crawl finished. Performing final save...\")\n",
    "        final_save_msg = self._save_buffer_to_parquet()\n",
    "        # Final save of the frontier state\n",
    "        current_frontier = []\n",
    "        if hasattr(self.crawling_strategy, \"queue\"):\n",
    "            current_frontier = list(self.crawling_strategy.queue)\n",
    "        elif hasattr(self.crawling_strategy, \"stack\"):\n",
    "            current_frontier = list(self.crawling_strategy.stack)\n",
    "        self.state_manager.save_frontier(current_frontier)\n",
    "\n",
    "        yield {\n",
    "            \"status\": f\"Crawl finished. Processed {self.pages_crawled_session} pages.\",\n",
    "            \"save_event\": final_save_msg,\n",
    "        }\n",
    "\n",
    "\n",
    "def run_gradio_crawler_interface(\n",
    "    initial_start_url: str,\n",
    "    allowed_path_segment: str,\n",
    "    crawling_strategy_type: str,\n",
    "    state_db_path_input: str,\n",
    "    parquet_path_input: str,\n",
    "    max_pages_to_crawl: int,\n",
    "):\n",
    "    log_stream = io.StringIO()\n",
    "    logger = ConsoleAndGradioLogger(log_stream)\n",
    "\n",
    "    try:\n",
    "        base_domain = urlparse(initial_start_url).netloc\n",
    "        if not base_domain:\n",
    "            raise ValueError(\"Invalid Initial Start URL.\")\n",
    "\n",
    "        config = CrawlerConfig(\n",
    "            initial_start_url=initial_start_url,\n",
    "            allowed_path_segment=allowed_path_segment,\n",
    "            state_db_path=state_db_path_input,\n",
    "            parquet_path=parquet_path_input,\n",
    "            max_pages_to_crawl=max_pages_to_crawl,\n",
    "            base_domain=base_domain,\n",
    "        )\n",
    "        os.makedirs(config.parquet_path, exist_ok=True)\n",
    "        yield (\n",
    "            \"Initializing...\",\n",
    "            log_stream.getvalue(),\n",
    "            \"### Save Events Log\\n\\n- Waiting for first save event...\",\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "        state_manager = StateManager(config.state_db_path, logger)\n",
    "        visited_manager = VisitedUrlManager()\n",
    "\n",
    "        logger.info(\"Rebuilding visited set from existing Parquet data...\")\n",
    "        try:\n",
    "            # Use os.path.join for path construction to ensure cross-platform compatibility\n",
    "            parquet_glob_path = os.path.join(config.parquet_path, \"**\", \"*.parquet\")\n",
    "            # DuckDB needs forward slashes for glob paths, so convert if on Windows\n",
    "            if os.sep != \"/\":\n",
    "                parquet_glob_path = parquet_glob_path.replace(os.sep, \"/\")\n",
    "            visited_urls_df = duckdb.query(\n",
    "                f\"SELECT DISTINCT URL FROM read_parquet('{parquet_glob_path}')\"\n",
    "            ).to_df()\n",
    "            for url in visited_urls_df[\"URL\"]:\n",
    "                visited_manager.add(url)\n",
    "            logger.info(f\"Rebuilt visited set with {visited_manager.size()} URLs.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                f\"Could not rebuild visited set from Parquet (may be a new crawl or empty directory): {e}\"\n",
    "            )\n",
    "\n",
    "        strategy_class = (\n",
    "            BFSCrawlingStrategy\n",
    "            if crawling_strategy_type == \"BFS\"\n",
    "            else DFSCrawlingStrategy\n",
    "        )\n",
    "        crawling_strategy = strategy_class(visited_manager, logger)\n",
    "\n",
    "        loaded_frontier = state_manager.load_frontier()\n",
    "        unvisited_frontier = [\n",
    "            info for info in loaded_frontier if not visited_manager.contains(info[0])\n",
    "        ]\n",
    "\n",
    "        if unvisited_frontier:\n",
    "            crawling_strategy.prime_with_frontier(unvisited_frontier)\n",
    "            logger.info(f\"Primed frontier with {len(unvisited_frontier)} URLs from DB.\")\n",
    "        elif not visited_manager.contains(config.initial_start_url):\n",
    "            crawling_strategy.add_links([(config.initial_start_url, 0)])\n",
    "            logger.info(f\"Added initial URL {config.initial_start_url} to frontier.\")\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"Initial URL {config.initial_start_url} already visited. No new URLs to start with from DB or initial URL.\"\n",
    "            )\n",
    "\n",
    "        http_client = HttpClient(config, logger)\n",
    "        url_filter = UrlFilter(config.allowed_path_segment, config.base_domain)\n",
    "        # --- Pass the whitelist to the LinkExtractor ---\n",
    "        link_extractor = LinkExtractor(url_filter, config.allowed_query_params)\n",
    "\n",
    "        crawler = WebCrawler(\n",
    "            config,\n",
    "            crawling_strategy,\n",
    "            state_manager,\n",
    "            http_client,\n",
    "            url_filter,\n",
    "            link_extractor,\n",
    "            logger,\n",
    "        )\n",
    "\n",
    "        final_status = \"\"\n",
    "        save_events_log = [\"### Save Events Log\"]\n",
    "\n",
    "        for event in crawler.crawl():\n",
    "            status_msg = event.get(\"status\")\n",
    "            save_event = event.get(\"save_event\")\n",
    "\n",
    "            final_status = status_msg\n",
    "            if save_event:\n",
    "                save_events_log.append(f\"- {save_event}\")\n",
    "            yield status_msg, log_stream.getvalue(), \"\\n\".join(save_events_log), \"\"\n",
    "\n",
    "        logger.info(\"Generating final summary from Parquet data...\")\n",
    "        final_save_events = \"\\n\".join(save_events_log)\n",
    "        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Crawled Data Location**: `{config.parquet_path}`\"\n",
    "        try:\n",
    "            parquet_glob_path = os.path.join(config.parquet_path, \"**\", \"*.parquet\")\n",
    "            if os.sep != \"/\":\n",
    "                parquet_glob_path = parquet_glob_path.replace(os.sep, \"/\")\n",
    "            summary_df = duckdb.query(\n",
    "                f\"SELECT CASE WHEN Status_Code >= 200 AND Status_Code < 300 THEN 'Success (Content Saved)' WHEN Status_Code >= 300 AND Status_Code < 400 THEN 'Redirect' ELSE 'Error / Other' END AS Category, COUNT(*) as Total FROM read_parquet('{parquet_glob_path}') GROUP BY Category ORDER BY Total DESC\"\n",
    "            ).to_df()\n",
    "            total_urls = summary_df[\"Total\"].sum()\n",
    "            summary_md += f\"\\n- **Total URLs in Parquet Dataset**: {total_urls}\\n\\n### Crawl Summary by Category\\n\\n\"\n",
    "            summary_md += summary_df.to_markdown(index=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not generate summary from Parquet: {e}\")\n",
    "            summary_md += \"\\n\\n**Could not generate summary from Parquet data.**\"\n",
    "\n",
    "        yield final_status, log_stream.getvalue(), final_save_events, summary_md\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"A critical error occurred: {e}\")\n",
    "        yield \"Crawl Failed!\", log_stream.getvalue(), \"\", f\"**Error:** {e}\"\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ðŸ•¸ï¸ Memory-Optimized Parquet Web Crawler\")\n",
    "    gr.Markdown(\n",
    "        \"This crawler saves data to a partitioned Parquet dataset and uses SQLite only to manage the crawl state.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Crawler Configuration\")\n",
    "            initial_url_input = gr.Textbox(\n",
    "                label=\"Initial Start URL\", value=CrawlerConfig.initial_start_url\n",
    "            )\n",
    "            allowed_path_input = gr.Textbox(\n",
    "                label=\"Allowed Path Segment\", value=CrawlerConfig.allowed_path_segment\n",
    "            )\n",
    "            crawling_strategy_radio = gr.Radio(\n",
    "                choices=[\"BFS\", \"DFS\"], label=\"Crawling Strategy\", value=\"BFS\"\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                - **BFS (Breadth-First Search)**: Ideal for performing a wide crawl across a website, exploring all links at the current depth level before moving to the next.\n",
    "                - **DFS (Depth-First Search)**: More suitable for targeting specific folders or branches of a website, exploring as deeply as possible along one path before backtracking.\n",
    "                \"\"\"\n",
    "            )\n",
    "            max_pages_input = gr.Number(\n",
    "                label=\"Maximum Pages to Crawl (per session)\",\n",
    "                value=CrawlerConfig.max_pages_to_crawl,\n",
    "                minimum=1,\n",
    "                step=100,\n",
    "            )\n",
    "            gr.Markdown(\"### Storage Paths\")\n",
    "            state_db_path_input = gr.Textbox(\n",
    "                label=\"Crawl State DB Path (SQLite)\", value=CrawlerConfig.state_db_path\n",
    "            )\n",
    "            parquet_path_input = gr.Textbox(\n",
    "                label=\"Crawled Data Path (Parquet)\", value=CrawlerConfig.parquet_path\n",
    "            )\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"## Actions and Status\")\n",
    "            start_button = gr.Button(\"ðŸš€ Start Crawl\", variant=\"primary\")\n",
    "            status_message_output = gr.Textbox(\n",
    "                label=\"Status Message\", interactive=False\n",
    "            )\n",
    "            logs_output = gr.Textbox(\n",
    "                label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20\n",
    "            )\n",
    "            with gr.Row():\n",
    "                save_events_output = gr.Markdown(\"### Save Events Log\")\n",
    "                summary_output = gr.Markdown(\"---\")\n",
    "\n",
    "    start_button.click(\n",
    "        fn=run_gradio_crawler_interface,\n",
    "        inputs=[\n",
    "            initial_url_input,\n",
    "            allowed_path_input,\n",
    "            crawling_strategy_radio,\n",
    "            state_db_path_input,\n",
    "            parquet_path_input,\n",
    "            max_pages_input,\n",
    "        ],\n",
    "        outputs=[\n",
    "            status_message_output,\n",
    "            logs_output,\n",
    "            save_events_output,\n",
    "            summary_output,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}