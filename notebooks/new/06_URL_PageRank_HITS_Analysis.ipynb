{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Make sure to restart the Runtime and have a clean state to work with"
      ],
      "metadata": {
        "id": "r-DYI18ZXq6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 1: Install Required Libraries ---\n",
        "#\n",
        "!pip install pandas gradio networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aghKngLiWJjt",
        "outputId": "0d6bae8e-3bf7-45f5-ab65-b4c82a60183a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.34.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.3->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "_pDap4ewV4m8",
        "outputId": "49f957ef-a7dc-42cd-ec8f-a56faba7363b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to mount Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mount command executed.\n",
            "PageRank CSV Path: /content/drive/My Drive/master_july_2025/data/url_analysis_results.csv\n",
            "Link Graph CSV Path: /content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\n",
            "GradioApp initialization started.\n",
            "Attempting to load PageRank data from /content/drive/My Drive/master_july_2025/data/url_analysis_results.csv\n",
            "CSVLoader initialized with file_path: /content/drive/My Drive/master_july_2025/data/url_analysis_results.csv\n",
            "Attempting to load data from: /content/drive/My Drive/master_july_2025/data/url_analysis_results.csv\n",
            "Successfully loaded data. DataFrame shape: (884, 3)\n",
            "PageRankAnalyzer initialized. Processed DataFrame shape: (884, 3)\n",
            "Attempting to load Link Graph data from /content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\n",
            "CSVLoader initialized with file_path: /content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\n",
            "Attempting to load data from: /content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\n",
            "Successfully loaded data. DataFrame shape: (10026, 2)\n",
            "HITSGraphAnalyzer initialized. Graph has 884 nodes and 10026 edges.\n",
            "GradioApp initialization finished.\n",
            "Launching Gradio demo...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e4c3b22b5e70684390.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e4c3b22b5e70684390.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import io\n",
        "import os\n",
        "import networkx as nx # Import networkx for graph operations and HITS\n",
        "import re # For URL parsing in trimming function\n",
        "\n",
        "# --- 1. CSVLoader Class (Single Responsibility: Loading data) ---\n",
        "class CSVLoader:\n",
        "    \"\"\"\n",
        "    Handles loading data from a CSV file.\n",
        "    Follows SRP by being solely responsible for data acquisition.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path: str):\n",
        "        \"\"\"\n",
        "        Initializes the CSVLoader with the path to the CSV file.\n",
        "        :param file_path: The full path to the CSV file.\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.dataframe = None\n",
        "        print(f\"CSVLoader initialized with file_path: {self.file_path}\") # Log\n",
        "\n",
        "    def load_data(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Loads the CSV content from the specified file path into a pandas DataFrame.\n",
        "        Raises FileNotFoundError if the file is not found.\n",
        "        Raises ValueError if essential columns are missing.\n",
        "        :return: A pandas DataFrame containing the loaded data.\n",
        "        \"\"\"\n",
        "        print(f\"Attempting to load data from: {self.file_path}\") # Log\n",
        "        if not os.path.exists(self.file_path):\n",
        "            print(f\"FileNotFoundError: File does not exist at {self.file_path}\") # Log\n",
        "            raise FileNotFoundError(f\"CSV file not found at: {self.file_path}\")\n",
        "\n",
        "        try:\n",
        "            self.dataframe = pd.read_csv(self.file_path)\n",
        "            print(f\"Successfully loaded data. DataFrame shape: {self.dataframe.shape}\") # Log\n",
        "            return self.dataframe\n",
        "        except Exception as e:\n",
        "            print(f\"IOError: Error loading CSV data: {e}\") # Log\n",
        "            raise IOError(f\"Error loading CSV data from {self.file_path}: {e}\")\n",
        "\n",
        "# --- 2. PageRankAnalyzer Class (Single Responsibility: Analyzing PageRank data) ---\n",
        "class PageRankAnalyzer:\n",
        "    \"\"\"\n",
        "    Performs analysis on URL data to find worst PageRank candidates.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Initializes the PageRankAnalyzer with a pandas DataFrame containing URL, Folder_Depth, and PageRank.\n",
        "        \"\"\"\n",
        "        required_columns = ['URL', 'Folder_Depth', 'PageRank']\n",
        "        if not all(col in dataframe.columns for col in required_columns):\n",
        "            print(f\"ValueError: PageRank data missing required columns. Found: {dataframe.columns.tolist()}\") # Log\n",
        "            raise ValueError(\n",
        "                f\"PageRank data must contain 'URL', 'Folder_Depth', and 'PageRank' columns. \"\n",
        "                f\"Found columns: {dataframe.columns.tolist()}\"\n",
        "            )\n",
        "        # Ensure correct data types and handle missing values\n",
        "        dataframe['Folder_Depth'] = pd.to_numeric(dataframe['Folder_Depth'], errors='coerce').fillna(-1).astype(int)\n",
        "        dataframe['PageRank'] = pd.to_numeric(dataframe['PageRank'], errors='coerce')\n",
        "        dataframe.dropna(subset=['PageRank'], inplace=True)\n",
        "\n",
        "        self.dataframe = dataframe\n",
        "        print(f\"PageRankAnalyzer initialized. Processed DataFrame shape: {self.dataframe.shape}\") # Log\n",
        "\n",
        "    def find_worst_candidates(self, depth_level: int, top_n: int) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Finds the top 'n' URL candidates with the worst (lowest) PageRank\n",
        "        at a specific folder depth level.\n",
        "        \"\"\"\n",
        "        print(f\"Finding worst PageRank candidates for depth: {depth_level}, top_n: {top_n}\") # Log\n",
        "        if self.dataframe.empty:\n",
        "            print(\"PageRank dataframe is empty.\") # Log\n",
        "            return pd.DataFrame(columns=['URL', 'Folder_Depth', 'PageRank'])\n",
        "\n",
        "        filtered_df = self.dataframe[self.dataframe['Folder_Depth'] == depth_level].copy()\n",
        "        print(f\"Filtered PageRank DataFrame shape at depth {depth_level}: {filtered_df.shape}\") # Log\n",
        "\n",
        "        if filtered_df.empty:\n",
        "            print(f\"No PageRank entries found at depth level {depth_level}.\") # Log\n",
        "            return pd.DataFrame(columns=['URL', 'Folder_Depth', 'PageRank'])\n",
        "\n",
        "        sorted_df = filtered_df.sort_values(by='PageRank', ascending=True)\n",
        "        worst_candidates = sorted_df.head(top_n)\n",
        "\n",
        "        # Apply URL trimming for display - REMOVED AS PER USER REQUEST\n",
        "        # worst_candidates['URL'] = worst_candidates['URL'].apply(_trim_url_for_display)\n",
        "\n",
        "        print(f\"Found {len(worst_candidates)} worst PageRank candidates.\") # Log\n",
        "        return worst_candidates[['URL', 'Folder_Depth', 'PageRank']]\n",
        "\n",
        "# --- 3. HITSGraphAnalyzer Class (Single Responsibility: Analyzing graph data with HITS) ---\n",
        "class HITSGraphAnalyzer:\n",
        "    \"\"\"\n",
        "    Performs HITS algorithm analysis on a link graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, edges_dataframe: pd.DataFrame, pagerank_dataframe: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Initializes the HITSGraphAnalyzer with a pandas DataFrame containing 'FROM' and 'TO' columns\n",
        "        and the pagerank_dataframe to merge Folder_Depth.\n",
        "        :param edges_dataframe: DataFrame with 'FROM' and 'TO' columns representing directed edges.\n",
        "        :param pagerank_dataframe: DataFrame containing 'URL' and 'Folder_Depth' for merging.\n",
        "        \"\"\"\n",
        "        required_edges_columns = ['FROM', 'TO']\n",
        "        if not all(col in edges_dataframe.columns for col in required_edges_columns):\n",
        "            print(f\"ValueError: Link graph data missing required columns. Found: {edges_dataframe.columns.tolist()}\") # Log\n",
        "            raise ValueError(\n",
        "                f\"Link graph data must contain 'FROM' and 'TO' columns. \"\n",
        "                f\"Found columns: {edges_dataframe.columns.tolist()}\"\n",
        "            )\n",
        "\n",
        "        self.graph = nx.DiGraph() # Create a directed graph\n",
        "        for _, row in edges_dataframe.iterrows():\n",
        "            self.graph.add_edge(row['FROM'], row['TO'])\n",
        "        print(f\"HITSGraphAnalyzer initialized. Graph has {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges.\") # Log\n",
        "\n",
        "        # Store pagerank_dataframe for merging folder depth later\n",
        "        self.pagerank_dataframe = pagerank_dataframe[['URL', 'Folder_Depth']].copy()\n",
        "        # Ensure Folder_Depth is correctly typed in the pagerank_dataframe\n",
        "        self.pagerank_dataframe['Folder_Depth'] = pd.to_numeric(self.pagerank_dataframe['Folder_Depth'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "    def calculate_hits_scores(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculates Hub and Authority scores using the HITS algorithm and merges Folder_Depth.\n",
        "        :return: A DataFrame with 'URL', 'Folder_Depth', 'Hub Score', and 'Authority Score'.\n",
        "                 Returns an empty DataFrame if the graph is empty.\n",
        "        \"\"\"\n",
        "        print(\"Calculating HITS scores...\") # Log\n",
        "        if self.graph.number_of_nodes() == 0:\n",
        "            print(\"Graph is empty, returning empty HITS DataFrame.\") # Log\n",
        "            return pd.DataFrame(columns=['URL', 'Folder_Depth', 'Hub Score', 'Authority Score'])\n",
        "\n",
        "        try:\n",
        "            hubs, authorities = nx.hits(self.graph)\n",
        "            print(f\"HITS calculation complete. Found {len(hubs)} hubs and {len(authorities)} authorities.\") # Log\n",
        "\n",
        "            hits_data = []\n",
        "            for node in self.graph.nodes():\n",
        "                hits_data.append({\n",
        "                    'URL': node,\n",
        "                    'Hub Score': hubs.get(node, 0.0),\n",
        "                    'Authority Score': authorities.get(node, 0.0)\n",
        "                })\n",
        "\n",
        "            hits_df = pd.DataFrame(hits_data)\n",
        "\n",
        "            # Merge with pagerank_dataframe to get Folder_Depth\n",
        "            merged_hits_df = pd.merge(\n",
        "                hits_df,\n",
        "                self.pagerank_dataframe,\n",
        "                on='URL',\n",
        "                how='left' # Use left merge to keep all HITS results\n",
        "            )\n",
        "            # Fill NaN Folder_Depth values for URLs not found in pagerank_dataframe\n",
        "            merged_hits_df['Folder_Depth'].fillna(-1, inplace=True)\n",
        "            merged_hits_df['Folder_Depth'] = merged_hits_df['Folder_Depth'].astype(int)\n",
        "\n",
        "            merged_hits_df = merged_hits_df.sort_values(by='Authority Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "            # Apply URL trimming for display - REMOVED AS PER USER REQUEST\n",
        "            # merged_hits_df['URL'] = merged_hits_df['URL'].apply(_trim_url_for_display)\n",
        "\n",
        "            print(f\"HITS DataFrame created. Shape: {merged_hits_df.shape}\") # Log\n",
        "            return merged_hits_df[['URL', 'Folder_Depth', 'Hub Score', 'Authority Score']] # Ensure column order\n",
        "        except Exception as e:\n",
        "            print(f\"RuntimeError: Error during HITS calculation: {e}\") # Log\n",
        "            raise RuntimeError(f\"Error during HITS calculation: {e}\")\n",
        "\n",
        "# --- 4. GradioApp Class (Single Responsibility: UI interaction) ---\n",
        "class GradioApp:\n",
        "    \"\"\"\n",
        "    Manages the Gradio user interface for the URL analysis application.\n",
        "    It composes CSVLoader, PageRankAnalyzer, and HITSGraphAnalyzer.\n",
        "    \"\"\"\n",
        "    def __init__(self, pagerank_csv_path: str, link_graph_csv_path: str):\n",
        "        \"\"\"\n",
        "        Initializes the GradioApp and attempts to load both CSV files at startup.\n",
        "        \"\"\"\n",
        "        self.pagerank_csv_path = pagerank_csv_path\n",
        "        self.link_graph_csv_path = link_graph_csv_path\n",
        "\n",
        "        self.pagerank_analyzer = None\n",
        "        self.hits_graph_analyzer = None\n",
        "        self.initial_load_status = [] # List to store status messages\n",
        "        self.full_pagerank_df = pd.DataFrame() # Store the full pagerank df for HITS analyzer\n",
        "\n",
        "        print(\"GradioApp initialization started.\") # Log\n",
        "        # Attempt to load PageRank data\n",
        "        try:\n",
        "            print(f\"Attempting to load PageRank data from {self.pagerank_csv_path}\") # Log\n",
        "            pagerank_loader = CSVLoader(self.pagerank_csv_path)\n",
        "            self.full_pagerank_df = pagerank_loader.load_data() # Load and store full df\n",
        "            self.pagerank_analyzer = PageRankAnalyzer(self.full_pagerank_df.copy()) # Pass a copy to analyzer\n",
        "            self.initial_load_status.append(f\"✅ PageRank data loaded from {self.pagerank_csv_path}\")\n",
        "        except (FileNotFoundError, IOError, ValueError) as e:\n",
        "            self.initial_load_status.append(f\"❌ Error loading PageRank CSV: {e}\")\n",
        "            self.pagerank_analyzer = None\n",
        "            print(f\"PageRank CSV load failed: {e}\") # Log\n",
        "\n",
        "        # Attempt to load Link Graph data for HITS\n",
        "        try:\n",
        "            print(f\"Attempting to load Link Graph data from {self.link_graph_csv_path}\") # Log\n",
        "            link_graph_loader = CSVLoader(self.link_graph_csv_path)\n",
        "            link_graph_df = link_graph_loader.load_data()\n",
        "            # Pass the full_pagerank_df to HITSGraphAnalyzer for folder depth merging\n",
        "            if not self.full_pagerank_df.empty:\n",
        "                self.hits_graph_analyzer = HITSGraphAnalyzer(link_graph_df, self.full_pagerank_df)\n",
        "            else:\n",
        "                self.initial_load_status.append(f\"⚠️ PageRank data empty, HITS may not have Folder_Depth info.\")\n",
        "                self.hits_graph_analyzer = HITSGraphAnalyzer(link_graph_df, pd.DataFrame(columns=['URL', 'Folder_Depth'])) # Pass empty df\n",
        "            self.initial_load_status.append(f\"✅ Link graph data loaded from {self.link_graph_csv_path}\")\n",
        "        except (FileNotFoundError, IOError, ValueError) as e:\n",
        "            self.initial_load_status.append(f\"❌ Error loading Link Graph CSV: {e}\")\n",
        "            self.hits_graph_analyzer = None\n",
        "            print(f\"Link Graph CSV load failed: {e}\") # Log\n",
        "        except RuntimeError as e:\n",
        "            self.initial_load_status.append(f\"❌ Error building graph for HITS: {e}\")\n",
        "            self.hits_graph_analyzer = None\n",
        "            print(f\"HITS graph build failed: {e}\") # Log\n",
        "\n",
        "        self.initial_load_status_str = \"\\n\".join(self.initial_load_status)\n",
        "        print(\"GradioApp initialization finished.\") # Log\n",
        "\n",
        "\n",
        "    def perform_analysis(self, analysis_type: str, depth_level: int, top_n: int):\n",
        "        \"\"\"\n",
        "        Performs the selected analysis (PageRank or HITS) and returns updates for the DataFrame\n",
        "        and status message. This function now prepares all updates in one go.\n",
        "        :param analysis_type: 'PageRank' or 'HITS'.\n",
        "        :param depth_level: Relevant for PageRank analysis.\n",
        "        :param top_n: Number of top results.\n",
        "        :return: A tuple containing a gr.update object for the dataframe and a status message string.\n",
        "        \"\"\"\n",
        "        print(f\"perform_analysis called with type: {analysis_type}, depth: {depth_level}, top_n: {top_n}\") # Log\n",
        "\n",
        "        # Default empty dataframe and generic headers\n",
        "        empty_df = pd.DataFrame()\n",
        "\n",
        "        status_msg = \"\"\n",
        "\n",
        "        results_df = empty_df\n",
        "        new_headers = [] # Will be set based on analysis_type\n",
        "\n",
        "        if analysis_type == 'PageRank':\n",
        "            if self.pagerank_analyzer is None:\n",
        "                status_msg = self.initial_load_status_str + \"\\n\\nCannot perform PageRank analysis: Data not loaded.\"\n",
        "                print(\"PageRank analyzer is not initialized.\") # Log\n",
        "                new_headers = ['URL', 'Folder_Depth', 'PageRank'] # Fallback\n",
        "            else:\n",
        "                try:\n",
        "                    results_df = self.pagerank_analyzer.find_worst_candidates(depth_level, top_n)\n",
        "                    if results_df.empty:\n",
        "                        status_msg = f\"No PageRank candidates found at Depth Level {depth_level} or after filtering.\"\n",
        "                        print(\"No PageRank candidates found.\") # Log\n",
        "                    else:\n",
        "                        status_msg = f\"Top {top_n} Worst PageRank Candidates at Depth Level {depth_level}:\"\n",
        "                        print(f\"PageRank analysis successful, {len(results_df)} results.\") # Log\n",
        "                    new_headers = ['URL', 'Folder_Depth', 'PageRank'] # Explicitly set correct headers\n",
        "                except Exception as e:\n",
        "                    status_msg = f\"An error occurred during PageRank analysis: {e}\"\n",
        "                    print(f\"Error during PageRank analysis: {e}\") # Log\n",
        "                    new_headers = ['URL', 'Folder_Depth', 'PageRank'] # Fallback\n",
        "\n",
        "        elif analysis_type == 'HITS':\n",
        "            if self.hits_graph_analyzer is None:\n",
        "                status_msg = self.initial_load_status_str + \"\\n\\nCannot perform HITS analysis: Graph data not loaded.\"\n",
        "                print(\"HITS graph analyzer is not initialized.\") # Log\n",
        "                new_headers = ['URL', 'Folder_Depth', 'Hub Score', 'Authority Score'] # Fallback\n",
        "            else:\n",
        "                try:\n",
        "                    results_df = self.hits_graph_analyzer.calculate_hits_scores()\n",
        "                    if results_df.empty:\n",
        "                        status_msg = \"No HITS scores calculated (empty graph).\"\n",
        "                        print(\"No HITS scores calculated.\") # Log\n",
        "                    else:\n",
        "                        results_df = results_df.head(top_n) # Apply top_n here\n",
        "                        status_msg = f\"Top {top_n} HITS Authority/Hub Score Candidates:\"\n",
        "                        print(f\"HITS analysis successful, {len(results_df)} results.\") # Log\n",
        "                    new_headers = ['URL', 'Folder_Depth', 'Hub Score', 'Authority Score'] # Explicitly set correct headers\n",
        "                except Exception as e:\n",
        "                    status_msg = f\"An error occurred during HITS analysis: {e}\"\n",
        "                    print(f\"Error during HITS analysis: {e}\") # Log\n",
        "                    new_headers = ['URL', 'Folder_Depth', 'Hub Score', 'Authority Score'] # Fallback\n",
        "\n",
        "        else:\n",
        "            status_msg = \"Invalid analysis type selected.\"\n",
        "            print(f\"Invalid analysis type: {analysis_type}\") # Log\n",
        "            new_headers = ['URL', 'Score1', 'Score2'] # Fallback for unknown type\n",
        "\n",
        "        # Determine datatype based on the number of headers\n",
        "        # This is important as Gradio's `datatype` needs to match the actual data being returned\n",
        "        if len(new_headers) == 3:\n",
        "            new_datatype = ['str', 'number', 'number']\n",
        "        elif len(new_headers) == 4:\n",
        "            new_datatype = ['str', 'number', 'number', 'number']\n",
        "        else:\n",
        "            new_datatype = ['str', 'number', 'number'] # Fallback\n",
        "\n",
        "        print(f\"Returning DataFrame update with headers: {new_headers}, datatype: {new_datatype}, and status: {status_msg}\") # Log\n",
        "        # Use gr.update() which is the universal way to update component properties\n",
        "        return gr.update(value=results_df, headers=new_headers, datatype=new_datatype, col_count=len(new_headers)), status_msg\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Launches the Gradio interface.\n",
        "        \"\"\"\n",
        "        print(\"Launching Gradio demo...\") # Log\n",
        "        with gr.Blocks(title=\"URL & HITS Analyzer\") as demo:\n",
        "            gr.Markdown(\n",
        "                \"\"\"\n",
        "                # URL & HITS Analyzer\n",
        "                This application loads PageRank and link graph data from your mounted Google Drive.\n",
        "                Please ensure your Google Drive is mounted correctly in Google Colab before running.\n",
        "                Select an analysis type to view results.\n",
        "                \"\"\"\n",
        "            )\n",
        "            # Display initial load status for both files\n",
        "            gr.Markdown(\"### File Loading Status:\")\n",
        "            gr.Markdown(self.initial_load_status_str)\n",
        "\n",
        "            # New section for explanation\n",
        "            gr.Markdown(\n",
        "                \"\"\"\n",
        "                ## Understanding Results for Website Re-architecture\n",
        "\n",
        "                This tool helps identify pages that are good candidates for improving your website's overall PageRank and structural authority.\n",
        "\n",
        "                **PageRank Analysis:**\n",
        "                * **Worst PageRank Candidates:** These are pages with low PageRank values, indicating they are not highly valued by the linking structure of your website (and potentially the broader web). Improving their internal linking (from high PageRank pages) or acquiring external backlinks can significantly boost their visibility and \"link juice\" distribution.\n",
        "\n",
        "                **HITS Analysis:**\n",
        "                The HITS algorithm provides a complementary view by identifying two types of influential pages:\n",
        "                * **High Authority Score:** These pages are recognized as definitive sources of information on a topic (i.e., they are *pointed to* by many good hubs). If a page has a high Authority score but relatively low PageRank, it suggests the content is valuable, but it might not be receiving enough PageRank flow. Focus on internal linking from high-PageRank pages and external link building to these pages.\n",
        "                * **High Hub Score:** These pages serve as excellent resource lists, pointing to many good authoritative pages. If a page has a high Hub score but low PageRank, it's a valuable navigational asset but isn't getting enough inbound link equity itself. Boosting the PageRank of such a hub (via internal/external links) will improve the \"link juice\" it passes to the authorities it links to.\n",
        "\n",
        "                By understanding these scores, you can strategically re-architect your website's internal linking, content, and external link building efforts to maximize PageRank and improve overall SEO performance.\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                analysis_type_radio = gr.Radio(\n",
        "                    [\"PageRank\", \"HITS\"],\n",
        "                    label=\"Select Analysis Type\",\n",
        "                    value=\"PageRank\", # Default to PageRank\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                # PageRank specific inputs (will only be used if PageRank is selected)\n",
        "                depth_level_input = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=10,\n",
        "                    step=1,\n",
        "                    value=1,\n",
        "                    label=\"Folder Depth Level (for PageRank)\",\n",
        "                    visible=True # Default to visible for PageRank initially\n",
        "                )\n",
        "                top_n_input = gr.Slider(\n",
        "                    minimum=1,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=5,\n",
        "                    label=\"Number of Top Candidates (N)\"\n",
        "                )\n",
        "\n",
        "            analyze_button = gr.Button(\"Perform Analysis\")\n",
        "\n",
        "            status_message_output = gr.Markdown(\"Analysis results will appear below.\")\n",
        "            output_dataframe = gr.Dataframe(\n",
        "                # Removed initial headers, datatype, and col_count for full dynamic behavior\n",
        "                # Gradio will infer these properties from the `value` provided by gr.update()\n",
        "                row_count=5,\n",
        "                interactive=True,\n",
        "                label=\"Analysis Results\",\n",
        "                visible=True\n",
        "            )\n",
        "\n",
        "            # Define a function to control visibility of depth_level_input\n",
        "            def update_depth_input_visibility(analysis_type):\n",
        "                if analysis_type == \"PageRank\":\n",
        "                    return gr.update(visible=True)\n",
        "                else:\n",
        "                    return gr.update(visible=False)\n",
        "\n",
        "            # Bind the radio button change event to update the visibility of the depth slider\n",
        "            analysis_type_radio.change(\n",
        "                fn=update_depth_input_visibility,\n",
        "                inputs=[analysis_type_radio],\n",
        "                outputs=[depth_level_input]\n",
        "            )\n",
        "\n",
        "            # Bind the button click to the main analysis function.\n",
        "            # This function now returns all the necessary updates directly.\n",
        "            analyze_button.click(\n",
        "                fn=self.perform_analysis,\n",
        "                inputs=[analysis_type_radio, depth_level_input, top_n_input],\n",
        "                outputs=[output_dataframe, status_message_output]\n",
        "            )\n",
        "\n",
        "        demo.launch()\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # --- IMPORTANT: For Google Colab, mount your drive first ---\n",
        "    from google.colab import drive\n",
        "    print(\"Attempting to mount Google Drive...\") # Log\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mount command executed.\") # Log\n",
        "\n",
        "    # Define the paths to your CSV files in Google Drive\n",
        "    pagerank_csv_path = '/content/drive/My Drive/master_july_2025/data/url_analysis_results.csv'\n",
        "    link_graph_csv_path = '/content/drive/My Drive/master_july_2025/data/link_graph_edges.csv'\n",
        "\n",
        "    print(f\"PageRank CSV Path: {pagerank_csv_path}\") # Log\n",
        "    print(f\"Link Graph CSV Path: {link_graph_csv_path}\") # Log\n",
        "\n",
        "    # Instantiate and run the Gradio application\n",
        "    app = GradioApp(pagerank_csv_path=pagerank_csv_path, link_graph_csv_path=link_graph_csv_path)\n",
        "    app.run()\n"
      ]
    }
  ]
}