{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xw4QDh41TRn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xlhbx_4z1WEx"
   },
   "source": [
    "# pip install -r requirements.txt first, before continuing with the rest of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SRE7N21AKE6"
   },
   "outputs": [],
   "source": [
    "# Installing necessary packages!\n",
    "# These should ideally be in requirements.txt and installed once for the environment.\n",
    "# But for a notebook that's meant to be self-contained for easy sharing/running,\n",
    "# it's common to keep them here.\n",
    "# !pip install trafilatura sentence-transformers torch pandas pyarrow duckdb scipy -q\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress a common warning from the sentence-transformers library\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",
    ")\n",
    "\n",
    "# Add the project root to the Python path so we can import from src\n",
    "# Adjust this path if your notebook is located differently relative to the 'src' folder\n",
    "# This assumes your project root is '/content/drive/My Drive/WebKnoGraph'\n",
    "project_root = \"/content/drive/My Drive/WebKnoGraph\"  # Explicitly set\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"sys.path: {sys.path}\")\n",
    "\n",
    "# Google Colab Drive Mount\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Check if already mounted before attempting to mount again (as in embeddings_ui.ipynb Cell 1)\n",
    "    if not os.path.exists(\"/content/drive/My Drive\"):\n",
    "        drive.mount(\"/content/drive/\")\n",
    "        print(\"Google Drive mounted successfully.\")\n",
    "    else:\n",
    "        print(\"Google Drive already mounted.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "\n",
    "# Import from your refactored backend and shared modules\n",
    "import gradio as gr\n",
    "import io\n",
    "import duckdb\n",
    "import pandas as pd  # Added pandas import\n",
    "from tqdm.auto import tqdm  # Added tqdm import for progress bar\n",
    "import traceback  # Added traceback for error logging\n",
    "\n",
    "# Specific imports for the Embedding Pipeline\n",
    "from src.backend.config.embeddings_config import EmbeddingConfig\n",
    "from src.backend.data.embedding_state_manager import EmbeddingStateManager\n",
    "from src.backend.data.embeddings_loader import DataLoader\n",
    "from src.backend.data.embeddings_saver import DataSaver\n",
    "from src.backend.utils.text_processing import TextExtractor\n",
    "from src.backend.utils.embedding_generation import EmbeddingGenerator\n",
    "from src.backend.services.embeddings_service import EmbeddingPipeline\n",
    "from src.shared.logging_config import (\n",
    "    ConsoleAndGradioLogger,\n",
    ")  # Using the updated generic logger\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQ5OJHuPALDo"
   },
   "outputs": [],
   "source": [
    "# File: embeddings_ui.ipynb - Cell 3\n",
    "def run_gradio_interface(input_path: str, output_path: str, batch_size: int):\n",
    "    \"\"\"Wires up all components and runs the pipeline, yielding UI updates.\"\"\"\n",
    "    log_stream = io.StringIO()\n",
    "    logger = ConsoleAndGradioLogger(\n",
    "        log_stream, logger_name=\"EmbeddingLogger\"\n",
    "    )  # Pass logger_name\n",
    "\n",
    "    config = EmbeddingConfig(\n",
    "        input_path=input_path, output_path=output_path, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Instantiate all our components\n",
    "    state_manager = EmbeddingStateManager(config.output_path, logger)\n",
    "    data_loader = DataLoader(config.input_path, logger)\n",
    "    text_extractor = TextExtractor()\n",
    "    embedding_generator = EmbeddingGenerator(config.model_name, logger)\n",
    "    data_saver = DataSaver(config.output_path, logger)\n",
    "\n",
    "    # Create a modified EmbeddingPipeline class within this scope that includes the fix\n",
    "    # Alternatively, you would apply this fix directly in src/backend/services/embeddings_service.py\n",
    "    class FixedEmbeddingPipeline(EmbeddingPipeline):\n",
    "        def run(self):\n",
    "            \"\"\"\n",
    "            Orchestrates the embedding generation pipeline.\n",
    "            Yields status updates for Gradio UI.\n",
    "            \"\"\"\n",
    "            self.logger.info(\"Starting embedding pipeline...\")\n",
    "            yield \"Status: Initializing Pipeline...\"\n",
    "\n",
    "            try:\n",
    "                processed_urls = self.state_manager.get_processed_urls()\n",
    "                self.logger.info(\n",
    "                    f\"Found {len(processed_urls)} URLs that have already been processed. They will be skipped.\"\n",
    "                )\n",
    "                yield f\"Status: Resuming, skipping {len(processed_urls)} already processed URLs.\"\n",
    "\n",
    "                self.logger.info(\"Querying for new pages to process...\")\n",
    "                data_iterator = self.data_loader.stream_unprocessed_data(\n",
    "                    processed_urls=processed_urls, batch_size=self.config.batch_size\n",
    "                )\n",
    "                yield \"Status: Loading new data...\"\n",
    "\n",
    "                total_processed_in_session = 0\n",
    "                for batch_num, df_batch_arrow in enumerate(data_iterator):\n",
    "                    if df_batch_arrow.num_rows == 0:\n",
    "                        self.logger.info(f\"Batch {batch_num + 1} is empty, skipping.\")\n",
    "                        yield f\"Status: Processed Batch {batch_num + 1}: Empty.\"\n",
    "                        continue\n",
    "\n",
    "                    self.logger.info(\n",
    "                        f\"Processing Batch {batch_num + 1} ({len(df_batch_arrow)} pages)...\"\n",
    "                    )\n",
    "\n",
    "                    # Convert PyArrow RecordBatch to Pandas DataFrame for modification\n",
    "                    df_batch = df_batch_arrow.to_pandas()\n",
    "\n",
    "                    # Extract clean text\n",
    "                    df_batch[\"clean_text\"] = [\n",
    "                        self.text_extractor.extract(\n",
    "                            html_content\n",
    "                        )  # Changed from .extract_text to .extract\n",
    "                        for html_content in tqdm(\n",
    "                            df_batch[\"Content\"],  # Use \"Content\" as per DataLoader\n",
    "                            desc=\"Extracting Text\",\n",
    "                            leave=False,\n",
    "                            unit=\"docs\",\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                    # Filter out pages where text extraction might have failed or resulted in empty strings\n",
    "                    original_count = len(df_batch)\n",
    "                    df_batch = df_batch[df_batch[\"clean_text\"].str.strip().astype(bool)]\n",
    "                    filtered_count = original_count - len(df_batch)\n",
    "                    if filtered_count > 0:\n",
    "                        self.logger.warning(\n",
    "                            f\"Filtered out {filtered_count} pages with no extractable text in Batch {batch_num + 1}.\"\n",
    "                        )\n",
    "\n",
    "                    if df_batch.empty:  # This .empty check is correct for the Pandas DataFrame after conversion\n",
    "                        self.logger.warning(\n",
    "                            f\"Batch {batch_num + 1} resulted in no extractable text after filtering, skipping.\"\n",
    "                        )\n",
    "                        yield f\"Status: Processed Batch {batch_num + 1}: No valid text extracted.\"\n",
    "                        continue\n",
    "\n",
    "                    # Generate embeddings\n",
    "                    self.logger.info(\n",
    "                        f\"Generating Embeddings for Batch {batch_num + 1}...\"\n",
    "                    )\n",
    "                    try:\n",
    "                        # Corrected: Use the actual method name 'generate' from EmbeddingGenerator\n",
    "                        df_batch[\"embedding\"] = self.embedding_generator.generate(\n",
    "                            df_batch[\"clean_text\"].tolist()\n",
    "                        ).tolist()\n",
    "                    except Exception as e:\n",
    "                        # Do not pass exc_info to ConsoleAndGradioLogger.error()\n",
    "                        # Instead, include the traceback in the message.\n",
    "                        error_message = f\"Error generating embeddings for Batch {batch_num + 1}: {e}\\n{traceback.format_exc()}\"\n",
    "                        self.logger.error(error_message)\n",
    "                        continue\n",
    "\n",
    "                    # --- ADDED SAVING LOGIC HERE ---\n",
    "                    # Save the generated embeddings after each batch\n",
    "                    self.data_saver.save_embeddings_batch(df_batch)  # Added this line\n",
    "                    # --- END OF ADDED LOGIC ---\n",
    "\n",
    "                    # The update_processed_urls method is not in the provided EmbeddingStateManager.\n",
    "                    # You will need to implement an 'update_processed_urls' method in\n",
    "                    # src/backend/data/embedding_state_manager.py if you intend to save the state.\n",
    "                    # For now, this line is commented out.\n",
    "                    # If you need resume functionality, ensure this method is implemented in EmbeddingStateManager:\n",
    "                    # def update_processed_urls(self, new_urls: list):\n",
    "                    #     \"\"\"\n",
    "                    #     Updates the persistent record of processed URLs.\n",
    "                    #     This method should append or merge `new_urls` with the existing state\n",
    "                    #     and save it to a durable storage (e.g., a JSON file or a dedicated DuckDB table).\n",
    "                    #     \"\"\"\n",
    "                    #     pass # Placeholder for actual implementation in embedding_state_manager.py\n",
    "\n",
    "                    # self.state_manager.update_processed_urls(df_batch[\"URL\"].tolist())\n",
    "                    total_processed_in_session += len(df_batch)\n",
    "\n",
    "                    yield f\"Status: Processed Batch {batch_num + 1} ({len(df_batch)} embeddings generated). Total in session: {total_processed_in_session}\"\n",
    "\n",
    "                self.logger.info(\"Embedding pipeline finished successfully.\")\n",
    "                yield \"Status: Pipeline Finished Successfully!\"\n",
    "\n",
    "            except Exception as e:\n",
    "                error_message = (\n",
    "                    f\"A critical pipeline error occurred: {e}\\n{traceback.format_exc()}\"\n",
    "                )\n",
    "                self.logger.error(error_message)\n",
    "                yield f\"Status: Critical Error! Check logs. Error: {e}\"\n",
    "                raise\n",
    "\n",
    "    pipeline = FixedEmbeddingPipeline(  # Use the fixed pipeline\n",
    "        config,\n",
    "        logger,\n",
    "        state_manager,\n",
    "        data_loader,\n",
    "        text_extractor,\n",
    "        embedding_generator,\n",
    "        data_saver,\n",
    "    )\n",
    "\n",
    "    final_status = \"Initializing...\"\n",
    "    for status in pipeline.run():\n",
    "        final_status = status\n",
    "        # Yield the current status and the full log content\n",
    "        yield status, log_stream.getvalue(), \"\"\n",
    "\n",
    "    # Generate final summary after the pipeline finishes\n",
    "    try:\n",
    "        # Ensure output_glob_path uses forward slashes for DuckDB even on Windows\n",
    "        output_glob_path = os.path.join(output_path, \"*.parquet\").replace(os.sep, \"/\")\n",
    "        total_embeddings = duckdb.query(\n",
    "            f\"SELECT COUNT(URL) FROM read_parquet('{output_glob_path}')\"\n",
    "        ).fetchone()[0]\n",
    "        summary_md = f\"### ✅ Pipeline Finished\\n\\n- **Final Status:** {final_status}\\n- **Total embeddings saved:** {total_embeddings}\\n- **Output location:** `{output_path}`\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not generate final summary. Error: {e}\")\n",
    "        summary_md = (\n",
    "            f\"### Pipeline Finished\\n\\n- Could not generate summary. Error: {e}\"\n",
    "        )\n",
    "\n",
    "    yield final_status, log_stream.getvalue(), summary_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KFXkND3AO9Z"
   },
   "outputs": [],
   "source": [
    "# File: embeddings_ui.ipynb - Cell 4\n",
    "# Build the Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# 🤖 Resumable Embedding Pipeline\")\n",
    "    gr.Markdown(\n",
    "        \"This tool reads HTML from Parquet files, cleans it, generates embeddings, and saves the results in batches. It can be stopped and resumed at any time.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## 1. Configuration\")\n",
    "            input_path_box = gr.Textbox(\n",
    "                label=\"Input Parquet Folder Path\", value=EmbeddingConfig.input_path\n",
    "            )\n",
    "            output_path_box = gr.Textbox(\n",
    "                label=\"Output Embeddings Directory Path\",\n",
    "                value=EmbeddingConfig.output_path,\n",
    "            )\n",
    "            batch_size_input = gr.Number(\n",
    "                minimum=1,\n",
    "                maximum=5,\n",
    "                value=EmbeddingConfig.batch_size,\n",
    "                step=1,\n",
    "                label=\"Batch Size\",\n",
    "                info=\"How many pages to process in memory at a time.\",\n",
    "            )\n",
    "            start_button = gr.Button(\n",
    "                \"🚀 Start/Resume Embedding Generation\", variant=\"primary\"\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"## 2. Status & Results\")\n",
    "            status_output = gr.Textbox(label=\"Current Status\", interactive=False)\n",
    "            log_output = gr.Textbox(\n",
    "                label=\"Detailed Logs\", interactive=False, lines=10, max_lines=20\n",
    "            )\n",
    "            summary_output = gr.Markdown(\"---\")\n",
    "\n",
    "    start_button.click(\n",
    "        fn=run_gradio_interface,\n",
    "        inputs=[input_path_box, output_path_box, batch_size_input],\n",
    "        outputs=[status_output, log_output, summary_output],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hvi9yg7QARNf"
   },
   "outputs": [],
   "source": [
    "# File: embeddings_ui.ipynb - Cell 5\n",
    "# --- Launch the Application ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        # It's better to mount once at the very start of the notebook\n",
    "        # or main.py. If it's already mounted, no need to force_remount unless necessary.\n",
    "        # Check if already mounted before attempting to mount again (as in embeddings_ui.ipynb Cell 1)\n",
    "        if not os.path.exists(\"/content/drive/My Drive\"):\n",
    "            drive.mount(\"/content/drive/\")\n",
    "            print(\"Google Drive mounted successfully.\")\n",
    "        else:\n",
    "            print(\"Google Drive already mounted.\")\n",
    "\n",
    "        demo.launch(debug=True, share=True)\n",
    "    except Exception as e:\n",
    "        print(\"Could not launch Gradio demo in this environment.\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWf9iM0KPxH9"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
