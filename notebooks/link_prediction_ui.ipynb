{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM9jrJJD6aQo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p3euqGm6bHd"
      },
      "source": [
        "# pip install -r requirements.txt first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vQuELkE6Chs"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_prediction_ui.ipynb - Cell 1\n",
        "\n",
        "# Installing necessary packages\n",
        "# !pip install -q torch torch-geometric pandas duckdb pyarrow networkx gradio -q\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress common warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",
        ")\n",
        "\n",
        "# Add the project root to the Python path\n",
        "project_root = \"/content/drive/My Drive/WebKnoGraph\"  # Explicitly set\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"Project root added to sys.path: {project_root}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"sys.path: {sys.path}\")\n",
        "\n",
        "# Google Colab Drive Mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "        drive.mount(\"/content/drive/\")\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    else:\n",
        "        print(\"Google Drive already mounted.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "# Import from your refactored backend and shared modules\n",
        "import gradio as gr\n",
        "import io\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Specific imports for Link Prediction Pipeline\n",
        "from src.backend.config.link_prediction_config import LinkPredictionConfig\n",
        "from src.backend.data.graph_dataloader import GraphDataLoader\n",
        "from src.backend.data.graph_processor import GraphDataProcessor\n",
        "from src.backend.models.graph_models import GraphSAGEModel\n",
        "from src.backend.utils.url_processing import URLProcessor\n",
        "from src.backend.services.graph_training_service import LinkPredictionTrainer\n",
        "from src.backend.services.recommendation_engine import RecommendationEngine\n",
        "from src.shared.logging_config import ConsoleAndGradioLogger\n",
        "\n",
        "print(\"All modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-jZ2PDT6PUi"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_prediction_ui.ipynb - Cell 2\n",
        "\n",
        "\n",
        "def get_all_nodes_for_dropdown():\n",
        "    \"\"\"\n",
        "    Dynamically loads node URLs from the saved model metadata (artifacts).\n",
        "    If artifacts are not found, it returns a message indicating training is needed.\n",
        "    \"\"\"\n",
        "    log_stream_dummy = io.StringIO()\n",
        "    logger_dummy = ConsoleAndGradioLogger(\n",
        "        log_stream_dummy, logger_name=\"DropdownLogger\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        config = LinkPredictionConfig()\n",
        "        model_metadata_path = config.node_mapping_path\n",
        "\n",
        "        if os.path.exists(model_metadata_path):\n",
        "            with open(model_metadata_path, \"r\") as f:\n",
        "                model_metadata = json.load(f)\n",
        "            if \"url_to_idx\" in model_metadata:\n",
        "                url_to_idx = model_metadata[\"url_to_idx\"]\n",
        "                return sorted(list(url_to_idx.keys()))\n",
        "            else:\n",
        "                logger_dummy.error(\"Model metadata is incomplete (missing url_to_idx).\")\n",
        "                return [\"Error: Model metadata is incomplete (missing url_to_idx).\"]\n",
        "        else:\n",
        "            logger_dummy.info(\n",
        "                \"Model artifacts not found. Run training first to generate artifacts.\"\n",
        "            )\n",
        "            return [\"Run training first to generate artifacts.\"]\n",
        "    except Exception as e:\n",
        "        logger_dummy.exception(f\"Could not load URLs for dropdown from artifacts: {e}\")\n",
        "        return [\n",
        "            f\"Could not load URLs from artifacts: {e}. Ensure Google Drive is mounted and artifacts exist.\"\n",
        "        ]\n",
        "\n",
        "\n",
        "def run_training_pipeline(\n",
        "    csv_path,\n",
        "    embeddings_path,\n",
        "    hidden_channels,\n",
        "    out_channels,\n",
        "    lr,\n",
        "    epochs,\n",
        "    progress=gr.Progress(track_tqdm=True),\n",
        "):\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(\n",
        "        log_stream, logger_name=\"LinkPredictionTrainerLogger\"\n",
        "    )\n",
        "\n",
        "    dropdown_choices = get_all_nodes_for_dropdown()\n",
        "\n",
        "    try:\n",
        "        yield \"Step 1/5: Initializing...\", log_stream.getvalue(), None, dropdown_choices\n",
        "        config = LinkPredictionConfig(\n",
        "            edge_csv_path=csv_path,\n",
        "            embeddings_dir_path=embeddings_path,\n",
        "            hidden_channels=int(hidden_channels),\n",
        "            out_channels=int(out_channels),\n",
        "            learning_rate=lr,\n",
        "            epochs=int(epochs),\n",
        "        )\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "        all_artifacts_exist = (\n",
        "            os.path.exists(config.model_state_path)\n",
        "            and os.path.exists(config.node_embeddings_path)\n",
        "            and os.path.exists(config.node_mapping_path)\n",
        "            and os.path.exists(config.edge_index_path)\n",
        "        )\n",
        "\n",
        "        if all_artifacts_exist:\n",
        "            status_message = (\n",
        "                \"âœ… All artifacts already exist. Skipping training and saving.\"\n",
        "            )\n",
        "            logger.info(status_message)\n",
        "            dropdown_choices = get_all_nodes_for_dropdown()\n",
        "            yield (\n",
        "                status_message,\n",
        "                log_stream.getvalue(),\n",
        "                pd.DataFrame(\n",
        "                    {\n",
        "                        \"Message\": [\n",
        "                            \"Artifacts found. You can now use the recommendation tab.\"\n",
        "                        ]\n",
        "                    },\n",
        "                    columns=[\"Message\"],\n",
        "                ),\n",
        "                dropdown_choices,\n",
        "            )\n",
        "            return\n",
        "\n",
        "        yield (\n",
        "            \"Step 2/5: Loading & processing data...\",\n",
        "            log_stream.getvalue(),\n",
        "            None,\n",
        "            dropdown_choices,\n",
        "        )\n",
        "        loader = GraphDataLoader(config, logger)\n",
        "        node_features_df, edge_list_df = loader.load_data()\n",
        "        processor = GraphDataProcessor(logger)\n",
        "        data, url_to_idx = processor.process(node_features_df, edge_list_df)\n",
        "\n",
        "        yield (\n",
        "            \"Step 3/5: Initializing model...\",\n",
        "            log_stream.getvalue(),\n",
        "            None,\n",
        "            dropdown_choices,\n",
        "        )\n",
        "        model = GraphSAGEModel(\n",
        "            in_channels=data.num_node_features,\n",
        "            hidden_channels=config.hidden_channels,\n",
        "            out_channels=config.out_channels,\n",
        "        )\n",
        "        trainer = LinkPredictionTrainer(model, data, config, logger)\n",
        "\n",
        "        yield (\n",
        "            \"Step 4/5: Training model...\",\n",
        "            log_stream.getvalue(),\n",
        "            None,\n",
        "            dropdown_choices,\n",
        "        )\n",
        "        for epoch, loss in progress.tqdm(\n",
        "            trainer.train(), total=config.epochs, desc=\"Training Model\"\n",
        "        ):\n",
        "            if epoch % 10 == 0 or epoch == 1:\n",
        "                logger.info(f\"Epoch {epoch}/{config.epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "        yield (\n",
        "            \"Step 5/5: Evaluating and saving artifacts...\",\n",
        "            log_stream.getvalue(),\n",
        "            None,\n",
        "            dropdown_choices,\n",
        "        )\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            final_node_embeddings = model(data.x, data.edge_index)\n",
        "\n",
        "        logger.info(f\"Saving model metadata to {config.node_mapping_path}\")\n",
        "        model_metadata = {\n",
        "            \"url_to_idx\": url_to_idx,\n",
        "            \"in_channels\": data.num_node_features,\n",
        "            \"hidden_channels\": config.hidden_channels,\n",
        "            \"out_channels\": config.out_channels,\n",
        "        }\n",
        "        with open(config.node_mapping_path, \"w\") as f:\n",
        "            json.dump(model_metadata, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Saving model weights to {config.model_state_path}\")\n",
        "        torch.save(model.state_dict(), config.model_state_path)\n",
        "        logger.info(f\"Saving final node embeddings to {config.node_embeddings_path}\")\n",
        "        torch.save(final_node_embeddings, config.node_embeddings_path)\n",
        "        logger.info(f\"Saving edge index to {config.edge_index_path}\")\n",
        "        torch.save(data.edge_index, config.edge_index_path)\n",
        "\n",
        "        final_status = \"âœ… Pipeline Finished Successfully!\"\n",
        "        logger.info(final_status)\n",
        "        dropdown_choices = get_all_nodes_for_dropdown()\n",
        "        yield (\n",
        "            final_status,\n",
        "            log_stream.getvalue(),\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"Message\": [\n",
        "                        \"Artifacts saved successfully. You can now use the recommendation tab.\"\n",
        "                    ]\n",
        "                },\n",
        "                columns=[\"Message\"],\n",
        "            ),\n",
        "            dropdown_choices,\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"A critical error occurred: {e}\")\n",
        "        dropdown_choices = get_all_nodes_for_dropdown()\n",
        "        yield (\n",
        "            \"Pipeline Failed\",\n",
        "            log_stream.getvalue(),\n",
        "            pd.DataFrame({\"Error\": [str(e)]}),\n",
        "            dropdown_choices,\n",
        "        )\n",
        "\n",
        "\n",
        "def run_recommendation_interface(source_url: str, min_depth: int, max_depth: int):\n",
        "    placeholder_message = \"Run training first to generate artifacts.\"\n",
        "    if source_url == placeholder_message or source_url.startswith(\"Error:\"):\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                \"Error\": [\n",
        "                    \"Please train the model first and select a valid URL from the dropdown. Current selection is a placeholder or error message.\"\n",
        "                ]\n",
        "            },\n",
        "            columns=[\"Error\"],\n",
        "        ), f\"Error: Selected source URL is a placeholder or invalid: '{source_url}'\"\n",
        "\n",
        "    if not source_url:\n",
        "        return None, \"Please select a source URL from the dropdown.\"\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(\n",
        "        log_stream, logger_name=\"RecommendationEngineLogger\"\n",
        "    )\n",
        "    config = LinkPredictionConfig()\n",
        "    url_processor = URLProcessor()\n",
        "    engine = RecommendationEngine(config, logger, url_processor)\n",
        "\n",
        "    if min_depth is None:\n",
        "        min_depth = 0\n",
        "    if max_depth is None:\n",
        "        max_depth = 100\n",
        "    if min_depth > max_depth:\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                \"Error\": [\n",
        "                    \"Minimum folder depth cannot be greater than maximum folder depth.\"\n",
        "                ]\n",
        "            },\n",
        "            columns=[\"Error\"],\n",
        "        ), \"Error: Minimum folder depth cannot be greater than maximum folder depth.\"\n",
        "\n",
        "    recommendations_df, error_msg = engine.get_recommendations(\n",
        "        source_url, top_n=20, min_folder_depth=min_depth, max_folder_depth=max_depth\n",
        "    )\n",
        "    if error_msg:\n",
        "        logger.error(error_msg)\n",
        "        return pd.DataFrame(\n",
        "            {\"Error\": [error_msg]}, columns=[\"Error\"]\n",
        "        ), log_stream.getvalue()\n",
        "\n",
        "    if recommendations_df is None or recommendations_df.empty:\n",
        "        logger.info(\"No recommendations found matching the specified filters.\")\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                \"Message\": [\n",
        "                    \"No recommendations found matching the specified filters. Try adjusting your depth range.\"\n",
        "                ]\n",
        "            },\n",
        "            columns=[\"Message\"],\n",
        "        ), log_stream.getvalue()\n",
        "\n",
        "    return recommendations_df, log_stream.getvalue()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvY2EiIt6SNZ"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_prediction_ui.ipynb - Cell 3\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ“ˆ GNN Link Prediction & Recommendation Engine\")\n",
        "    gr.Markdown(\n",
        "        \"First, use the 'Train Model' tab to process your data. Then, use the 'Get Link Recommendations' tab to get predictions for new, non-existent links.\"\n",
        "    )\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Train Model\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"## 1. Configuration\")\n",
        "                    with gr.Accordion(\"Data Paths\", open=True):\n",
        "                        edge_csv_path_input = gr.Textbox(\n",
        "                            label=\"Edge List CSV Path\",\n",
        "                            value=LinkPredictionConfig.edge_csv_path,\n",
        "                        )\n",
        "                        embeddings_dir_path_input = gr.Textbox(\n",
        "                            label=\"Embeddings Directory Path\",\n",
        "                            value=LinkPredictionConfig.embeddings_dir_path,\n",
        "                        )\n",
        "                    with gr.Accordion(\"Model Hyperparameters\", open=True):\n",
        "                        hidden_channels_input = gr.Number(\n",
        "                            label=\"Hidden Channels\",\n",
        "                            value=LinkPredictionConfig.hidden_channels,\n",
        "                        )\n",
        "                        out_channels_input = gr.Number(\n",
        "                            label=\"Output Embedding Size\",\n",
        "                            value=LinkPredictionConfig.out_channels,\n",
        "                        )\n",
        "                    with gr.Accordion(\"Training Parameters\", open=True):\n",
        "                        learning_rate_input = gr.Number(\n",
        "                            label=\"Learning Rate\",\n",
        "                            value=LinkPredictionConfig.learning_rate,\n",
        "                        )\n",
        "                        epochs_input = gr.Number(\n",
        "                            label=\"Training Epochs\", value=LinkPredictionConfig.epochs\n",
        "                        )\n",
        "                    start_button = gr.Button(\n",
        "                        \"Train Link Prediction Model\", variant=\"primary\"\n",
        "                    )\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"## 2. Training Status\")\n",
        "                    train_status_output = gr.Textbox(\n",
        "                        label=\"Current Status\", interactive=False\n",
        "                    )\n",
        "                    train_log_output = gr.Textbox(\n",
        "                        label=\"Pipeline Logs\", interactive=False, lines=15\n",
        "                    )\n",
        "                    train_results_output = gr.DataFrame(\n",
        "                        label=\"Training Completion Status\"\n",
        "                    )\n",
        "\n",
        "        with gr.TabItem(\"Get Link Recommendations\"):\n",
        "            gr.Markdown(\"## 1. Select a Source Page & Filters\")\n",
        "            gr.Markdown(\n",
        "                \"Choose a URL and the model will recommend top pages it should link to. (You must train the model on the tab to the left first).\"\n",
        "            )\n",
        "            with gr.Row():\n",
        "                source_url_dropdown = gr.Dropdown(\n",
        "                    label=\"Source URL\",\n",
        "                    choices=get_all_nodes_for_dropdown(),\n",
        "                    interactive=True,\n",
        "                )\n",
        "            with gr.Row():\n",
        "                min_folder_depth_input = gr.Number(\n",
        "                    label=\"Minimum Folder Depth\", value=0, precision=0\n",
        "                )\n",
        "                max_folder_depth_input = gr.Number(\n",
        "                    label=\"Maximum Folder Depth\", value=100, precision=0\n",
        "                )\n",
        "\n",
        "            recommend_button = gr.Button(\"Get Recommendations\", variant=\"primary\")\n",
        "            gr.Markdown(\"## 2. Results: High-Potential Missing Links\")\n",
        "            recommend_results_output = gr.DataFrame(\n",
        "                label=\"Top Link Recommendations\",\n",
        "                headers=[\"RECOMMENDED_URL\", \"SCORE\", \"FOLDER_DEPTH\"],\n",
        "            )\n",
        "            recommend_log_output = gr.Textbox(label=\"Logs\", interactive=False, lines=4)\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_training_pipeline,\n",
        "        inputs=[\n",
        "            edge_csv_path_input,\n",
        "            embeddings_dir_path_input,\n",
        "            hidden_channels_input,\n",
        "            out_channels_input,\n",
        "            learning_rate_input,\n",
        "            epochs_input,\n",
        "        ],\n",
        "        outputs=[\n",
        "            train_status_output,\n",
        "            train_log_output,\n",
        "            train_results_output,\n",
        "            source_url_dropdown,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    recommend_button.click(\n",
        "        fn=run_recommendation_interface,\n",
        "        inputs=[source_url_dropdown, min_folder_depth_input, max_folder_depth_input],\n",
        "        outputs=[recommend_results_output, recommend_log_output],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1tCyyce6UD6"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_prediction_ui.ipynb - Cell 4\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "\n",
        "        if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "            drive.mount(\"/content/drive/\")\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "        else:\n",
        "            print(\"Google Drive already mounted.\")\n",
        "\n",
        "        demo.launch(debug=True, share=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not launch Gradio demo in this environment: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
