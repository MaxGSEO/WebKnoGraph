{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 1: Install Required Libraries ---\n",
        "#\n",
        "!pip install -q torch torch-geometric pandas duckdb pyarrow networkx gradio"
      ],
      "metadata": {
        "id": "bcv9yJ83xbTq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 2: Import Libraries ---\n",
        "#\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "import logging\n",
        "import io\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from abc import ABC, abstractmethod\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "#\n",
        "# --- Step 3: Google Colab Drive Mount ---\n",
        "#\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in a Google Colab environment. Please ensure your data paths are correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "#\n",
        "# --- Step 4: Configuration & Logging Classes ---\n",
        "#\n",
        "\n",
        "@dataclass\n",
        "class LinkPredictionConfig:\n",
        "    \"\"\"Holds all configuration for the graph prediction pipeline.\"\"\"\n",
        "    # Input Paths\n",
        "    edge_csv_path: str = \"/content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\"\n",
        "    embeddings_dir_path: str = \"/content/drive/My Drive/master_july_2025/data/url_embeddings/\"\n",
        "\n",
        "    # Output Artifact Paths\n",
        "    output_dir: str = \"/content/drive/My Drive/master_july_2025/data/prediction_model/\"\n",
        "    model_state_path: str = os.path.join(output_dir, \"graphsage_link_predictor.pth\")\n",
        "    node_embeddings_path: str = os.path.join(output_dir, \"final_node_embeddings.pt\")\n",
        "    node_mapping_path: str = os.path.join(output_dir, \"model_metadata.json\") # Changed name for clarity\n",
        "    edge_index_path: str = os.path.join(output_dir, \"edge_index.pt\")\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    hidden_channels: int = 128\n",
        "    out_channels: int = 64\n",
        "    learning_rate: float = 0.01\n",
        "    epochs: int = 100\n",
        "\n",
        "class ILogger(ABC):\n",
        "    @abstractmethod\n",
        "    def info(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def error(self, message: str): pass\n",
        "\n",
        "class ConsoleAndGradioLogger(ILogger):\n",
        "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
        "        self._logger = logging.getLogger(\"GraphLogger\")\n",
        "        self._logger.setLevel(level)\n",
        "        if self._logger.hasHandlers():\n",
        "            self._logger.handlers.clear()\n",
        "        gradio_handler = logging.StreamHandler(log_output_stream)\n",
        "        gradio_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        self._logger.addHandler(gradio_handler)\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        self._logger.addHandler(console_handler)\n",
        "\n",
        "    def info(self, message: str): self._logger.info(message)\n",
        "    def error(self, message: str): self._logger.error(message)\n",
        "\n",
        "#\n",
        "# --- Step 5: OOP Component Classes ---\n",
        "#\n",
        "\n",
        "class GraphDataLoader:\n",
        "    def __init__(self, config: LinkPredictionConfig, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def load_data(self):\n",
        "        self.logger.info(\"Loading data using DuckDB...\")\n",
        "        try:\n",
        "            con = duckdb.connect()\n",
        "            all_nodes_query = f\"\"\"\n",
        "                (SELECT \"FROM\" AS url FROM read_csv_auto('{self.config.edge_csv_path}', header=true))\n",
        "                UNION\n",
        "                (SELECT \"TO\" AS url FROM read_csv_auto('{self.config.edge_csv_path}', header=true))\n",
        "            \"\"\"\n",
        "            embeddings_glob_path = os.path.join(self.config.embeddings_dir_path, '*.parquet')\n",
        "            node_features_query = f\"\"\"\n",
        "                WITH all_nodes AS ({all_nodes_query})\n",
        "                SELECT n.url, e.Embedding AS features\n",
        "                FROM all_nodes AS n\n",
        "                LEFT JOIN read_parquet('{embeddings_glob_path}') AS e ON n.url = e.URL\n",
        "            \"\"\"\n",
        "            node_features_df = con.execute(node_features_query).fetchdf()\n",
        "            edge_list_df = con.execute(f\"SELECT * FROM read_csv_auto('{self.config.edge_csv_path}', header=true)\").fetchdf()\n",
        "            self.logger.info(f\"Loaded {len(edge_list_df)} edges and {len(node_features_df)} unique nodes.\")\n",
        "            return node_features_df, edge_list_df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load data: {e}\")\n",
        "            raise\n",
        "\n",
        "class GraphDataProcessor:\n",
        "    def __init__(self, logger: ILogger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def process(self, node_features_df: pd.DataFrame, edge_list_df: pd.DataFrame):\n",
        "        self.logger.info(\"Processing data into tensors with neighbor feature inference...\")\n",
        "        url_to_features = pd.Series(node_features_df.features.values, index=node_features_df.url).to_dict()\n",
        "        adj = defaultdict(set)\n",
        "        for _, row in edge_list_df.iterrows():\n",
        "            adj[row['FROM']].add(row['TO'])\n",
        "            adj[row['TO']].add(row['FROM'])\n",
        "\n",
        "        imputed_features = {}\n",
        "        nodes_with_missing_features_count = 0\n",
        "\n",
        "        # Drop rows with no valid embeddings to find the feature dimension reliably\n",
        "        valid_features = node_features_df['features'].dropna()\n",
        "        if valid_features.empty:\n",
        "            raise ValueError(\"No nodes with features found. Cannot determine feature dimension.\")\n",
        "        feature_dim = len(valid_features.iloc[0])\n",
        "        self.logger.info(f\"Detected feature dimension: {feature_dim}\")\n",
        "\n",
        "        for url, features in url_to_features.items():\n",
        "            is_missing = pd.isna(features)\n",
        "            if (isinstance(is_missing, bool) and is_missing) or (hasattr(is_missing, 'any') and is_missing.any()):\n",
        "                nodes_with_missing_features_count += 1\n",
        "                neighbors = adj.get(url, set())\n",
        "                neighbor_features = [np.array(url_to_features.get(n), dtype=np.float32) for n in neighbors if url_to_features.get(n) is not None and not pd.isna(url_to_features.get(n)).any()]\n",
        "                if neighbor_features:\n",
        "                    imputed_features[url] = np.mean(neighbor_features, axis=0)\n",
        "                else:\n",
        "                    imputed_features[url] = np.zeros(feature_dim, dtype=np.float32)\n",
        "            else:\n",
        "                imputed_features[url] = np.array(features, dtype=np.float32)\n",
        "        if nodes_with_missing_features_count > 0:\n",
        "            self.logger.info(f\"Imputed features for {nodes_with_missing_features_count} nodes.\")\n",
        "\n",
        "        self.logger.info(\"Constructing final PyTorch tensors...\")\n",
        "        node_list = node_features_df['url'].tolist()\n",
        "        url_to_idx = {url: i for i, url in enumerate(node_list)}\n",
        "        final_feature_list = [imputed_features[url] for url in node_list]\n",
        "        x = torch.tensor(np.array(final_feature_list), dtype=torch.float)\n",
        "        source_indices = [url_to_idx.get(url) for url in edge_list_df['FROM']]\n",
        "        dest_indices = [url_to_idx.get(url) for url in edge_list_df['TO']]\n",
        "        edge_index = torch.tensor([source_indices, dest_indices], dtype=torch.long)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index)\n",
        "        self.logger.info(f\"Created PyG Data object with {data.num_nodes} nodes and {data.num_edges} edges.\")\n",
        "        return data, url_to_idx\n",
        "\n",
        "class GraphSAGEModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def predict_link(self, z, edge_label_index):\n",
        "        source_emb = z[edge_label_index[0]]\n",
        "        dest_emb = z[edge_label_index[1]]\n",
        "        return (source_emb * dest_emb).sum(dim=-1)\n",
        "\n",
        "class LinkPredictionTrainer:\n",
        "    def __init__(self, model: GraphSAGEModel, data: Data, config: LinkPredictionConfig, logger: ILogger):\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _get_negative_samples(self):\n",
        "        return torch.randint(0, self.data.num_nodes, (2, self.data.num_edges), dtype=torch.long)\n",
        "\n",
        "    def train(self):\n",
        "        edge_label_index = torch.cat([self.data.edge_index, self._get_negative_samples()], dim=1)\n",
        "        edge_label = torch.cat([torch.ones(self.data.num_edges), torch.zeros(self.data.num_edges)], dim=0)\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "            z = self.model(self.data.x, self.data.edge_index)\n",
        "            out = self.model.predict_link(z, edge_label_index)\n",
        "            loss = self.criterion(out, edge_label)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            yield epoch, loss.item()\n",
        "\n",
        "class RecommendationEngine:\n",
        "    \"\"\"Loads trained artifacts and provides link recommendations using a Top-K strategy.\"\"\"\n",
        "    def __init__(self, config: LinkPredictionConfig, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Artifacts will be loaded on the first request\n",
        "        self.model = None\n",
        "        self.node_embeddings = None\n",
        "        self.url_to_idx = None\n",
        "        self.idx_to_url = None\n",
        "        self.existing_edges = None\n",
        "\n",
        "    def load_artifacts(self):\n",
        "        \"\"\"Loads the trained model, embeddings, and mappings into memory.\"\"\"\n",
        "        if self.model is not None:\n",
        "            return True # Already loaded\n",
        "\n",
        "        self.logger.info(\"Loading trained artifacts for recommendations...\")\n",
        "        try:\n",
        "            # Load the metadata file which contains the model's \"blueprint\"\n",
        "            with open(self.config.node_mapping_path, 'r') as f:\n",
        "                model_metadata = json.load(f)\n",
        "\n",
        "            self.url_to_idx = model_metadata['url_to_idx']\n",
        "            in_channels = model_metadata['in_channels']\n",
        "            hidden_channels = model_metadata['hidden_channels']\n",
        "            out_channels = model_metadata['out_channels']\n",
        "\n",
        "            self.idx_to_url = {v: k for k, v in self.url_to_idx.items()}\n",
        "\n",
        "            # Load the final tensors\n",
        "            self.node_embeddings = torch.load(self.config.node_embeddings_path).to(self.device)\n",
        "            edge_index = torch.load(self.config.edge_index_path)\n",
        "            self.existing_edges = set(zip(edge_index[0].tolist(), edge_index[1].tolist()))\n",
        "\n",
        "            # Recreate model with the correct dimensions and load its state\n",
        "            self.model = GraphSAGEModel(in_channels, hidden_channels, out_channels)\n",
        "            self.model.load_state_dict(torch.load(self.config.model_state_path))\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval() # Set model to evaluation mode\n",
        "\n",
        "            self.logger.info(\"Artifacts loaded successfully.\")\n",
        "            return True\n",
        "        except FileNotFoundError:\n",
        "            self.logger.error(\"Could not find trained model artifacts. Please run the training pipeline first.\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"An error occurred while loading artifacts: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_recommendations(self, source_url: str, top_n: int = 20):\n",
        "        if not self.load_artifacts():\n",
        "            return None, \"Error: Trained model artifacts not found. Please run the training pipeline first.\"\n",
        "        if source_url not in self.url_to_idx:\n",
        "            return None, f\"Error: Source URL '{source_url}' not found in the graph's training data.\"\n",
        "\n",
        "        source_idx = self.url_to_idx[source_url]\n",
        "        num_nodes = len(self.url_to_idx)\n",
        "\n",
        "        # --- NEW \"Top-K\" LOGIC ---\n",
        "\n",
        "        # 1. Create candidate edges from the source to ALL other nodes\n",
        "        candidate_dest_indices = torch.arange(num_nodes, device=self.device)\n",
        "        candidate_source_indices = torch.full_like(candidate_dest_indices, fill_value=source_idx)\n",
        "        candidate_edge_index = torch.stack([candidate_source_indices, candidate_dest_indices])\n",
        "\n",
        "        # 2. Score all candidates at once\n",
        "        with torch.no_grad():\n",
        "            scores = self.model.predict_link(self.node_embeddings, candidate_edge_index)\n",
        "\n",
        "        # 3. Find the top K highest-scoring candidates (e.g., top 100)\n",
        "        # We get more than `top_n` to account for filtering out existing links.\n",
        "        k = min(num_nodes, top_n + 50)\n",
        "        top_scores, top_indices = torch.topk(scores, k=k)\n",
        "\n",
        "        # 4. Filter this small list to find novel recommendations\n",
        "        recommendations = []\n",
        "        for i in range(k):\n",
        "            dest_idx = top_indices[i].item()\n",
        "\n",
        "            # Stop if we've found enough recommendations\n",
        "            if len(recommendations) >= top_n:\n",
        "                break\n",
        "\n",
        "            # Check if the candidate is valid (not the source itself and not an existing link)\n",
        "            is_self_link = (dest_idx == source_idx)\n",
        "            is_existing_link = (source_idx, dest_idx) in self.existing_edges\n",
        "\n",
        "            if not is_self_link and not is_existing_link:\n",
        "                recommendations.append({\n",
        "                    \"RECOMMENDED_URL\": self.idx_to_url[dest_idx],\n",
        "                    \"SCORE\": torch.sigmoid(top_scores[i]).item() # Apply sigmoid to get probability\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(recommendations), None\n",
        "#\n",
        "# --- Main Gradio Application Functions ---\n",
        "#\n",
        "\n",
        "def run_training_pipeline(csv_path, embeddings_path, hidden_channels, out_channels, lr, epochs, progress=gr.Progress(track_tqdm=True)):\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(log_stream)\n",
        "    try:\n",
        "        yield \"Step 1/5: Initializing...\", log_stream.getvalue(), None\n",
        "        config = LinkPredictionConfig(\n",
        "            edge_csv_path=csv_path, embeddings_dir_path=embeddings_path,\n",
        "            hidden_channels=int(hidden_channels), out_channels=int(out_channels),\n",
        "            learning_rate=lr, epochs=int(epochs)\n",
        "        )\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "        yield \"Step 2/5: Loading & processing data...\", log_stream.getvalue(), None\n",
        "        loader = GraphDataLoader(config, logger)\n",
        "        node_features_df, edge_list_df = loader.load_data()\n",
        "        processor = GraphDataProcessor(logger)\n",
        "        data, url_to_idx = processor.process(node_features_df, edge_list_df)\n",
        "\n",
        "        yield \"Step 3/5: Initializing model...\", log_stream.getvalue(), None\n",
        "        model = GraphSAGEModel(in_channels=data.num_node_features, hidden_channels=config.hidden_channels, out_channels=config.out_channels)\n",
        "        trainer = LinkPredictionTrainer(model, data, config, logger)\n",
        "\n",
        "        yield \"Step 4/5: Training model...\", log_stream.getvalue(), None\n",
        "        for epoch, loss in progress.tqdm(trainer.train(), total=config.epochs, desc=\"Training Model\"):\n",
        "            if epoch % 10 == 0 or epoch == 1:\n",
        "                logger.info(f\"Epoch {epoch}/{config.epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "        yield \"Step 5/5: Evaluating and saving artifacts...\", log_stream.getvalue(), None\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            final_node_embeddings = model(data.x, data.edge_index)\n",
        "\n",
        "        # --- THIS IS THE FIX ---\n",
        "        # Save the model's architecture metadata along with the URL mapping\n",
        "        logger.info(f\"Saving model metadata to {config.node_mapping_path}\")\n",
        "        model_metadata = {\n",
        "            \"url_to_idx\": url_to_idx,\n",
        "            \"in_channels\": data.num_node_features,\n",
        "            \"hidden_channels\": config.hidden_channels,\n",
        "            \"out_channels\": config.out_channels\n",
        "        }\n",
        "        with open(config.node_mapping_path, 'w') as f:\n",
        "            json.dump(model_metadata, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Saving model weights to {config.model_state_path}\")\n",
        "        torch.save(model.state_dict(), config.model_state_path)\n",
        "        logger.info(f\"Saving final node embeddings to {config.node_embeddings_path}\")\n",
        "        torch.save(final_node_embeddings, config.node_embeddings_path)\n",
        "        logger.info(f\"Saving edge index to {config.edge_index_path}\")\n",
        "        torch.save(data.edge_index, config.edge_index_path)\n",
        "\n",
        "        final_status = \"âœ… Pipeline Finished Successfully!\"\n",
        "        logger.info(final_status)\n",
        "        yield final_status, log_stream.getvalue(), pd.DataFrame({\"Message\": [\"Artifacts saved successfully. You can now use the recommendation tab.\"]})\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"A critical error occurred: {e}\")\n",
        "        yield \"Pipeline Failed\", log_stream.getvalue(), pd.DataFrame({\"Error\": [str(e)]})\n",
        "\n",
        "\n",
        "def run_recommendation_interface(source_url: str):\n",
        "    if not source_url:\n",
        "        return None, \"Please select a source URL from the dropdown.\"\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(log_stream)\n",
        "    config = LinkPredictionConfig()\n",
        "    engine = RecommendationEngine(config, logger)\n",
        "    recommendations_df, error_msg = engine.get_recommendations(source_url, top_n=20)\n",
        "    if error_msg:\n",
        "        logger.error(error_msg)\n",
        "    return recommendations_df, log_stream.getvalue()\n",
        "\n",
        "def get_all_nodes_for_dropdown():\n",
        "    try:\n",
        "        config = LinkPredictionConfig()\n",
        "        if not os.path.exists(config.edge_csv_path):\n",
        "            return [\"Run training first to create edge list\"]\n",
        "        con = duckdb.connect()\n",
        "        nodes_df = con.execute(f\"\"\"\n",
        "            (SELECT \"FROM\" AS url FROM read_csv_auto('{config.edge_csv_path}', header=true))\n",
        "            UNION\n",
        "            (SELECT \"TO\" AS url FROM read_csv_auto('{config.edge_csv_path}', header=true))\n",
        "        \"\"\").fetchdf()\n",
        "        return sorted(nodes_df['url'].tolist())\n",
        "    except Exception as e:\n",
        "        return [f\"Could not load URLs: {e}\"]\n",
        "\n",
        "#\n",
        "# --- Gradio UI Definition ---\n",
        "#\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ“ˆ GNN Link Prediction & Recommendation Engine\")\n",
        "    gr.Markdown(\"First, use the 'Train Model' tab to process your data. Then, use the 'Get Link Recommendations' tab to get predictions for new, non-existent links.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Train Model\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"## 1. Configuration\")\n",
        "                    with gr.Accordion(\"Data Paths\", open=True):\n",
        "                        edge_csv_path_input = gr.Textbox(label=\"Edge List CSV Path\", value=LinkPredictionConfig.edge_csv_path)\n",
        "                        embeddings_dir_path_input = gr.Textbox(label=\"Embeddings Directory Path\", value=LinkPredictionConfig.embeddings_dir_path)\n",
        "                    with gr.Accordion(\"Model Hyperparameters\", open=True):\n",
        "                        hidden_channels_input = gr.Number(label=\"Hidden Channels\", value=LinkPredictionConfig.hidden_channels)\n",
        "                        out_channels_input = gr.Number(label=\"Output Embedding Size\", value=LinkPredictionConfig.out_channels)\n",
        "                    with gr.Accordion(\"Training Parameters\", open=True):\n",
        "                        learning_rate_input = gr.Number(label=\"Learning Rate\", value=LinkPredictionConfig.learning_rate)\n",
        "                        epochs_input = gr.Number(label=\"Training Epochs\", value=LinkPredictionConfig.epochs)\n",
        "                    start_button = gr.Button(\"Train Link Prediction Model\", variant=\"primary\")\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"## 2. Training Status\")\n",
        "                    train_status_output = gr.Textbox(label=\"Current Status\", interactive=False)\n",
        "                    train_log_output = gr.Textbox(label=\"Pipeline Logs\", interactive=False, lines=15)\n",
        "                    train_results_output = gr.DataFrame(label=\"Training Completion Status\")\n",
        "\n",
        "        with gr.TabItem(\"Get Link Recommendations\"):\n",
        "            gr.Markdown(\"## 1. Select a Source Page\")\n",
        "            gr.Markdown(\"Choose a URL and the model will recommend top pages it should link to. (You must train the model on the tab to the left first).\")\n",
        "            with gr.Row():\n",
        "                source_url_dropdown = gr.Dropdown(label=\"Source URL\", choices=get_all_nodes_for_dropdown(), interactive=True)\n",
        "            recommend_button = gr.Button(\"Get Recommendations\", variant=\"primary\")\n",
        "            gr.Markdown(\"## 2. Results: High-Potential Missing Links\")\n",
        "            recommend_results_output = gr.DataFrame(label=\"Top 20 Link Recommendations\", headers=[\"RECOMMENDED_URL\", \"SCORE\"])\n",
        "            recommend_log_output = gr.Textbox(label=\"Logs\", interactive=False, lines=4)\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_training_pipeline,\n",
        "        inputs=[edge_csv_path_input, embeddings_dir_path_input, hidden_channels_input, out_channels_input, learning_rate_input, epochs_input],\n",
        "        outputs=[train_status_output, train_log_output, train_results_output]\n",
        "    )\n",
        "\n",
        "    recommend_button.click(\n",
        "        fn=run_recommendation_interface,\n",
        "        inputs=[source_url_dropdown],\n",
        "        outputs=[recommend_results_output, recommend_log_output]\n",
        "    )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive/', force_remount=True)\n",
        "        demo.launch(debug=True, share=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not launch Gradio demo in this environment: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "HSVNtFiVlsLS",
        "outputId": "810b77f2-0eae-4eea-9958-6664ae9946c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1908370964>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# --- Step 1: Install Required Libraries ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q torch torch-geometric pandas duckdb pyarrow networkx gradio'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mmake_file\u001b[0;34m(name, hash, size_str)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmake_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackagePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mPurePath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPureWindowsPath\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mPurePosixPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# right flavour.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    500\u001b[0m                         \u001b[0;34m\"object returning str, not %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                         % type(a))\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mparse_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maltsep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maltsep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mdrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}