{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfv9lZ4b2ERB"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgkbjrLDlcLJ"
   },
   "source": [
    "# pip install -r requirements.txt first, before continuing with the rest of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7FCmsKPKA__"
   },
   "outputs": [],
   "source": [
    "# Installing necessary packages!\n",
    "# These should ideally be in requirements.txt and installed once for the environment.\n",
    "# But for a notebook that's meant to be self-contained for easy sharing/running,\n",
    "# it's common to keep them here.\n",
    "# !pip install pandas pyarrow duckdb gradio requests beautifulsoup4 lxml tqdm -q\n",
    "# !pip install fireducks\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- IMPORTANT CHANGE HERE ---\n",
    "# Define your Google Drive root path where 'master_july_2025' folder is located.\n",
    "# This assumes 'master_july_2025' is directly inside 'My Drive'.\n",
    "# If 'master_july_2025' is inside another subfolder, adjust this path.\n",
    "# The project root should be the directory *containing* the 'src' folder.\n",
    "google_drive_base_path = \"/content/drive/My Drive/WebKnoGraph/\"\n",
    "\n",
    "# Now, define the project_root to be this specific path\n",
    "# This assumes the structure is /content/drive/My Drive/WebKnoGraph/src/...\n",
    "# If src is directly under WebKnoGraph, then WebKnoGraph is the correct root to add to path\n",
    "project_root = os.path.abspath(google_drive_base_path)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# This allows imports like 'from src.backend...' to work if src is directly under project_root\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"sys.path: {sys.path}\")\n",
    "\n",
    "\n",
    "# Google Colab Drive Mount (moved here from the main code)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Check if already mounted before attempting to mount again\n",
    "    if not os.path.exists(\"/content/drive/My Drive\"):\n",
    "        drive.mount(\"/content/drive/\")\n",
    "        print(\"Google Drive mounted successfully.\")\n",
    "    else:\n",
    "        print(\"Google Drive already mounted.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "\n",
    "# Import from your refactored backend and shared modules\n",
    "import gradio as gr\n",
    "import io\n",
    "import duckdb\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Specific imports from your project structure\n",
    "# Assuming your structure is project_root/src/backend/...\n",
    "try:\n",
    "    from src.backend.config.crawler_config import CrawlerConfig\n",
    "    from src.backend.utils.strategies import (\n",
    "        VisitedUrlManager,\n",
    "        BFSCrawlingStrategy,\n",
    "        DFSCrawlingStrategy,\n",
    "    )\n",
    "    from src.backend.data.repositories import CrawlStateRepository\n",
    "    from src.backend.utils.http import HttpClient\n",
    "    from src.backend.utils.url import UrlFilter, LinkExtractor\n",
    "    from src.backend.services.crawler_service import WebCrawler\n",
    "    from src.shared.logging_config import ConsoleAndGradioLogger\n",
    "\n",
    "    print(\"All modules imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import failed: {e}\")\n",
    "    print(\n",
    "        \"Please ensure your project structure under Google Drive is correct and matches the import paths.\"\n",
    "    )\n",
    "    print(f\"Expected project root: {project_root}\")\n",
    "    print(\n",
    "        \"Check if 'src' directory exists directly under the project root and contains the necessary subdirectories (backend, shared).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKt4uwh41ML4"
   },
   "outputs": [],
   "source": [
    "def run_gradio_crawler_interface(\n",
    "    initial_start_url: str,\n",
    "    allowed_path_segment: str,\n",
    "    crawling_strategy_type: str,\n",
    "    state_db_path_input: str,\n",
    "    parquet_path_input: str,\n",
    "    max_pages_to_crawl: int,\n",
    "):\n",
    "    log_stream = io.StringIO()\n",
    "    logger = ConsoleAndGradioLogger(log_stream)\n",
    "\n",
    "    try:\n",
    "        base_domain = urlparse(initial_start_url).netloc\n",
    "        if not base_domain:\n",
    "            raise ValueError(\"Invalid Initial Start URL.\")\n",
    "\n",
    "        config = CrawlerConfig(\n",
    "            initial_start_url=initial_start_url,\n",
    "            allowed_path_segment=allowed_path_segment,\n",
    "            state_db_path=state_db_path_input,\n",
    "            parquet_path=parquet_path_input,\n",
    "            max_pages_to_crawl=max_pages_to_crawl,\n",
    "            base_domain=base_domain,\n",
    "        )\n",
    "        os.makedirs(config.parquet_path, exist_ok=True)\n",
    "        yield (\n",
    "            \"Initializing...\",\n",
    "            log_stream.getvalue(),\n",
    "            \"### Save Events Log\\n\\n- Waiting for first save event...\",\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "        state_repository = CrawlStateRepository(config.state_db_path, logger)\n",
    "        visited_manager = VisitedUrlManager()\n",
    "\n",
    "        logger.info(\"Rebuilding visited set from existing Parquet data...\")\n",
    "        try:\n",
    "            parquet_glob_path = os.path.join(config.parquet_path, \"**\", \"*.parquet\")\n",
    "            if os.sep != \"/\":\n",
    "                parquet_glob_path = parquet_glob_path.replace(os.sep, \"/\")\n",
    "            visited_urls_df = duckdb.query(\n",
    "                f\"SELECT DISTINCT URL FROM read_parquet('{parquet_glob_path}')\"\n",
    "            ).to_df()\n",
    "            for url in visited_urls_df[\"URL\"]:\n",
    "                visited_manager.add(url)\n",
    "            logger.info(f\"Rebuilt visited set with {visited_manager.size()} URLs.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                f\"Could not rebuild visited set from Parquet (may be a new crawl or empty directory): {e}\"\n",
    "            )\n",
    "\n",
    "        strategy_class = (\n",
    "            BFSCrawlingStrategy\n",
    "            if crawling_strategy_type == \"BFS\"\n",
    "            else DFSCrawlingStrategy\n",
    "        )\n",
    "        crawling_strategy = strategy_class(visited_manager, logger)\n",
    "\n",
    "        loaded_frontier = state_repository.load_frontier()\n",
    "        unvisited_frontier = [\n",
    "            info for info in loaded_frontier if not visited_manager.contains(info[0])\n",
    "        ]\n",
    "\n",
    "        if unvisited_frontier:\n",
    "            crawling_strategy.prime_with_frontier(unvisited_frontier)\n",
    "            logger.info(f\"Primed frontier with {len(unvisited_frontier)} URLs from DB.\")\n",
    "        elif not visited_manager.contains(config.initial_start_url):\n",
    "            crawling_strategy.add_links([(config.initial_start_url, 0)])\n",
    "            logger.info(f\"Added initial URL {config.initial_start_url} to frontier.\")\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"Initial URL {config.initial_start_url} already visited. No new URLs to start with from DB or initial URL.\"\n",
    "            )\n",
    "\n",
    "        http_client = HttpClient(config, logger)\n",
    "        url_filter = UrlFilter(config.allowed_path_segment, config.base_domain)\n",
    "        link_extractor = LinkExtractor(url_filter, config.allowed_query_params)\n",
    "\n",
    "        crawler = WebCrawler(\n",
    "            config,\n",
    "            crawling_strategy,\n",
    "            state_repository,\n",
    "            http_client,\n",
    "            url_filter,\n",
    "            link_extractor,\n",
    "            logger,\n",
    "        )\n",
    "\n",
    "        final_status = \"\"\n",
    "        save_events_log = [\"### Save Events Log\"]\n",
    "\n",
    "        for event in crawler.crawl():\n",
    "            status_msg = event.get(\"status\")\n",
    "            save_event = event.get(\"save_event\")\n",
    "\n",
    "            final_status = status_msg\n",
    "            if save_event:\n",
    "                save_events_log.append(f\"- {save_event}\")\n",
    "            yield status_msg, log_stream.getvalue(), \"\\n\".join(save_events_log), \"\"\n",
    "\n",
    "        logger.info(\"Generating final summary from Parquet data...\")\n",
    "        final_save_events = \"\\n\".join(save_events_log)\n",
    "        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Crawled Data Location**: `{config.parquet_path}`\"\n",
    "        try:\n",
    "            parquet_glob_path = os.path.join(config.parquet_path, \"**\", \"*.parquet\")\n",
    "            if os.sep != \"/\":\n",
    "                parquet_glob_path = parquet_glob_path.replace(os.sep, \"/\")\n",
    "            summary_df = duckdb.query(\n",
    "                f\"SELECT CASE WHEN Status_Code >= 200 AND Status_Code < 300 THEN 'Success (Content Saved)' WHEN Status_Code >= 300 AND Status_Code < 400 THEN 'Redirect' ELSE 'Error / Other' END AS Category, COUNT(*) as Total FROM read_parquet('{parquet_glob_path}') GROUP BY Category ORDER BY Total DESC\"\n",
    "            ).to_df()\n",
    "            total_urls = summary_df[\"Total\"].sum()\n",
    "            summary_md += f\"\\n- **Total URLs in Parquet Dataset**: {total_urls}\\n\\n### Crawl Summary by Category\\n\\n\"\n",
    "            summary_md += summary_df.to_markdown(index=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not generate summary from Parquet: {e}\")\n",
    "            summary_md += \"\\n\\n**Could not generate summary from Parquet data.**\"\n",
    "\n",
    "        yield final_status, log_stream.getvalue(), final_save_events, summary_md\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"A critical error occurred: {e}\")\n",
    "        yield \"Crawl Failed!\", log_stream.getvalue(), \"\", f\"**Error:** {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSU2jxlY1ML6"
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ðŸ•¸ï¸ Memory-Optimized Parquet Web Crawler\")\n",
    "    gr.Markdown(\n",
    "        \"This crawler saves data to a partitioned Parquet dataset and uses SQLite only to manage the crawl state.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Crawler Configuration\")\n",
    "            initial_url_input = gr.Textbox(\n",
    "                label=\"Initial Start URL\", value=CrawlerConfig.initial_start_url\n",
    "            )\n",
    "            allowed_path_input = gr.Textbox(\n",
    "                label=\"Allowed Path Segment\", value=CrawlerConfig.allowed_path_segment\n",
    "            )\n",
    "            crawling_strategy_radio = gr.Radio(\n",
    "                choices=[\"BFS\", \"DFS\"], label=\"Crawling Strategy\", value=\"BFS\"\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                - **BFS (Breadth-First Search)**: Ideal for performing a wide crawl across a website, exploring all links at the current depth level before moving to the next.\n",
    "                - **DFS (Depth-First Search)**: More suitable for targeting specific folders or branches of a website, exploring as deeply as possible along one path before backtracking.\n",
    "                \"\"\"\n",
    "            )\n",
    "            max_pages_input = gr.Number(\n",
    "                label=\"Maximum Pages to Crawl (per session)\",\n",
    "                value=CrawlerConfig.max_pages_to_crawl,\n",
    "                minimum=1,\n",
    "                step=100,\n",
    "            )\n",
    "            gr.Markdown(\"### Storage Paths\")\n",
    "            state_db_path_input = gr.Textbox(\n",
    "                label=\"Crawl State DB Path (SQLite)\", value=CrawlerConfig.state_db_path\n",
    "            )\n",
    "            parquet_path_input = gr.Textbox(\n",
    "                label=\"Crawled Data Path (Parquet)\", value=CrawlerConfig.parquet_path\n",
    "            )\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"## Actions and Status\")\n",
    "            start_button = gr.Button(\"ðŸš€ Start Crawl\", variant=\"primary\")\n",
    "            status_message_output = gr.Textbox(\n",
    "                label=\"Status Message\", interactive=False\n",
    "            )\n",
    "            logs_output = gr.Textbox(\n",
    "                label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20\n",
    "            )\n",
    "            with gr.Row():\n",
    "                save_events_output = gr.Markdown(\"### Save Events Log\")\n",
    "                summary_output = gr.Markdown(\"---\")\n",
    "\n",
    "    start_button.click(\n",
    "        fn=run_gradio_crawler_interface,\n",
    "        inputs=[\n",
    "            initial_url_input,\n",
    "            allowed_path_input,\n",
    "            crawling_strategy_radio,\n",
    "            state_db_path_input,\n",
    "            parquet_path_input,\n",
    "            max_pages_input,\n",
    "        ],\n",
    "        outputs=[\n",
    "            status_message_output,\n",
    "            logs_output,\n",
    "            save_events_output,\n",
    "            summary_output,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}