{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfHzWjmH3vM_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sknmac1N3v_S"
      },
      "source": [
        "# pip install -r requirements.txt first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8LvLA27rcjJ"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_crawler_ui.ipynb - Cell 1\n",
        "\n",
        "# Installing necessary packages\n",
        "# !pip install gradio pandas requests tqdm beautifulsoup4 lxml -q\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress common warnings if desired\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",
        ")\n",
        "\n",
        "# Add the project root to the Python path\n",
        "project_root = \"/content/drive/My Drive/WebKnoGraph\"  # Explicitly set\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"Project root added to sys.path: {project_root}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"sys.path: {sys.path}\")\n",
        "\n",
        "# Google Colab Drive Mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "        drive.mount(\"/content/drive/\")\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    else:\n",
        "        print(\"Google Drive already mounted.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "\n",
        "# Import from your refactored backend and shared modules\n",
        "import gradio as gr\n",
        "import io\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Specific imports for the Link Graph Extractor\n",
        "from src.backend.config.link_crawler_config import (\n",
        "    LinkCrawlerConfig,\n",
        ")  # Using the new config\n",
        "from src.backend.data.link_graph_repository import (\n",
        "    LinkGraphStateManager,\n",
        ")  # Using the new state manager\n",
        "from src.backend.utils.strategies import (\n",
        "    VisitedUrlManager,\n",
        "    BFSCrawlingStrategy,\n",
        "    DFSCrawlingStrategy,\n",
        ")  # Reusing strategies\n",
        "from src.backend.utils.http import HttpClient  # Reusing HttpClient\n",
        "from src.backend.utils.link_url import (\n",
        "    LinkUrlFilter,\n",
        "    LinkExtractorForEdges,\n",
        ")  # Using the new URL components\n",
        "from src.backend.services.link_crawler_service import (\n",
        "    EdgeCrawler,\n",
        ")  # Using the new orchestrator\n",
        "from src.shared.logging_config import ConsoleAndGradioLogger  # Using the generic logger\n",
        "\n",
        "print(\"All modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1gRjU7hrfMT"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_crawler_ui.ipynb - Cell 2\n",
        "\n",
        "\n",
        "def run_edge_crawler_interface(\n",
        "    initial_start_url: str,\n",
        "    crawling_scope_path: str,\n",
        "    saving_scope_path: str,\n",
        "    crawling_strategy_type: str,\n",
        "    state_db_path_input: str,\n",
        "    edge_list_path_input: str,\n",
        "    max_pages_to_crawl: int,\n",
        "):\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(\n",
        "        log_stream, logger_name=\"EdgeCrawlerLogger\"\n",
        "    )  # Pass logger_name\n",
        "\n",
        "    try:\n",
        "        base_domain = urlparse(initial_start_url).netloc\n",
        "        if not base_domain:\n",
        "            raise ValueError(\"Invalid Initial Start URL.\")\n",
        "\n",
        "        config = LinkCrawlerConfig(  # Using new config class\n",
        "            initial_start_url=initial_start_url,\n",
        "            crawling_scope_path=crawling_scope_path,\n",
        "            saving_scope_path=saving_scope_path,\n",
        "            state_db_path=state_db_path_input,\n",
        "            edge_list_path=edge_list_path_input,\n",
        "            max_pages_to_crawl=max_pages_to_crawl,\n",
        "            base_domain=base_domain,\n",
        "        )\n",
        "        os.makedirs(\n",
        "            os.path.dirname(config.edge_list_path), exist_ok=True\n",
        "        )  # Ensure output dir exists\n",
        "        yield \"Initializing...\", log_stream.getvalue(), \"\"\n",
        "\n",
        "        state_manager = LinkGraphStateManager(\n",
        "            config.state_db_path, config.edge_list_path, logger\n",
        "        )  # Using new state manager\n",
        "        visited_manager = VisitedUrlManager()\n",
        "\n",
        "        logger.info(\"Rebuilding visited set from existing edge list CSV...\")\n",
        "        # Load visited URLs from CSV using the state manager\n",
        "        rebuilt_visited_urls = state_manager.load_visited_from_edges()\n",
        "        for url in rebuilt_visited_urls:\n",
        "            visited_manager.add(url)\n",
        "        if rebuilt_visited_urls:\n",
        "            logger.info(\n",
        "                f\"Rebuilt visited set with {visited_manager.size()} URLs from CSV.\"\n",
        "            )\n",
        "        else:\n",
        "            logger.warning(\n",
        "                \"No previously processed edges found. Starting with a fresh visited set.\"\n",
        "            )\n",
        "\n",
        "        if crawling_strategy_type == \"BFS\":\n",
        "            crawling_strategy = BFSCrawlingStrategy(\n",
        "                visited_manager, logger\n",
        "            )  # Reusing BFSCrawlingStrategy\n",
        "        else:\n",
        "            crawling_strategy = DFSCrawlingStrategy(\n",
        "                visited_manager, logger\n",
        "            )  # Reusing DFSCrawlingStrategy\n",
        "\n",
        "        loaded_frontier = state_manager.load_frontier()\n",
        "        unvisited_frontier = [\n",
        "            info for info in loaded_frontier if not visited_manager.contains(info[0])\n",
        "        ]\n",
        "\n",
        "        if unvisited_frontier:\n",
        "            crawling_strategy.prime_with_frontier(unvisited_frontier)\n",
        "            logger.info(f\"Primed frontier with {len(unvisited_frontier)} URLs from DB.\")\n",
        "        elif not visited_manager.contains(config.initial_start_url):\n",
        "            crawling_strategy.add_links([(config.initial_start_url, 0)])\n",
        "            logger.info(f\"Added initial URL {config.initial_start_url} to frontier.\")\n",
        "        else:\n",
        "            logger.info(\n",
        "                f\"Initial URL {config.initial_start_url} already visited. No new URLs to start with from DB or initial URL.\"\n",
        "            )\n",
        "\n",
        "        url_filter = LinkUrlFilter(\n",
        "            config.crawling_scope_path, config.base_domain\n",
        "        )  # Using new LinkUrlFilter\n",
        "        link_extractor = LinkExtractorForEdges(\n",
        "            url_filter\n",
        "        )  # Using new LinkExtractorForEdges\n",
        "        # HttpClient expects a config object that has 'user_agents', 'max_retries_request', 'request_timeout', 'min_request_delay' attributes. LinkCrawlerConfig has these.\n",
        "        crawler = EdgeCrawler(\n",
        "            config,\n",
        "            crawling_strategy,\n",
        "            state_manager,\n",
        "            HttpClient(config, logger),\n",
        "            link_extractor,\n",
        "            logger,\n",
        "        )  # Using new EdgeCrawler\n",
        "\n",
        "        final_status = \"\"\n",
        "        for status_msg in crawler.crawl():\n",
        "            final_status = status_msg\n",
        "            yield status_msg, log_stream.getvalue(), \"\"\n",
        "\n",
        "        logger.info(\"Generating final summary from CSV file...\")\n",
        "        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Edge List Location**: `{config.edge_list_path}`\"\n",
        "        try:\n",
        "            if os.path.exists(config.edge_list_path):\n",
        "                edge_df = pd.read_csv(config.edge_list_path)\n",
        "                num_edges = len(edge_df)\n",
        "                num_nodes = len(pd.concat([edge_df[\"FROM\"], edge_df[\"TO\"]]).unique())\n",
        "                summary_md += f\"\\n- **Total Unique Pages (Nodes):** {num_nodes}\\n- **Total Links (Edges):** {num_edges}\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Could not generate summary from CSV: {e}\")\n",
        "            summary_md += \"\\n\\n**Could not generate summary from CSV file.**\"\n",
        "\n",
        "        yield final_status, log_stream.getvalue(), summary_md\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"A critical error occurred: {e}\")\n",
        "        yield \"Crawl Failed!\", log_stream.getvalue(), f\"**Error:** {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nISLHL5urhAD"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_crawler_ui.ipynb - Cell 3\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üï∏Ô∏è Link Graph Extractor\")\n",
        "    gr.Markdown(\n",
        "        \"This tool crawls a website to produce a simple `FROM, TO` list of all hyperlinks, saved as a CSV file.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Configuration\")\n",
        "            initial_url_input = gr.Textbox(\n",
        "                label=\"Initial Start URL\", value=LinkCrawlerConfig.initial_start_url\n",
        "            )  # Uses LinkCrawlerConfig\n",
        "            max_pages_input = gr.Number(\n",
        "                label=\"Maximum Pages to Process\",\n",
        "                value=LinkCrawlerConfig.max_pages_to_crawl,\n",
        "                minimum=1,\n",
        "                step=100,\n",
        "            )  # Uses LinkCrawlerConfig\n",
        "            crawling_strategy_radio = gr.Radio(\n",
        "                choices=[\"BFS\", \"DFS\"], label=\"Crawling Strategy\", value=\"BFS\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### üìú Scopes\")\n",
        "            crawling_scope_path_input = gr.Textbox(\n",
        "                label=\"Crawling Scope Path\",\n",
        "                value=LinkCrawlerConfig.crawling_scope_path,\n",
        "                info=\"The 'playground'. Set to '/' to explore the entire site.\",\n",
        "            )  # Uses LinkCrawlerConfig\n",
        "            saving_scope_path_input = gr.Textbox(\n",
        "                label=\"Saving Scope Path\",\n",
        "                value=LinkCrawlerConfig.saving_scope_path,\n",
        "                info=\"The 'rulebook'. Only save links where FROM and TO are in this path.\",\n",
        "            )  # Uses LinkCrawlerConfig\n",
        "\n",
        "            gr.Markdown(\"### üíæ Storage Paths\")\n",
        "            state_db_path_input = gr.Textbox(\n",
        "                label=\"Crawl State DB Path (SQLite)\",\n",
        "                value=LinkCrawlerConfig.state_db_path,\n",
        "            )  # Uses LinkCrawlerConfig\n",
        "            edge_list_path_input = gr.Textbox(\n",
        "                label=\"Output Edge List Path (CSV)\",\n",
        "                value=LinkCrawlerConfig.edge_list_path,\n",
        "            )  # Uses LinkCrawlerConfig\n",
        "\n",
        "            start_button = gr.Button(\"üöÄ Start Extraction\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## Status & Results\")\n",
        "            status_message_output = gr.Textbox(\n",
        "                label=\"Status Message\", interactive=False\n",
        "            )\n",
        "            logs_output = gr.Textbox(\n",
        "                label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20\n",
        "            )\n",
        "            summary_output = gr.Markdown(\"---\")\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_edge_crawler_interface,\n",
        "        inputs=[\n",
        "            initial_url_input,\n",
        "            crawling_scope_path_input,\n",
        "            saving_scope_path_input,\n",
        "            crawling_strategy_radio,\n",
        "            state_db_path_input,\n",
        "            edge_list_path_input,\n",
        "            max_pages_input,\n",
        "        ],\n",
        "        outputs=[status_message_output, logs_output, summary_output],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYIfh5m64yYU"
      },
      "outputs": [],
      "source": [
        "# File: notebooks/link_crawler_ui.ipynb - Cell 4\n",
        "\n",
        "# --- Launch the Application ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "\n",
        "        # It's better to mount once at the very start of the notebook\n",
        "        # or main.py. If it's already mounted, no need to force_remount unless necessary.\n",
        "        # Check if already mounted before attempting to mount again (as in Cell 1)\n",
        "        if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "            drive.mount(\"/content/drive/\")\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "        else:\n",
        "            print(\"Google Drive already mounted.\")\n",
        "\n",
        "        demo.launch(debug=True, share=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not launch Gradio demo in this environment: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
