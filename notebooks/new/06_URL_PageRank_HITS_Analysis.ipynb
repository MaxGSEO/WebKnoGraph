{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-DYI18ZXq6r"
   },
   "source": [
    "# Make sure to restart the Runtime and have a clean state to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aghKngLiWJjt"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# --- Step 1: Install Required Libraries ---\n",
    "#\n",
    "!pip install pandas gradio networkx -q\n",
    "!pip install fireducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pDap4ewV4m8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd_standard  # Standard pandas for Gradio compatibility and specific operations\n",
    "import fireducks.pandas as fpd  # Fireducks pandas for high-performance data loading and manipulation\n",
    "import gradio as gr\n",
    "import io\n",
    "import os\n",
    "import networkx as nx  # Import networkx for graph operations and HITS\n",
    "import re  # For URL parsing in trimming function\n",
    "from urllib.parse import (\n",
    "    urlparse,\n",
    ")  # Already imported in original, keeping for completeness\n",
    "\n",
    "# --- Google Colab Specific: Import and Mount Google Drive ---\n",
    "# If you are running this code in Google Colab, uncomment the following lines\n",
    "# and run them first to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"Attempting to mount Google Drive...\")  # Log\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "print(\"Google Drive mount command executed.\")  # Log\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1. CSVLoader Class (Single Responsibility: Loading data) ---\n",
    "class CSVLoader:\n",
    "    \"\"\"\n",
    "    Handles loading data from a CSV file.\n",
    "    Follows SRP by being solely responsible for data acquisition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the CSVLoader with the path to the CSV file.\n",
    "        :param file_path: The full path to the CSV file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.dataframe = None\n",
    "        print(f\"CSVLoader initialized with file_path: {self.file_path}\")  # Log\n",
    "\n",
    "    def load_data(self) -> fpd.DataFrame:  # Specify return type as fpd.DataFrame\n",
    "        \"\"\"\n",
    "        Loads the CSV content from the specified file path into a fireducks.pandas DataFrame.\n",
    "        Raises FileNotFoundError if the file is not found.\n",
    "        Raises ValueError if essential columns are missing.\n",
    "        :return: A fireducks.pandas DataFrame containing the loaded data.\n",
    "        \"\"\"\n",
    "        print(f\"Attempting to load data from: {self.file_path}\")  # Log\n",
    "        if not os.path.exists(self.file_path):\n",
    "            print(f\"FileNotFoundError: File does not exist at {self.file_path}\")  # Log\n",
    "            raise FileNotFoundError(f\"CSV file not found at: {self.file_path}\")\n",
    "\n",
    "        try:\n",
    "            self.dataframe = fpd.read_csv(self.file_path)  # Use fpd.read_csv\n",
    "            print(\n",
    "                f\"Successfully loaded data. DataFrame shape: {self.dataframe.shape}\"\n",
    "            )  # Log\n",
    "            return self.dataframe\n",
    "        except Exception as e:\n",
    "            print(f\"IOError: Error loading CSV data: {e}\")  # Log\n",
    "            raise IOError(f\"Error loading CSV data from {self.file_path}: {e}\")\n",
    "\n",
    "\n",
    "# --- 2. PageRankAnalyzer Class (Single Responsibility: Analyzing PageRank data) ---\n",
    "class PageRankAnalyzer:\n",
    "    \"\"\"\n",
    "    Performs analysis on URL data to find worst PageRank candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe: pd_standard.DataFrame\n",
    "    ):  # Expect standard pandas DataFrame\n",
    "        \"\"\"\n",
    "        Initializes the PageRankAnalyzer with a pandas DataFrame containing URL, Folder_Depth, and PageRank.\n",
    "        \"\"\"\n",
    "        required_columns = [\"URL\", \"Folder_Depth\", \"PageRank\"]\n",
    "        if not all(col in dataframe.columns for col in required_columns):\n",
    "            print(\n",
    "                f\"ValueError: PageRank data missing required columns. Found: {dataframe.columns.tolist()}\"\n",
    "            )  # Log\n",
    "            raise ValueError(\n",
    "                f\"PageRank data must contain 'URL', 'Folder_Depth', and 'PageRank' columns. \"\n",
    "                f\"Found columns: {dataframe.columns.tolist()}\"\n",
    "            )\n",
    "        # Ensure correct data types and handle missing values using pd_standard\n",
    "        dataframe[\"Folder_Depth\"] = (\n",
    "            pd_standard.to_numeric(dataframe[\"Folder_Depth\"], errors=\"coerce\")\n",
    "            .fillna(-1)\n",
    "            .astype(int)\n",
    "        )\n",
    "        dataframe[\"PageRank\"] = pd_standard.to_numeric(\n",
    "            dataframe[\"PageRank\"], errors=\"coerce\"\n",
    "        )\n",
    "        dataframe.dropna(subset=[\"PageRank\"], inplace=True)\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        print(\n",
    "            f\"PageRankAnalyzer initialized. Processed DataFrame shape: {self.dataframe.shape}\"\n",
    "        )  # Log\n",
    "\n",
    "    def find_worst_candidates(\n",
    "        self, depth_level: int, top_n: int\n",
    "    ) -> pd_standard.DataFrame:  # Return standard pd.DataFrame\n",
    "        \"\"\"\n",
    "        Finds the top 'n' URL candidates with the worst (lowest) PageRank\n",
    "        at a specific folder depth level.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"Finding worst PageRank candidates for depth: {depth_level}, top_n: {top_n}\"\n",
    "        )  # Log\n",
    "        if self.dataframe.empty:\n",
    "            print(\"PageRank dataframe is empty.\")  # Log\n",
    "            return pd_standard.DataFrame(columns=[\"URL\", \"Folder_Depth\", \"PageRank\"])\n",
    "\n",
    "        filtered_df = self.dataframe[\n",
    "            self.dataframe[\"Folder_Depth\"] == depth_level\n",
    "        ].copy()\n",
    "        print(\n",
    "            f\"Filtered PageRank DataFrame shape at depth {depth_level}: {filtered_df.shape}\"\n",
    "        )  # Log\n",
    "\n",
    "        if filtered_df.empty:\n",
    "            print(f\"No PageRank entries found at depth level {depth_level}.\")  # Log\n",
    "            return pd_standard.DataFrame(columns=[\"URL\", \"Folder_Depth\", \"PageRank\"])\n",
    "\n",
    "        sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=True)\n",
    "        worst_candidates = sorted_df.head(top_n)\n",
    "\n",
    "        print(f\"Found {len(worst_candidates)} worst PageRank candidates.\")  # Log\n",
    "        return worst_candidates[\n",
    "            [\"URL\", \"Folder_Depth\", \"PageRank\"]\n",
    "        ]  # Already standard pd.DataFrame\n",
    "\n",
    "\n",
    "# --- 3. HITSGraphAnalyzer Class (Single Responsibility: Analyzing graph data with HITS) ---\n",
    "class HITSGraphAnalyzer:\n",
    "    \"\"\"\n",
    "    Performs HITS algorithm analysis on a link graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        edges_dataframe: pd_standard.DataFrame,\n",
    "        pagerank_dataframe: pd_standard.DataFrame,\n",
    "    ):  # Expect standard pandas\n",
    "        \"\"\"\n",
    "        Initializes the HITSGraphAnalyzer with a pandas DataFrame containing 'FROM' and 'TO' columns\n",
    "        and the pagerank_dataframe to merge Folder_Depth.\n",
    "        :param edges_dataframe: DataFrame with 'FROM' and 'TO' columns representing directed edges.\n",
    "        :param pagerank_dataframe: DataFrame containing 'URL' and 'Folder_Depth' for merging.\n",
    "        \"\"\"\n",
    "        required_edges_columns = [\"FROM\", \"TO\"]\n",
    "        if not all(col in edges_dataframe.columns for col in required_edges_columns):\n",
    "            print(\n",
    "                f\"ValueError: Link graph data missing required columns. Found: {edges_dataframe.columns.tolist()}\"\n",
    "            )  # Log\n",
    "            raise ValueError(\n",
    "                f\"Link graph data must contain 'FROM' and 'TO' columns. \"\n",
    "                f\"Found columns: {edges_dataframe.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        self.graph = nx.DiGraph()  # Create a directed graph\n",
    "        # networkx expects standard Python types or numpy arrays for nodes/edges\n",
    "        # So convert to list for iteration\n",
    "        for _, row in edges_dataframe.iterrows():\n",
    "            self.graph.add_edge(row[\"FROM\"], row[\"TO\"])\n",
    "        print(\n",
    "            f\"HITSGraphAnalyzer initialized. Graph has {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges.\"\n",
    "        )  # Log\n",
    "\n",
    "        # Store pagerank_dataframe for merging folder depth later\n",
    "        self.pagerank_dataframe = pagerank_dataframe[[\"URL\", \"Folder_Depth\"]].copy()\n",
    "        # Ensure Folder_Depth is correctly typed in the pagerank_dataframe\n",
    "        self.pagerank_dataframe[\"Folder_Depth\"] = (\n",
    "            pd_standard.to_numeric(\n",
    "                self.pagerank_dataframe[\"Folder_Depth\"], errors=\"coerce\"\n",
    "            )\n",
    "            .fillna(-1)\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "    def calculate_hits_scores(\n",
    "        self,\n",
    "    ) -> pd_standard.DataFrame:  # Return standard pd.DataFrame\n",
    "        \"\"\"\n",
    "        Calculates Hub and Authority scores using the HITS algorithm and merges Folder_Depth.\n",
    "        :return: A DataFrame with 'URL', 'Folder_Depth', 'Hub Score', and 'Authority Score'.\n",
    "                 Returns an empty DataFrame if the graph is empty.\n",
    "        \"\"\"\n",
    "        print(\"Calculating HITS scores...\")  # Log\n",
    "        if self.graph.number_of_nodes() == 0:\n",
    "            print(\"Graph is empty, returning empty HITS DataFrame.\")  # Log\n",
    "            return pd_standard.DataFrame(\n",
    "                columns=[\"URL\", \"Folder_Depth\", \"Hub Score\", \"Authority Score\"]\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            hubs, authorities = nx.hits(self.graph)\n",
    "            print(\n",
    "                f\"HITS calculation complete. Found {len(hubs)} hubs and {len(authorities)} authorities.\"\n",
    "            )  # Log\n",
    "\n",
    "            hits_data = []\n",
    "            for node in self.graph.nodes():\n",
    "                hits_data.append(\n",
    "                    {\n",
    "                        \"URL\": node,\n",
    "                        \"Hub Score\": hubs.get(node, 0.0),\n",
    "                        \"Authority Score\": authorities.get(node, 0.0),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create standard pandas DataFrame here\n",
    "            hits_df = pd_standard.DataFrame(hits_data)\n",
    "\n",
    "            # Merge with pagerank_dataframe to get Folder_Depth\n",
    "            merged_hits_df = pd_standard.merge(  # Use pd_standard.merge\n",
    "                hits_df,\n",
    "                self.pagerank_dataframe,\n",
    "                on=\"URL\",\n",
    "                how=\"left\",  # Use left merge to keep all HITS results\n",
    "            )\n",
    "            # Fill NaN Folder_Depth values for URLs not found in pagerank_dataframe\n",
    "            merged_hits_df[\"Folder_Depth\"].fillna(-1, inplace=True)\n",
    "            merged_hits_df[\"Folder_Depth\"] = merged_hits_df[\"Folder_Depth\"].astype(int)\n",
    "\n",
    "            merged_hits_df = merged_hits_df.sort_values(\n",
    "                by=\"Authority Score\", ascending=False\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "            print(f\"HITS DataFrame created. Shape: {merged_hits_df.shape}\")  # Log\n",
    "            return merged_hits_df[\n",
    "                [\"URL\", \"Folder_Depth\", \"Hub Score\", \"Authority Score\"]\n",
    "            ]  # Already standard pd.DataFrame\n",
    "        except Exception as e:\n",
    "            print(f\"RuntimeError: Error during HITS calculation: {e}\")  # Log\n",
    "            raise RuntimeError(f\"Error during HITS calculation: {e}\")\n",
    "\n",
    "\n",
    "# --- 4. GradioApp Class (Single Responsibility: UI interaction) ---\n",
    "class GradioApp:\n",
    "    \"\"\"\n",
    "    Manages the Gradio user interface for the URL analysis application.\n",
    "    It composes CSVLoader, PageRankAnalyzer, and HITSGraphAnalyzer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pagerank_csv_path: str, link_graph_csv_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the GradioApp and attempts to load both CSV files at startup.\n",
    "        \"\"\"\n",
    "        self.pagerank_csv_path = pagerank_csv_path\n",
    "        self.link_graph_csv_path = link_graph_csv_path\n",
    "\n",
    "        self.pagerank_analyzer = None\n",
    "        self.hits_graph_analyzer = None\n",
    "        self.initial_load_status = []  # List to store status messages\n",
    "        self.full_pagerank_df = (\n",
    "            pd_standard.DataFrame()\n",
    "        )  # Store the full pagerank df (standard pandas)\n",
    "\n",
    "        print(\"GradioApp initialization started.\")  # Log\n",
    "        # Attempt to load PageRank data\n",
    "        try:\n",
    "            print(\n",
    "                f\"Attempting to load PageRank data from {self.pagerank_csv_path}\"\n",
    "            )  # Log\n",
    "            pagerank_loader = CSVLoader(self.pagerank_csv_path)\n",
    "            # Load with fireducks, then convert to standard pandas for the analyzer\n",
    "            fpd_pagerank_df = pagerank_loader.load_data()\n",
    "            self.full_pagerank_df = fpd_pagerank_df.to_pandas()  # Convert here!\n",
    "            self.pagerank_analyzer = PageRankAnalyzer(\n",
    "                self.full_pagerank_df.copy()\n",
    "            )  # Pass a copy to analyzer\n",
    "            self.initial_load_status.append(\n",
    "                f\"✅ PageRank data loaded from {self.pagerank_csv_path}\"\n",
    "            )\n",
    "        except (FileNotFoundError, IOError, ValueError) as e:\n",
    "            self.initial_load_status.append(f\"❌ Error loading PageRank CSV: {e}\")\n",
    "            self.pagerank_analyzer = None\n",
    "            print(f\"PageRank CSV load failed: {e}\")  # Log\n",
    "\n",
    "        # Attempt to load Link Graph data for HITS\n",
    "        try:\n",
    "            print(\n",
    "                f\"Attempting to load Link Graph data from {self.link_graph_csv_path}\"\n",
    "            )  # Log\n",
    "            link_graph_loader = CSVLoader(self.link_graph_csv_path)\n",
    "            # Load with fireducks, then convert to standard pandas for networkx\n",
    "            fpd_link_graph_df = link_graph_loader.load_data()\n",
    "            link_graph_df_standard = fpd_link_graph_df.to_pandas()  # Convert here!\n",
    "\n",
    "            # Pass the full_pagerank_df (already standard pandas) to HITSGraphAnalyzer for folder depth merging\n",
    "            if not self.full_pagerank_df.empty:\n",
    "                self.hits_graph_analyzer = HITSGraphAnalyzer(\n",
    "                    link_graph_df_standard, self.full_pagerank_df\n",
    "                )\n",
    "            else:\n",
    "                self.initial_load_status.append(\n",
    "                    f\"⚠️ PageRank data empty, HITS may not have Folder_Depth info.\"\n",
    "                )\n",
    "                self.hits_graph_analyzer = HITSGraphAnalyzer(\n",
    "                    link_graph_df_standard,\n",
    "                    pd_standard.DataFrame(columns=[\"URL\", \"Folder_Depth\"]),\n",
    "                )  # Pass empty standard df\n",
    "            self.initial_load_status.append(\n",
    "                f\"✅ Link graph data loaded from {self.link_graph_csv_path}\"\n",
    "            )\n",
    "        except (FileNotFoundError, IOError, ValueError) as e:\n",
    "            self.initial_load_status.append(f\"❌ Error loading Link Graph CSV: {e}\")\n",
    "            self.hits_graph_analyzer = None\n",
    "            print(f\"Link Graph CSV load failed: {e}\")  # Log\n",
    "        except RuntimeError as e:\n",
    "            self.initial_load_status.append(f\"❌ Error building graph for HITS: {e}\")\n",
    "            self.hits_graph_analyzer = None\n",
    "            print(f\"HITS graph build failed: {e}\")  # Log\n",
    "\n",
    "        self.initial_load_status_str = \"\\n\".join(self.initial_load_status)\n",
    "        print(\"GradioApp initialization finished.\")  # Log\n",
    "\n",
    "    def perform_analysis(self, analysis_type: str, depth_level: int, top_n: int):\n",
    "        \"\"\"\n",
    "        Performs the selected analysis (PageRank or HITS) and returns updates for the DataFrame\n",
    "        and status message. This function now prepares all updates in one go.\n",
    "        :param analysis_type: 'PageRank' or 'HITS'.\n",
    "        :param depth_level: Relevant for PageRank analysis.\n",
    "        :param top_n: Number of top results.\n",
    "        :return: A tuple containing a gr.update object for the dataframe and a status message string.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"perform_analysis called with type: {analysis_type}, depth: {depth_level}, top_n: {top_n}\"\n",
    "        )  # Log\n",
    "\n",
    "        status_msg = \"\"\n",
    "        # Initialize an empty standard pandas DataFrame\n",
    "        results_df = pd_standard.DataFrame()\n",
    "        new_headers = []  # Will be set based on analysis_type\n",
    "\n",
    "        if analysis_type == \"PageRank\":\n",
    "            if self.pagerank_analyzer is None:\n",
    "                status_msg = (\n",
    "                    self.initial_load_status_str\n",
    "                    + \"\\n\\nCannot perform PageRank analysis: Data not loaded.\"\n",
    "                )\n",
    "                print(\"PageRank analyzer is not initialized.\")  # Log\n",
    "                new_headers = [\"URL\", \"Folder_Depth\", \"PageRank\"]  # Fallback\n",
    "            else:\n",
    "                try:\n",
    "                    results_df = self.pagerank_analyzer.find_worst_candidates(\n",
    "                        depth_level, top_n\n",
    "                    )\n",
    "                    if results_df.empty:\n",
    "                        status_msg = f\"No PageRank candidates found at Depth Level {depth_level} or after filtering.\"\n",
    "                        print(\"No PageRank candidates found.\")  # Log\n",
    "                    else:\n",
    "                        status_msg = f\"Top {top_n} Worst PageRank Candidates at Depth Level {depth_level}:\"\n",
    "                        print(\n",
    "                            f\"PageRank analysis successful, {len(results_df)} results.\"\n",
    "                        )  # Log\n",
    "                    new_headers = [\n",
    "                        \"URL\",\n",
    "                        \"Folder_Depth\",\n",
    "                        \"PageRank\",\n",
    "                    ]  # Explicitly set correct headers\n",
    "                except Exception as e:\n",
    "                    status_msg = f\"An error occurred during PageRank analysis: {e}\"\n",
    "                    print(f\"Error during PageRank analysis: {e}\")  # Log\n",
    "                    new_headers = [\"URL\", \"Folder_Depth\", \"PageRank\"]  # Fallback\n",
    "\n",
    "        elif analysis_type == \"HITS\":\n",
    "            if self.hits_graph_analyzer is None:\n",
    "                status_msg = (\n",
    "                    self.initial_load_status_str\n",
    "                    + \"\\n\\nCannot perform HITS analysis: Graph data not loaded.\"\n",
    "                )\n",
    "                print(\"HITS graph analyzer is not initialized.\")  # Log\n",
    "                new_headers = [\n",
    "                    \"URL\",\n",
    "                    \"Folder_Depth\",\n",
    "                    \"Hub Score\",\n",
    "                    \"Authority Score\",\n",
    "                ]  # Fallback\n",
    "            else:\n",
    "                try:\n",
    "                    results_df = self.hits_graph_analyzer.calculate_hits_scores()\n",
    "                    if results_df.empty:\n",
    "                        status_msg = \"No HITS scores calculated (empty graph).\"\n",
    "                        print(\"No HITS scores calculated.\")  # Log\n",
    "                    else:\n",
    "                        results_df = results_df.head(top_n)  # Apply top_n here\n",
    "                        status_msg = f\"Top {top_n} HITS Authority/Hub Score Candidates:\"\n",
    "                        print(\n",
    "                            f\"HITS analysis successful, {len(results_df)} results.\"\n",
    "                        )  # Log\n",
    "                    new_headers = [\n",
    "                        \"URL\",\n",
    "                        \"Folder_Depth\",\n",
    "                        \"Hub Score\",\n",
    "                        \"Authority Score\",\n",
    "                    ]  # Explicitly set correct headers\n",
    "                except Exception as e:\n",
    "                    status_msg = f\"An error occurred during HITS analysis: {e}\"\n",
    "                    print(f\"Error during HITS analysis: {e}\")  # Log\n",
    "                    new_headers = [\n",
    "                        \"URL\",\n",
    "                        \"Folder_Depth\",\n",
    "                        \"Hub Score\",\n",
    "                        \"Authority Score\",\n",
    "                    ]  # Fallback\n",
    "\n",
    "        else:\n",
    "            status_msg = \"Invalid analysis type selected.\"\n",
    "            print(f\"Invalid analysis type: {analysis_type}\")  # Log\n",
    "            new_headers = [\"URL\", \"Score1\", \"Score2\"]  # Fallback for unknown type\n",
    "\n",
    "        # Determine datatype based on the number of headers\n",
    "        if len(new_headers) == 3:\n",
    "            new_datatype = [\"str\", \"number\", \"number\"]\n",
    "        elif len(new_headers) == 4:\n",
    "            new_datatype = [\"str\", \"number\", \"number\", \"number\"]\n",
    "        else:\n",
    "            new_datatype = [\"str\", \"number\", \"number\"]  # Fallback\n",
    "\n",
    "        print(\n",
    "            f\"Returning DataFrame update with headers: {new_headers}, datatype: {new_datatype}, and status: {status_msg}\"\n",
    "        )  # Log\n",
    "        return gr.update(\n",
    "            value=results_df,\n",
    "            headers=new_headers,\n",
    "            datatype=new_datatype,\n",
    "            col_count=len(new_headers),\n",
    "        ), status_msg\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Launches the Gradio interface.\n",
    "        \"\"\"\n",
    "        print(\"Launching Gradio demo...\")  # Log\n",
    "        with gr.Blocks(title=\"URL & HITS Analyzer\") as demo:\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                # URL & HITS Analyzer\n",
    "                This application loads PageRank and link graph data from your mounted Google Drive.\n",
    "                Please ensure your Google Drive is mounted correctly in Google Colab before running.\n",
    "                Select an analysis type to view results.\n",
    "                \"\"\"\n",
    "            )\n",
    "            # Display initial load status for both files\n",
    "            gr.Markdown(\"### File Loading Status:\")\n",
    "            gr.Markdown(self.initial_load_status_str)\n",
    "\n",
    "            # New section for explanation\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                ## Understanding Results for Website Re-architecture\n",
    "\n",
    "                This tool helps identify pages that are good candidates for improving your website's overall PageRank and structural authority.\n",
    "\n",
    "                **PageRank Analysis:**\n",
    "                * **Worst PageRank Candidates:** These are pages with low PageRank values, indicating they are not highly valued by the linking structure of your website (and potentially the broader web). Improving their internal linking (from high PageRank pages) or acquiring external backlinks can significantly boost their visibility and \"link juice\" distribution.\n",
    "\n",
    "                **HITS Analysis:**\n",
    "                The HITS algorithm provides a complementary view by identifying two types of influential pages:\n",
    "                * **High Authority Score:** These pages are recognized as definitive sources of information on a topic (i.e., they are *pointed to* by many good hubs). If a page has a high Authority score but relatively low PageRank, it suggests the content is valuable, but it might not be receiving enough PageRank flow. Focus on internal linking from high-PageRank pages and external link building to these pages.\n",
    "                * **High Hub Score:** These pages serve as excellent resource lists, pointing to many good authoritative pages. If a page has a high Hub score but low PageRank, it's a valuable navigational asset but isn't getting enough inbound link equity itself. Boosting the PageRank of such a hub (via internal/external links) will improve the \"link juice\" it passes to the authorities it links to.\n",
    "\n",
    "                By understanding these scores, you can strategically re-architect your website's internal linking, content, and external link building efforts to maximize PageRank and improve overall SEO performance.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            with gr.Row():\n",
    "                analysis_type_radio = gr.Radio(\n",
    "                    [\"PageRank\", \"HITS\"],\n",
    "                    label=\"Select Analysis Type\",\n",
    "                    value=\"PageRank\",  # Default to PageRank\n",
    "                    interactive=True,\n",
    "                )\n",
    "\n",
    "            with gr.Row():\n",
    "                # PageRank specific inputs (will only be used if PageRank is selected)\n",
    "                depth_level_input = gr.Slider(\n",
    "                    minimum=0,\n",
    "                    maximum=10,\n",
    "                    step=1,\n",
    "                    value=1,\n",
    "                    label=\"Folder Depth Level (for PageRank)\",\n",
    "                    visible=True,  # Default to visible for PageRank initially\n",
    "                )\n",
    "                top_n_input = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=100,\n",
    "                    step=1,\n",
    "                    value=5,\n",
    "                    label=\"Number of Top Candidates (N)\",\n",
    "                )\n",
    "\n",
    "            analyze_button = gr.Button(\"Perform Analysis\")\n",
    "\n",
    "            status_message_output = gr.Markdown(\"Analysis results will appear below.\")\n",
    "            output_dataframe = gr.Dataframe(\n",
    "                row_count=5, interactive=True, label=\"Analysis Results\", visible=True\n",
    "            )\n",
    "\n",
    "            # Define a function to control visibility of depth_level_input\n",
    "            def update_depth_input_visibility(analysis_type):\n",
    "                if analysis_type == \"PageRank\":\n",
    "                    return gr.update(visible=True)\n",
    "                else:\n",
    "                    return gr.update(visible=False)\n",
    "\n",
    "            # Bind the radio button change event to update the visibility of the depth slider\n",
    "            analysis_type_radio.change(\n",
    "                fn=update_depth_input_visibility,\n",
    "                inputs=[analysis_type_radio],\n",
    "                outputs=[depth_level_input],\n",
    "            )\n",
    "\n",
    "            # Bind the button click to the main analysis function.\n",
    "            analyze_button.click(\n",
    "                fn=self.perform_analysis,\n",
    "                inputs=[analysis_type_radio, depth_level_input, top_n_input],\n",
    "                outputs=[output_dataframe, status_message_output],\n",
    "            )\n",
    "\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the paths to your CSV files in Google Drive\n",
    "    pagerank_csv_path = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/url_analysis_results.csv\"\n",
    "    )\n",
    "    link_graph_csv_path = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\"\n",
    "    )\n",
    "\n",
    "    print(f\"PageRank CSV Path: {pagerank_csv_path}\")  # Log\n",
    "    print(f\"Link Graph CSV Path: {link_graph_csv_path}\")  # Log\n",
    "\n",
    "    # Instantiate and run the Gradio application\n",
    "    app = GradioApp(\n",
    "        pagerank_csv_path=pagerank_csv_path, link_graph_csv_path=link_graph_csv_path\n",
    "    )\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}