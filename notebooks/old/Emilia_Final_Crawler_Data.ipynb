{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRO5NgA-kVPf"
      },
      "source": [
        "### **Final crawler implementation version 1.0**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**NOTES:**\n",
        "\n",
        "2.  When doing 1, take the final URL and process it's content, not the first one + also check if it is a valid URL - HALF THERE.\n",
        "3. Repeat the process until the DB hits 700 entry rows - SOLVED.\n",
        "4. Add input fields for the constants - OPTIONAL.\n",
        "5. Unit tests are not provided intentionally, since they're an overkill for our audience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YrTIkyLgccB",
        "outputId": "fd491d97-58ed-48e1-da08-f594d5b9704e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, outcome, h11, wsproto, webdriver_manager, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 python-dotenv-1.0.1 selenium-4.23.1 trio-0.26.2 trio-websocket-0.11.1 webdriver_manager-4.0.2 wsproto-1.2.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.20.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium webdriver_manager\n",
        "!pip install tqdm\n",
        "!pip install polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "hhO4ZW-Vgpfy",
        "outputId": "91756676-8af6-49bc-9ed2-78a5e5fca68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter crawling strategy (BFS/DFS): BFS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   0%|          | 0/700 [00:00<?, ?page/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/', 0], url e https://kalicube.com/learning-spaces/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   0%|          | 1/700 [00:00<04:16,  2.73page/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/knowledge-graph-update/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/knowledge-graph-update/', 0], url e https://kalicube.com/learning-spaces/knowledge-graph-update/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   0%|          | 2/700 [00:02<16:38,  1.43s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/faq/brand-serps/how-does-the-kalicube-process-work/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/faq/brand-serps/how-does-the-kalicube-process-work/', 0], url e https://kalicube.com/learning-spaces/faq/brand-serps/how-does-the-kalicube-process-work/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   0%|          | 3/700 [00:04<21:39,  1.86s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/faq/seo-glossary/how-kalicube-implements-the-kalicube-process/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/faq/seo-glossary/how-kalicube-implements-the-kalicube-process/', 0], url e https://kalicube.com/learning-spaces/faq/seo-glossary/how-kalicube-implements-the-kalicube-process/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|          | 4/700 [00:06<19:35,  1.69s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/', 0], url e https://kalicube.com/learning-spaces/knowledge-nuggets/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|          | 5/700 [00:08<21:26,  1.85s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/entity-seo/how-does-consistency-contribute-to-machine-understandability-virtual-antics-podcast/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/entity-seo/how-does-consistency-contribute-to-machine-understandability-virtual-antics-podcast/', 0], url e https://kalicube.com/learning-spaces/knowledge-nuggets/entity-seo/how-does-consistency-contribute-to-machine-understandability-virtual-antics-podcast/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|          | 6/700 [00:10<22:34,  1.95s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/faq/brand-serps/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/faq/brand-serps/', 0], url e https://kalicube.com/learning-spaces/faq/brand-serps/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|          | 7/700 [00:12<23:43,  2.05s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/faq/knowledge-panels/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/faq/knowledge-panels/', 0], url e https://kalicube.com/learning-spaces/faq/knowledge-panels/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|          | 8/700 [00:14<23:28,  2.04s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-does-googles-representation-of-you-shape-your-personal-brand-tailoring-talk-podcast/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-does-googles-representation-of-you-shape-your-personal-brand-tailoring-talk-podcast/', 0], url e https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-does-googles-representation-of-you-shape-your-personal-brand-tailoring-talk-podcast/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|▏         | 9/700 [00:16<23:00,  2.00s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/', 0], url e https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   1%|▏         | 10/700 [00:18<22:58,  2.00s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/how-does-consistency-contribute-to-machine-understandability/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/how-does-consistency-contribute-to-machine-understandability/', 0], url e https://kalicube.com/learning-spaces/how-does-consistency-contribute-to-machine-understandability/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   2%|▏         | 11/700 [00:21<23:39,  2.06s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/faq/seo-glossary/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/faq/seo-glossary/', 0], url e https://kalicube.com/learning-spaces/faq/seo-glossary/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   2%|▏         | 12/700 [00:22<23:18,  2.03s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-does-focusing-on-a-niche-impact-your-brand-and-business-growth-virtual-antics-podcast/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-does-focusing-on-a-niche-impact-your-brand-and-business-growth-virtual-antics-podcast/', 0], url e https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-does-focusing-on-a-niche-impact-your-brand-and-business-growth-virtual-antics-podcast/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   2%|▏         | 13/700 [00:24<20:43,  1.81s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/how-does-the-kalicube-process-stay-ahead-of-technological-advancements/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/how-does-the-kalicube-process-stay-ahead-of-technological-advancements/', 0], url e https://kalicube.com/learning-spaces/how-does-the-kalicube-process-stay-ahead-of-technological-advancements/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Crawling progress:   2%|▏         | 15/700 [00:27<18:33,  1.63s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current url data e ['https://kalicube.com/learning-spaces/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/', 0], url e https://kalicube.com/learning-spaces/, a num redirects e 0\n",
            "Current url data e ['https://kalicube.com/learning-spaces/page/2/', 0]\n",
            "url data e ['https://kalicube.com/learning-spaces/page/2/', 0], url e https://kalicube.com/learning-spaces/page/2/, a num redirects e 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCrawling progress:   2%|▏         | 15/700 [00:29<22:09,  1.94s/page]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-feb24e4fab50>\u001b[0m in \u001b[0;36m<cell line: 235>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-feb24e4fab50>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBFSCrawlingStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstrategy_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BFS'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDFSCrawlingStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mcrawler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-feb24e4fab50>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m               \u001b[0mcurrent_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_url_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m               \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently processing: {current_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m               \u001b[0mnew_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_page_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_url_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawling_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_links\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisited_urls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mcurrent_url\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisited_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-feb24e4fab50>\u001b[0m in \u001b[0;36mextract_page_data\u001b[0;34m(self, url_data)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'User-Agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"url data e {url_data}, url e {url}, a num redirects e {num_redirects}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "from urllib.parse import urljoin, urlparse, parse_qs\n",
        "from collections import deque\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from abc import ABC, abstractmethod\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define constants\n",
        "MIN_REQUEST_DELAY = 1\n",
        "MAX_REQUEST_DELAY = 30\n",
        "MAX_PAGES = 700\n",
        "SAVE_INTERVAL = 10\n",
        "MAX_RETRIES = 3\n",
        "MAX_REDIRECTS = 2\n",
        "\n",
        "# User Agents for realistic browsing\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/52.0',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36'\n",
        "]\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "db_path = '/content/drive/My Drive/db_data.db'\n",
        "table_name = 'crawled_data'\n",
        "\n",
        "def create_db_and_table_if_not_exists():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    conn.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (URL TEXT UNIQUE, Status_Code INTEGER, Content TEXT)\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_urls():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(f\"SELECT URL FROM {table_name}\")\n",
        "    all_urls = cursor.fetchall()\n",
        "    conn.close()\n",
        "    if all_urls:\n",
        "        return set(url[0] for url in all_urls), all_urls[-1][0]\n",
        "    else:\n",
        "        return set(), None\n",
        "\n",
        "def write_to_db(df):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    df.to_sql(table_name, conn, if_exists='append', index=False)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def count_rows_in_db():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
        "    row_count = cursor.fetchone()[0]\n",
        "    conn.close()\n",
        "    return row_count\n",
        "\n",
        "\n",
        "class CrawlingStrategy(ABC):\n",
        "    @abstractmethod\n",
        "    def add_links(self, links):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def get_next(self):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def has_next(self):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def count(self):\n",
        "        pass\n",
        "\n",
        "class BFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, start_url):\n",
        "        self.queue = deque([start_url])\n",
        "    def add_links(self, links):\n",
        "        self.queue.extend(links)\n",
        "    def get_next(self):\n",
        "        return self.queue.popleft()\n",
        "    def has_next(self):\n",
        "        return len(self.queue) > 0\n",
        "    def count(self):\n",
        "        return len(self.queue)\n",
        "\n",
        "class DFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, start_url):\n",
        "        self.stack = [start_url]\n",
        "    def add_links(self, links):\n",
        "        self.stack.extend(links)\n",
        "    def get_next(self):\n",
        "        return self.stack.pop()\n",
        "    def has_next(self):\n",
        "        return len(self.stack) > 0\n",
        "    def count(self):\n",
        "        return len(self.stack)\n",
        "\n",
        "\n",
        "class WebCrawler:\n",
        "    def __init__(self, crawling_strategy):\n",
        "        self.visited_urls, last_url = get_urls()\n",
        "        self.start_url = last_url if last_url else 'https://kalicube.com/learning-spaces/'\n",
        "        self.crawling_strategy = crawling_strategy\n",
        "        self.crawling_strategy.add_links([self.start_url] if not self.crawling_strategy.has_next() else [])\n",
        "        self.data = []\n",
        "        self.current_delay = MIN_REQUEST_DELAY\n",
        "        self.pages_crawled = 0\n",
        "\n",
        "    from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        parsed_url = urlparse(url)\n",
        "        query_params = parse_qs(parsed_url.query)\n",
        "\n",
        "        # Check for the presence of any UTM parameters which start with 'utm_'\n",
        "        has_utm_params = any(param.startswith('utm_') for param in query_params)\n",
        "\n",
        "        # Conditions for a URL to be considered valid\n",
        "        return (parsed_url.scheme in ('http', 'https') and\n",
        "                not any(re.search(pattern, url) for pattern in [r'\\.(jpg|jpeg|png|gif|pdf|doc|xls|zip|rar|mp3|mp4)$', r'/user/[a-zA-Z0-9_-]+/?$']) and\n",
        "                '#' not in url and\n",
        "                'https://twitter.com' not in url and\n",
        "                \"/learning-spaces/\" in parsed_url.path and # REPLACE /learning-spaces/ with your website directory!\n",
        "                not has_utm_params)\n",
        "\n",
        "\n",
        "    def extract_page_data(self, url_data):\n",
        "        url = url_data[0]\n",
        "        num_redirects = url_data[1]\n",
        "        original_url = url\n",
        "        user_agent = random.choice(USER_AGENTS)\n",
        "        headers = {'User-Agent': user_agent}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            content = response.text\n",
        "            if url not in self.visited_urls:\n",
        "                self.data.append({'URL': original_url, 'Status_Code': response.status_code, 'Content': content})\n",
        "                logging.info(f\"Crawled: {url} with status code {response.status_code}\")\n",
        "            links = set(urljoin(url, link) for link in re.findall(r'href=[\"\\'](.*?)[\"\\']', content))\n",
        "            valid_links = [[link,0] for link in links if self.is_valid_url(link)]\n",
        "            return valid_links\n",
        "\n",
        "        elif response.status_code == 301 or response.status_code == 302:\n",
        "            if num_redirects > MAX_REDIRECTS:\n",
        "              logging.info(f\"Reached max redirects. Stopping...\")\n",
        "              return []\n",
        "            url = response.headers['Location']\n",
        "            if not url.startswith('http'):\n",
        "                url = urljoin(original_url, url)\n",
        "            return [[url, num_redirects+1]]\n",
        "            logging.info(f\"Redirected to: {url}\")\n",
        "\n",
        "        else:\n",
        "            return []  # Exit the redirect loop on non-200/301/302 status codes\n",
        "        time.sleep(self.current_delay)\n",
        "\n",
        "\n",
        "    def adjust_delay(self, response_time):\n",
        "        if response_time < 2:\n",
        "            self.current_delay = max(MIN_REQUEST_DELAY, self.current_delay * 0.9)\n",
        "        elif response_time > 5:\n",
        "            self.current_delay = min(MAX_REQUEST_DELAY, self.current_delay * 1.1)\n",
        "\n",
        "    def crawl(self):\n",
        "      with tqdm(total=MAX_PAGES, desc=\"Crawling progress\", unit=\"page\") as pbar:\n",
        "          while self.pages_crawled < MAX_PAGES and self.crawling_strategy.has_next():\n",
        "              current_url_data = self.crawling_strategy.get_next()\n",
        "              print(f\"Current url data e {current_url_data}\")\n",
        "              current_url = current_url_data[0]\n",
        "              logging.info(f\"Currently processing: {current_url}\")\n",
        "              new_links = self.extract_page_data(current_url_data)\n",
        "              self.crawling_strategy.add_links([link for link in new_links if link[0] not in self.visited_urls])\n",
        "              if current_url not in self.visited_urls:\n",
        "                  self.visited_urls.add(current_url)\n",
        "              self.pages_crawled += 1\n",
        "              pbar.update(1)\n",
        "              time.sleep(self.current_delay)\n",
        "\n",
        "              # Check if we have processed 700 entries\n",
        "              if self.pages_crawled % 700 == 0:\n",
        "                  row_count = count_rows_in_db()\n",
        "                  logging.info(f\"Processed entries: {self.pages_crawled}, Rows in DB: {row_count}\")\n",
        "                  if row_count < 700:\n",
        "                      logging.info(\"Less than 700 rows in the database, resetting pages_crawled.\")\n",
        "                      self.pages_crawled = 0  # Reset pages_crawled\n",
        "                      continue  # Continue the crawling process\n",
        "\n",
        "              if self.pages_crawled % SAVE_INTERVAL == 0:\n",
        "                  self.save_data()\n",
        "\n",
        "          if self.data:\n",
        "              self.save_data(final=True)\n",
        "\n",
        "\n",
        "\n",
        "    def save_data(self):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        write_to_db(df)\n",
        "        self.data = []  # Reset data to avoid duplication\n",
        "\n",
        "def main():\n",
        "    create_db_and_table_if_not_exists()\n",
        "    strategy_type = input(\"Enter crawling strategy (BFS/DFS): \").strip().upper()\n",
        "    visited_urls, last_url = get_urls()\n",
        "    start_url = [last_url, 0] if last_url else ['https://kalicube.com/learning-spaces/',0]\n",
        "    strategy = BFSCrawlingStrategy(start_url) if strategy_type == 'BFS' else DFSCrawlingStrategy(start_url)\n",
        "    crawler = WebCrawler(strategy)\n",
        "    crawler.crawl()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Ts2nHh4d9a"
      },
      "source": [
        "### **Read the DB to check results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwrL9g6v4l20",
        "outputId": "d6aa4898-8752-4e95-b445-fa330de6eea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                                 URL  \\\n",
            "0              https://kalicube.com/learning-spaces/   \n",
            "1       https://kalicube.com/learning-spaces/page/2/   \n",
            "2         https://kalicube.com/learning-spaces/feed/   \n",
            "3      https://kalicube.com/learning-spaces/page/45/   \n",
            "4  https://kalicube.com/learning-spaces/faq/perso...   \n",
            "5  https://kalicube.com/learning-spaces/faq/seo-g...   \n",
            "6  https://kalicube.com/learning-spaces/why-shoul...   \n",
            "7  https://kalicube.com/learning-spaces/knowledge...   \n",
            "8  https://kalicube.com/learning-spaces/faq/perso...   \n",
            "9  https://kalicube.com/learning-spaces/faq/perso...   \n",
            "\n",
            "                                             Content  \n",
            "0  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "1  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "2  <?xml version=\"1.0\" encoding=\"UTF-8\"?><rss ver...  \n",
            "3  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "4  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "5  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "6  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "7  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "8  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "9  <!doctype html>\\n<html lang=\"en-US\" class=\"no-...  \n",
            "19\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the database file on Google Drive\n",
        "db_path = '/content/drive/My Drive/db_data.db'  # Update with your actual path\n",
        "\n",
        "# Function to read the database and return a DataFrame\n",
        "def read_database(db_path):\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect(db_path)\n",
        "\n",
        "    # SQL query to select all data from the table\n",
        "    query = \"SELECT URL, Status_Code, Content FROM crawled_data\"  # Replace 'your_table_name' with the actual table name\n",
        "\n",
        "    # Execute the query and convert to a Pandas DataFrame\n",
        "    df = pd.read_sql_query(query, conn)\n",
        "\n",
        "    # Close the connection to the database\n",
        "    conn.close()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Call the function and display the data\n",
        "df = read_database(db_path)\n",
        "print(df[['URL', 'Content']].head(10))\n",
        "print(len(df.index))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdtJ_le-mAHR"
      },
      "source": [
        "### **Cleaning the content** from the HTML in each DB row and saving it into each respective Cleaned_Content row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ynq2AfumJrX",
        "outputId": "971310f0-9384-4c48-fec3-88cea07f8c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  k = self.parse_starttag(i)\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to clean HTML content\n",
        "def clean_html(content):\n",
        "    # Create a BeautifulSoup object and specify the parser\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "    # Remove all script and style elements\n",
        "    for script_or_style in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
        "        script_or_style.decompose()  # rip it out\n",
        "\n",
        "    # Get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "\n",
        "    # Drop blank lines\n",
        "    cleaned_content = '\\n'.join(line for line in lines if line)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to check if the Cleaned_Content column exists\n",
        "def add_cleaned_content_column_if_not_exists(cursor):\n",
        "    # Check if 'Cleaned_Content' column exists\n",
        "    cursor.execute(\"PRAGMA table_info(crawled_data)\")\n",
        "    columns = [column[1] for column in cursor.fetchall()]\n",
        "\n",
        "    if 'Cleaned_Content' not in columns:\n",
        "        # Add the Cleaned_Content column\n",
        "        cursor.execute(\"ALTER TABLE crawled_data ADD COLUMN Cleaned_Content TEXT\")\n",
        "\n",
        "# Function to update the database with cleaned content\n",
        "def update_database(db_file):\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect(db_file)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Ensure the Cleaned_Content column exists\n",
        "    add_cleaned_content_column_if_not_exists(cursor)\n",
        "\n",
        "    # Query to select URL and Content columns\n",
        "    cursor.execute(\"SELECT rowid, URL, Content FROM crawled_data\")\n",
        "\n",
        "    # Iterate over each row\n",
        "    for row in cursor.fetchall():\n",
        "        rowid, url, content = row\n",
        "\n",
        "        # Clean the HTML content\n",
        "        cleaned_content = clean_html(content)\n",
        "\n",
        "        # Update the row with the cleaned content\n",
        "        cursor.execute(\n",
        "            \"UPDATE crawled_data SET Cleaned_Content = ? WHERE rowid = ?\",\n",
        "            (cleaned_content, rowid)\n",
        "        )\n",
        "\n",
        "    # Commit the transaction and close the connection\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Google Drive path to your database file\n",
        "    db_file = '/content/drive/My Drive/db_data.db'  # Adjust the path accordingly\n",
        "    update_database(db_file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}