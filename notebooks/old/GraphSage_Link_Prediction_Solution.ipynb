{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NOTES:**\n",
        "\n",
        "1.   Currently the algorithm puts 0 values for pages that don't have embeddings. Since that's still some sort of information, we need to revamp it by inferring empty nodes from neighbouring nodes.\n",
        "2.   An OOP version of the algorithm was tested and performed worse, time-wise, compared to this current version.\n",
        "3. The embeddings are not binarized, this can be done post-MVP.\n",
        "4. The end results need to be evaluated, either through SEO or computer science approach and stored in a separate file:\n",
        "  *   A/B testing URL sets by using https://github.com/google/tfp-causalimpact and Google Search Console data (time series data for the chosen URL test sets)\n",
        "  *   Fake network modelling\n",
        "5. Unit tests are not provided intentionally, since they're an overkill for our audience.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xZbWtRAsn5BI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nTnWNZyeVK6C",
        "outputId": "45761b63-85ce-42db-f641-db53f11760ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-301d29ffd9ee>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Step 2: Define paths to your files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define paths to your files\n",
        "drive_root = '/content/drive/My Drive/'\n",
        "\n",
        "csv_file_path = os.path.join(drive_root, 'FinalLinkGraph.csv') # the file which is used to create the graph itself with columns Source and Destination\n",
        "db_file_path = os.path.join(drive_root, 'VectorJinaDuckDBwithFeatures.db')\n",
        "\n",
        "# Step 3: Load the data\n",
        "filtered_link_graph_df = pd.read_csv(csv_file_path)\n",
        "db_connection = duckdb.connect(db_file_path)\n",
        "\n",
        "# Load features from the DuckDB - assuming the table name is 'features'\n",
        "db_features_df = db_connection.execute(\"SELECT * FROM url_embeddings\").fetchdf()\n",
        "\n",
        "# Step 4: Combine the data\n",
        "# Merge the CSV with the features from the DuckDB based on the Source URL\n",
        "merged_df_source = filtered_link_graph_df.merge(\n",
        "    db_features_df,\n",
        "    left_on='Source',\n",
        "    right_on='url',\n",
        "    how='left',\n",
        "    suffixes=('_source', '_destination')\n",
        ")\n",
        "\n",
        "# Merge again for the Destination URL\n",
        "merged_df = merged_df_source.merge(\n",
        "    db_features_df,\n",
        "    left_on='Destination',\n",
        "    right_on='url',\n",
        "    how='left',\n",
        "    suffixes=('_source', '_destination')\n",
        ")\n",
        "\n",
        "# Step 5: Prepare the Graph for GraphSAGE\n",
        "G = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "# Determine the feature dimension (based on the embedding size or other features you have)\n",
        "feature_dim = len(db_features_df.iloc[0]['embedding']) if 'embedding' in db_features_df.columns else 10  # Example fallback\n",
        "\n",
        "# Add nodes with attributes, using zero vectors where features are missing\n",
        "for _, row in db_features_df.iterrows():\n",
        "    features = row['embedding'] if isinstance(row['embedding'], np.ndarray) and not pd.isna(row['embedding']).all() else np.zeros(feature_dim)\n",
        "    G.add_node(row['url'], features=features)\n",
        "\n",
        "# Ensure every node in the graph has the 'features' attribute, even if it wasn't in db_features_df\n",
        "for node in filtered_link_graph_df['Source'].unique():\n",
        "    if node not in G.nodes:\n",
        "        G.add_node(node, features=np.zeros(feature_dim))\n",
        "\n",
        "for node in filtered_link_graph_df['Destination'].unique():\n",
        "    if node not in G.nodes:\n",
        "        G.add_node(node, features=np.zeros(feature_dim))\n",
        "\n",
        "# Add edges from the CSV data\n",
        "for _, row in merged_df.iterrows():\n",
        "    G.add_edge(row['Source'], row['Destination'])\n",
        "\n",
        "# Create the feature matrix (X) and edge list (edge_index)\n",
        "node_features = []\n",
        "node_indices = {}\n",
        "index = 0\n",
        "\n",
        "for node in G.nodes(data=True):\n",
        "    node_features.append(node[1]['features'])\n",
        "    node_indices[node[0]] = index\n",
        "    index += 1\n",
        "\n",
        "# Convert the feature list and edge list to torch tensors\n",
        "X = torch.tensor(np.array(node_features), dtype=torch.float)\n",
        "edge_index = torch.tensor([[node_indices[edge[0]], node_indices[edge[1]]] for edge in G.edges], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Step 6: Create the PyTorch Geometric Data object\n",
        "data = Data(x=X, edge_index=edge_index)\n",
        "\n",
        "# Step 7: Define the GraphSAGE model\n",
        "class GraphSAGELinkPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGELinkPredictor, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.link_pred = nn.Linear(out_channels * 2, 1)  # Link prediction layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def predict_link(self, z, edge_label_index):\n",
        "        edge_embeddings = torch.cat([z[edge_label_index[0]], z[edge_label_index[1]]], dim=1)\n",
        "        return torch.sigmoid(self.link_pred(edge_embeddings)).view(-1)\n",
        "\n",
        "# Initialize the model\n",
        "model = GraphSAGELinkPredictor(in_channels=X.size(1), hidden_channels=64, out_channels=32)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Step 8: Train the GraphSAGE model\n",
        "def train(data, model, optimizer, criterion, edge_label_index, edge_labels):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model(data.x, data.edge_index)\n",
        "    link_logits = model.predict_link(z, edge_label_index)\n",
        "    loss = criterion(link_logits, edge_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Create positive and negative samples for link prediction\n",
        "def create_edge_labels(data):\n",
        "    edge_index = data.edge_index\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    # Create positive labels (1s)\n",
        "    edge_labels = torch.ones(num_edges)\n",
        "\n",
        "    # Sample negative edges (not in the graph)\n",
        "    neg_edge_index = torch.randint(0, data.num_nodes, edge_index.size(), dtype=torch.long)\n",
        "\n",
        "    # Combine positive and negative edges\n",
        "    edge_label_index = torch.cat([edge_index, neg_edge_index], dim=1)\n",
        "    edge_labels = torch.cat([edge_labels, torch.zeros(num_edges)])\n",
        "\n",
        "    return edge_label_index, edge_labels\n",
        "\n",
        "# Create edge labels for training\n",
        "edge_label_index, edge_labels = create_edge_labels(data)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(200):  # Example for 200 epochs\n",
        "    loss = train(data, model, optimizer, criterion, edge_label_index, edge_labels)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "# Step 9: Evaluate the model and include URLs\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = model(data.x, data.edge_index)\n",
        "    link_probs = model.predict_link(z, edge_label_index)\n",
        "\n",
        "# Get the source and destination indices for the edges\n",
        "source_indices = edge_label_index[0].cpu().numpy()\n",
        "destination_indices = edge_label_index[1].cpu().numpy()\n",
        "\n",
        "# Create a list of source and destination URLs\n",
        "index_to_url = {v: k for k, v in node_indices.items()}\n",
        "\n",
        "# Print the URLs with their associated probabilities\n",
        "for i in range(50):  # Adjust this range to print more or fewer results\n",
        "    source_url = index_to_url[source_indices[i]]\n",
        "    destination_url = index_to_url[destination_indices[i]]\n",
        "    probability = link_probs[i].item()\n",
        "    print(f\"Source URL: {source_url}, Destination URL: {destination_url}, Probability: {probability:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input URL to get recommendations"
      ],
      "metadata": {
        "id": "2emP_jkfJCAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define paths to your files\n",
        "drive_root = '/content/drive/My Drive/'\n",
        "\n",
        "csv_file_path = os.path.join(drive_root, 'FinalLinkGraph.csv')\n",
        "db_file_path = os.path.join(drive_root, 'VectorJinaDuckDBwithFeatures.db')\n",
        "\n",
        "# Step 3: Load the data\n",
        "filtered_link_graph_df = pd.read_csv(csv_file_path)\n",
        "db_connection = duckdb.connect(db_file_path)\n",
        "\n",
        "# Load features from the DuckDB - assuming the table name is 'features'\n",
        "db_features_df = db_connection.execute(\"SELECT * FROM url_embeddings\").fetchdf()\n",
        "\n",
        "# Step 4: Combine the data\n",
        "# Merge the CSV with the features from the DuckDB based on the Source URL\n",
        "merged_df_source = filtered_link_graph_df.merge(\n",
        "    db_features_df,\n",
        "    left_on='Source',\n",
        "    right_on='url',\n",
        "    how='left',\n",
        "    suffixes=('_source', '_destination')\n",
        ")\n",
        "\n",
        "# Merge again for the Destination URL\n",
        "merged_df = merged_df_source.merge(\n",
        "    db_features_df,\n",
        "    left_on='Destination',\n",
        "    right_on='url',\n",
        "    how='left',\n",
        "    suffixes=('_source', '_destination')\n",
        ")\n",
        "\n",
        "# Step 5: Prepare the Graph for GraphSAGE\n",
        "G = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "# Determine the feature dimension (based on the embedding size or other features you have)\n",
        "feature_dim = len(db_features_df.iloc[0]['embedding']) if 'embedding' in db_features_df.columns else 10  # Example fallback\n",
        "\n",
        "# Add nodes with attributes, using zero vectors where features are missing\n",
        "for _, row in db_features_df.iterrows():\n",
        "    features = row['embedding'] if isinstance(row['embedding'], np.ndarray) and not pd.isna(row['embedding']).all() else np.zeros(feature_dim)\n",
        "    G.add_node(row['url'], features=features)\n",
        "\n",
        "# Ensure every node in the graph has the 'features' attribute, even if it wasn't in db_features_df\n",
        "for node in filtered_link_graph_df['Source'].unique():\n",
        "    if node not in G.nodes:\n",
        "        G.add_node(node, features=np.zeros(feature_dim))\n",
        "\n",
        "for node in filtered_link_graph_df['Destination'].unique():\n",
        "    if node not in G.nodes:\n",
        "        G.add_node(node, features=np.zeros(feature_dim))\n",
        "\n",
        "# Add edges from the CSV data\n",
        "for _, row in merged_df.iterrows():\n",
        "    G.add_edge(row['Source'], row['Destination'])\n",
        "\n",
        "# Create the feature matrix (X) and edge list (edge_index)\n",
        "node_features = []\n",
        "node_indices = {}\n",
        "index = 0\n",
        "\n",
        "for node in G.nodes(data=True):\n",
        "    node_features.append(node[1]['features'])\n",
        "    node_indices[node[0]] = index\n",
        "    index += 1\n",
        "\n",
        "# Convert the feature list and edge list to torch tensors\n",
        "X = torch.tensor(np.array(node_features), dtype=torch.float)\n",
        "edge_index = torch.tensor([[node_indices[edge[0]], node_indices[edge[1]]] for edge in G.edges], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Step 6: Create the PyTorch Geometric Data object\n",
        "data = Data(x=X, edge_index=edge_index)\n",
        "\n",
        "# Step 7: Define the GraphSAGE model\n",
        "class GraphSAGELinkPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGELinkPredictor, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.link_pred = nn.Linear(out_channels * 2, 1)  # Link prediction layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def predict_link(self, z, edge_label_index):\n",
        "        edge_embeddings = torch.cat([z[edge_label_index[0]], z[edge_label_index[1]]], dim=1)\n",
        "        return torch.sigmoid(self.link_pred(edge_embeddings)).view(-1)\n",
        "\n",
        "# Initialize the model\n",
        "model = GraphSAGELinkPredictor(in_channels=X.size(1), hidden_channels=64, out_channels=32)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Step 8: Train the GraphSAGE model\n",
        "def train(data, model, optimizer, criterion, edge_label_index, edge_labels):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model(data.x, data.edge_index)\n",
        "    link_logits = model.predict_link(z, edge_label_index)\n",
        "    loss = criterion(link_logits, edge_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Create positive and negative samples for link prediction\n",
        "def create_edge_labels(data):\n",
        "    edge_index = data.edge_index\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    # Create positive labels (1s)\n",
        "    edge_labels = torch.ones(num_edges)\n",
        "\n",
        "    # Sample negative edges (not in the graph)\n",
        "    neg_edge_index = torch.randint(0, data.num_nodes, edge_index.size(), dtype=torch.long)\n",
        "\n",
        "    # Combine positive and negative edges\n",
        "    edge_label_index = torch.cat([edge_index, neg_edge_index], dim=1)\n",
        "    edge_labels = torch.cat([edge_labels, torch.zeros(num_edges)])\n",
        "\n",
        "    return edge_label_index, edge_labels\n",
        "\n",
        "# Create edge labels for training\n",
        "edge_label_index, edge_labels = create_edge_labels(data)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(200):  # Example for 200 epochs\n",
        "    loss = train(data, model, optimizer, criterion, edge_label_index, edge_labels)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "# Step 9: Define the recommendation function\n",
        "def recommend_links(input_url, model, data, top_k=10):\n",
        "    if input_url not in node_indices:\n",
        "        raise ValueError(f\"URL {input_url} not found in the graph.\")\n",
        "\n",
        "    input_index = node_indices[input_url]\n",
        "\n",
        "    possible_destinations = list(G.nodes)\n",
        "    possible_edge_index = torch.tensor([[input_index, node_indices[url]] for url in possible_destinations if url != input_url], dtype=torch.long).t().contiguous()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model(data.x, data.edge_index)\n",
        "        link_probs = model.predict_link(z, possible_edge_index)\n",
        "\n",
        "    top_indices = torch.argsort(link_probs, descending=True)[:top_k]\n",
        "\n",
        "    top_urls = [possible_destinations[i] for i in top_indices]\n",
        "    top_probs = link_probs[top_indices].cpu().numpy()\n",
        "\n",
        "    recommendations = []\n",
        "    for i in range(top_k):\n",
        "        recommendations.append({\n",
        "            \"Destination URL\": top_urls[i],\n",
        "            \"Probability\": top_probs[i]\n",
        "        })\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Example usage with user input\n",
        "input_url = input(\"Enter a URL to get link recommendations: \")\n",
        "\n",
        "try:\n",
        "    recommendations = recommend_links(input_url, model, data, top_k=10)\n",
        "    print(\"\\nTop 10 recommended links:\")\n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        print(f\"{i}. Destination URL: {rec['Destination URL']}, Probability: {rec['Probability']:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G423w_4nJG2f",
        "outputId": "3d7702df-c1c1-4da6-e200-ab11cecd765d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 0, Loss: 0.6995\n",
            "Epoch 10, Loss: 0.6900\n",
            "Epoch 20, Loss: 0.6715\n",
            "Epoch 30, Loss: 0.5931\n",
            "Epoch 40, Loss: 0.5470\n",
            "Epoch 50, Loss: 0.5277\n",
            "Epoch 60, Loss: 0.5169\n",
            "Epoch 70, Loss: 0.5084\n",
            "Epoch 80, Loss: 0.5016\n",
            "Epoch 90, Loss: 0.4963\n",
            "Epoch 100, Loss: 0.4922\n",
            "Epoch 110, Loss: 0.4880\n",
            "Epoch 120, Loss: 0.4845\n",
            "Epoch 130, Loss: 0.4823\n",
            "Epoch 140, Loss: 0.4793\n",
            "Epoch 150, Loss: 0.4769\n",
            "Epoch 160, Loss: 0.4751\n",
            "Epoch 170, Loss: 0.4737\n",
            "Epoch 180, Loss: 0.4723\n",
            "Epoch 190, Loss: 0.4715\n",
            "Enter a URL to get link recommendations: https://kalicube.com/learning-spaces/knowledge-nuggets/brand-seo/how-do-you-establish-a-solid-foundation-for-your-brand-online-optyoumize-podcast/\n",
            "\n",
            "Top 10 recommended links:\n",
            "1. Destination URL: https://kalicube.com/learning-spaces/, Probability: 0.9569\n",
            "2. Destination URL: https://kalicube.com/learning-spaces/faq/seo-glossary/the-12-concrete-results-of-implementing-the-kalicube-process/, Probability: 0.9393\n",
            "3. Destination URL: https://kalicube.com/learning-spaces/faq/brand-serps/what-are-the-kpis-for-the-kalicube-process/, Probability: 0.9391\n",
            "4. Destination URL: https://kalicube.com/learning-spaces/faq/digital-pr/the-kalicube-process/, Probability: 0.9390\n",
            "5. Destination URL: https://kalicube.com/learning-spaces/faq/brand-serps/, Probability: 0.9389\n",
            "6. Destination URL: https://kalicube.com/learning-spaces/faq/digital-pr/, Probability: 0.9389\n",
            "7. Destination URL: https://kalicube.com/learning-spaces/faq/knowledge-panels/, Probability: 0.9389\n",
            "8. Destination URL: https://kalicube.com/learning-spaces/faq/, Probability: 0.9389\n",
            "9. Destination URL: https://kalicube.com/learning-spaces/faq/brand-serps/how-does-the-kalicube-process-work/, Probability: 0.9389\n",
            "10. Destination URL: https://kalicube.com/learning-spaces/knowledge-nuggets/, Probability: 0.9389\n"
          ]
        }
      ]
    }
  ]
}