{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcv9yJ83xbTq"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# --- Step 1: Install Required Libraries ---\n",
    "#\n",
    "!pip install -q torch torch-geometric pandas duckdb pyarrow networkx gradio -q\n",
    "!pip install fireducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSVNtFiVlsLS"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd  # Re-import standard pandas as 'pd'\n",
    "import fireducks.pandas as fpd  # Use 'fpd' for fireducks.pandas to avoid conflict\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import logging\n",
    "import io\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "\n",
    "# Suppress a common warning from the sentence-transformers library (if still present or similar)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",
    ")\n",
    "\n",
    "# --- Google Colab Specific: Import and Mount Google Drive ---\n",
    "# If you are running this code in Google Colab, uncomment the following lines\n",
    "# and run them first to mount your Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "#\n",
    "# --- Step 4: Configuration & Logging Classes ---\n",
    "#\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LinkPredictionConfig:\n",
    "    \"\"\"Holds all configuration for the graph prediction pipeline.\"\"\"\n",
    "\n",
    "    # Input Paths\n",
    "    edge_csv_path: str = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\"\n",
    "    )\n",
    "    embeddings_dir_path: str = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/url_embeddings/\"\n",
    "    )\n",
    "\n",
    "    # Output Artifact Paths\n",
    "    output_dir: str = \"/content/drive/My Drive/master_july_2025/data/prediction_model/\"\n",
    "    model_state_path: str = os.path.join(output_dir, \"graphsage_link_predictor.pth\")\n",
    "    node_embeddings_path: str = os.path.join(output_dir, \"final_node_embeddings.pt\")\n",
    "    node_mapping_path: str = os.path.join(output_dir, \"model_metadata.json\")\n",
    "    edge_index_path: str = os.path.join(output_dir, \"edge_index.pt\")\n",
    "\n",
    "    # Model Hyperparameters\n",
    "    hidden_channels: int = 128\n",
    "    out_channels: int = 64\n",
    "    learning_rate: float = 0.01\n",
    "    epochs: int = 100\n",
    "\n",
    "\n",
    "class ILogger(ABC):\n",
    "    @abstractmethod\n",
    "    def info(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def error(self, message: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ConsoleAndGradioLogger(ILogger):\n",
    "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
    "        self._logger = logging.getLogger(\"GraphLogger\")\n",
    "        self._logger.setLevel(level)\n",
    "        if self._logger.hasHandlers():\n",
    "            self._logger.handlers.clear()\n",
    "        gradio_handler = logging.StreamHandler(log_output_stream)\n",
    "        gradio_handler.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        self._logger.addHandler(gradio_handler)\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        self._logger.addHandler(console_handler)\n",
    "\n",
    "    def info(self, message: str):\n",
    "        self._logger.info(message)\n",
    "\n",
    "    def error(self, message: str):\n",
    "        self._logger.error(message)\n",
    "\n",
    "\n",
    "#\n",
    "# --- Step 5: OOP Component Classes ---\n",
    "#\n",
    "\n",
    "\n",
    "class URLProcessor:\n",
    "    \"\"\"\n",
    "    Handles URL-related operations, specifically calculating folder depth.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_folder_depth(url: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the folder depth of a given URL.\n",
    "        Example: https://kalicube.com/learning-spaces/faq-list/generative-ai/ -> 2\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        if not path or path == \"/\":\n",
    "            return 0\n",
    "        # Remove leading/trailing slashes and split by '/'\n",
    "        segments = [s for s in path.strip(\"/\").split(\"/\") if s]\n",
    "        return len(segments)\n",
    "\n",
    "\n",
    "class GraphDataLoader:\n",
    "    def __init__(self, config: LinkPredictionConfig, logger: ILogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "    def load_data(self):\n",
    "        self.logger.info(\"Loading data using DuckDB...\")\n",
    "        try:\n",
    "            con = duckdb.connect()\n",
    "            all_nodes_query = f\"\"\"\n",
    "                (SELECT \"FROM\" AS url FROM read_csv_auto('{self.config.edge_csv_path}', header=true))\n",
    "                UNION\n",
    "                (SELECT \"TO\" AS url FROM read_csv_auto('{self.config.edge_csv_path}', header=true))\n",
    "            \"\"\"\n",
    "            embeddings_glob_path = os.path.join(\n",
    "                self.config.embeddings_dir_path, \"*.parquet\"\n",
    "            )\n",
    "            node_features_query = f\"\"\"\n",
    "                WITH all_nodes AS ({all_nodes_query})\n",
    "                SELECT n.url, e.Embedding AS features\n",
    "                FROM all_nodes AS n\n",
    "                LEFT JOIN read_parquet('{embeddings_glob_path}') AS e ON n.url = e.URL\n",
    "            \"\"\"\n",
    "            node_features_df = con.execute(node_features_query).fetchdf()\n",
    "            edge_list_df = con.execute(\n",
    "                f\"SELECT * FROM read_csv_auto('{self.config.edge_csv_path}', header=true)\"\n",
    "            ).fetchdf()\n",
    "            self.logger.info(\n",
    "                f\"Loaded {len(edge_list_df)} edges and {len(node_features_df)} unique nodes.\"\n",
    "            )\n",
    "            return node_features_df, edge_list_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load data: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class GraphDataProcessor:\n",
    "    def __init__(self, logger: ILogger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def process(self, node_features_df: pd.DataFrame, edge_list_df: pd.DataFrame):\n",
    "        self.logger.info(\n",
    "            \"Processing data into tensors with neighbor feature inference...\"\n",
    "        )\n",
    "        url_to_features = node_features_df.set_index(\"url\").features.to_dict()\n",
    "        adj = defaultdict(set)\n",
    "        for _, row in edge_list_df.iterrows():\n",
    "            adj[row[\"FROM\"]].add(row[\"TO\"])\n",
    "            adj[row[\"TO\"]].add(row[\"FROM\"])\n",
    "\n",
    "        imputed_features = {}\n",
    "        nodes_with_missing_features_count = 0\n",
    "\n",
    "        valid_features = node_features_df[\"features\"].dropna()\n",
    "        if valid_features.empty:\n",
    "            raise ValueError(\n",
    "                \"No nodes with features found. Cannot determine feature dimension.\"\n",
    "            )\n",
    "        feature_dim = len(valid_features.iloc[0])\n",
    "        self.logger.info(f\"Detected feature dimension: {feature_dim}\")\n",
    "\n",
    "        for url, features in url_to_features.items():\n",
    "            is_missing = pd.isna(features)\n",
    "            if (isinstance(is_missing, bool) and is_missing) or (\n",
    "                hasattr(is_missing, \"any\") and is_missing.any()\n",
    "            ):\n",
    "                nodes_with_missing_features_count += 1\n",
    "                neighbors = adj.get(url, set())\n",
    "                neighbor_features = [\n",
    "                    np.array(url_to_features.get(n), dtype=np.float32)\n",
    "                    for n in neighbors\n",
    "                    if url_to_features.get(n) is not None\n",
    "                    and not pd.isna(url_to_features.get(n)).any()\n",
    "                ]\n",
    "                if neighbor_features:\n",
    "                    imputed_features[url] = np.mean(neighbor_features, axis=0)\n",
    "                else:\n",
    "                    imputed_features[url] = np.zeros(feature_dim, dtype=np.float32)\n",
    "            else:\n",
    "                imputed_features[url] = np.array(features, dtype=np.float32)\n",
    "        if nodes_with_missing_features_count > 0:\n",
    "            self.logger.info(\n",
    "                f\"Imputed features for {nodes_with_missing_features_count} nodes.\"\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"Constructing final PyTorch tensors...\")\n",
    "        node_list = node_features_df[\"url\"].tolist()\n",
    "        url_to_idx = {url: i for i, url in enumerate(node_list)}\n",
    "        final_feature_list = [imputed_features[url] for url in node_list]\n",
    "        x = torch.tensor(np.array(final_feature_list), dtype=torch.float)\n",
    "        source_indices = [url_to_idx.get(url) for url in edge_list_df[\"FROM\"]]\n",
    "        dest_indices = [url_to_idx.get(url) for url in edge_list_df[\"TO\"]]\n",
    "        edge_index = torch.tensor([source_indices, dest_indices], dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        self.logger.info(\n",
    "            f\"Created PyG Data object with {data.num_nodes} nodes and {data.num_edges} edges.\"\n",
    "        )\n",
    "        return data, url_to_idx\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def predict_link(self, z, edge_label_index):\n",
    "        source_emb = z[edge_label_index[0]]\n",
    "        dest_emb = z[edge_label_index[1]]\n",
    "        return (source_emb * dest_emb).sum(dim=-1)\n",
    "\n",
    "\n",
    "class LinkPredictionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GraphSAGEModel,\n",
    "        data: Data,\n",
    "        config: LinkPredictionConfig,\n",
    "        logger: ILogger,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def _get_negative_samples(self):\n",
    "        return torch.randint(\n",
    "            0, self.data.num_nodes, (2, self.data.num_edges), dtype=torch.long\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        edge_label_index = torch.cat(\n",
    "            [self.data.edge_index, self._get_negative_samples()], dim=1\n",
    "        )\n",
    "        edge_label = torch.cat(\n",
    "            [torch.ones(self.data.num_edges), torch.zeros(self.data.num_edges)], dim=0\n",
    "        )\n",
    "        for epoch in range(1, self.config.epochs + 1):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            z = self.model(self.data.x, self.data.edge_index)\n",
    "            out = self.model.predict_link(z, edge_label_index)\n",
    "            loss = self.criterion(out, edge_label)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            yield epoch, loss.item()\n",
    "\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"Loads trained artifacts and provides link recommendations using a Top-K strategy.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: LinkPredictionConfig, logger: ILogger, url_processor: URLProcessor\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.url_processor = url_processor\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # Artifacts will be loaded on the first request\n",
    "        self.model = None\n",
    "        self.node_embeddings = None\n",
    "        self.url_to_idx = None\n",
    "        self.idx_to_url = None\n",
    "        self.existing_edges = None\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        \"\"\"Loads the trained model, embeddings, and mappings into memory.\"\"\"\n",
    "        if self.model is not None:\n",
    "            return True  # Already loaded\n",
    "\n",
    "        self.logger.info(\"Loading trained artifacts for recommendations...\")\n",
    "        try:\n",
    "            # Load the metadata file which contains the model's \"blueprint\"\n",
    "            with open(self.config.node_mapping_path, \"r\") as f:\n",
    "                model_metadata = json.load(f)\n",
    "\n",
    "            self.url_to_idx = model_metadata[\"url_to_idx\"]\n",
    "            in_channels = model_metadata[\"in_channels\"]\n",
    "            hidden_channels = model_metadata[\"hidden_channels\"]\n",
    "            out_channels = model_metadata[\"out_channels\"]\n",
    "\n",
    "            self.idx_to_url = {v: k for k, v in self.url_to_idx.items()}\n",
    "\n",
    "            # Load the final tensors\n",
    "            self.node_embeddings = torch.load(self.config.node_embeddings_path).to(\n",
    "                self.device\n",
    "            )\n",
    "            edge_index = torch.load(self.config.edge_index_path)\n",
    "            self.existing_edges = set(\n",
    "                zip(edge_index[0].tolist(), edge_index[1].tolist())\n",
    "            )\n",
    "\n",
    "            # Recreate model with the correct dimensions and load its state\n",
    "            self.model = GraphSAGEModel(in_channels, hidden_channels, out_channels)\n",
    "            self.model.load_state_dict(torch.load(self.config.model_state_path))\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "            self.logger.info(\"Artifacts loaded successfully.\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(\n",
    "                \"Could not find trained model artifacts. Please run the training pipeline first.\"\n",
    "            )\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while loading artifacts: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_recommendations(\n",
    "        self,\n",
    "        source_url: str,\n",
    "        top_n: int = 20,\n",
    "        min_folder_depth: int = 0,\n",
    "        max_folder_depth: int = 10,\n",
    "    ):\n",
    "        if not self.load_artifacts():\n",
    "            return (\n",
    "                None,\n",
    "                \"Error: Trained model artifacts not found. Please run the training pipeline first.\",\n",
    "            )\n",
    "        if source_url not in self.url_to_idx:\n",
    "            return (\n",
    "                None,\n",
    "                f\"Error: Source URL '{source_url}' not found in the graph's training data.\",\n",
    "            )\n",
    "\n",
    "        source_idx = self.url_to_idx[source_url]\n",
    "        num_nodes = len(self.url_to_idx)\n",
    "\n",
    "        # 1. Create candidate edges from the source to ALL other nodes\n",
    "        candidate_dest_indices = torch.arange(num_nodes, device=self.device)\n",
    "        candidate_source_indices = torch.full_like(\n",
    "            candidate_dest_indices, fill_value=source_idx\n",
    "        )\n",
    "        candidate_edge_index = torch.stack(\n",
    "            [candidate_source_indices, candidate_dest_indices]\n",
    "        )\n",
    "\n",
    "        # 2. Score all candidates at once\n",
    "        with torch.no_grad():\n",
    "            scores = self.model.predict_link(self.node_embeddings, candidate_edge_index)\n",
    "\n",
    "        # 3. Find a larger set of top K highest-scoring candidates to allow for filtering\n",
    "        # We get more than `top_n` to account for filtering out existing links and folder depth.\n",
    "        k = min(num_nodes, top_n + 200)  # Increased buffer\n",
    "        top_scores, top_indices = torch.topk(scores, k=k)\n",
    "\n",
    "        # 4. Filter this list to find novel recommendations and add folder depth\n",
    "        recommendations = []\n",
    "        for i in range(k):\n",
    "            dest_idx = top_indices[i].item()\n",
    "            recommended_url = self.idx_to_url[dest_idx]\n",
    "\n",
    "            # Stop if we've found enough recommendations AND we've processed a reasonable number of candidates\n",
    "            if (\n",
    "                len(recommendations) >= top_n and i > top_n * 2\n",
    "            ):  # added second condition to ensure we try enough candidates\n",
    "                break\n",
    "\n",
    "            # Check if the candidate is valid (not the source itself and not an existing link)\n",
    "            is_self_link = dest_idx == source_idx\n",
    "            is_existing_link = (source_idx, dest_idx) in self.existing_edges\n",
    "\n",
    "            if not is_self_link and not is_existing_link:\n",
    "                folder_depth = self.url_processor.get_folder_depth(recommended_url)\n",
    "                # Apply folder depth filter\n",
    "                if min_folder_depth <= folder_depth <= max_folder_depth:\n",
    "                    recommendations.append(\n",
    "                        {\n",
    "                            \"RECOMMENDED_URL\": recommended_url,\n",
    "                            \"SCORE\": torch.sigmoid(top_scores[i]).item(),\n",
    "                            \"FOLDER_DEPTH\": folder_depth,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Ensure we return exactly top_n if available, sorted by score\n",
    "        # Using standard pandas DataFrame for the final output to Gradio\n",
    "        final_recommendations_df = (\n",
    "            pd.DataFrame(recommendations)\n",
    "            .sort_values(by=\"SCORE\", ascending=False)\n",
    "            .head(top_n)\n",
    "        )\n",
    "\n",
    "        if final_recommendations_df.empty:\n",
    "            return (\n",
    "                None,\n",
    "                \"No recommendations found matching the criteria (filters, existing links, etc.). Try adjusting filters or source URL.\",\n",
    "            )\n",
    "\n",
    "        return final_recommendations_df, None\n",
    "\n",
    "\n",
    "#\n",
    "# --- Main Gradio Application Functions ---\n",
    "#\n",
    "\n",
    "\n",
    "def run_training_pipeline(\n",
    "    csv_path,\n",
    "    embeddings_path,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    lr,\n",
    "    epochs,\n",
    "    progress=gr.Progress(track_tqdm=True),\n",
    "):\n",
    "    log_stream = io.StringIO()\n",
    "    logger = ConsoleAndGradioLogger(log_stream)\n",
    "\n",
    "    # Initialize choices for the dropdown - assuming they might be empty initially\n",
    "    dropdown_choices = get_all_nodes_for_dropdown()\n",
    "\n",
    "    try:\n",
    "        yield \"Step 1/5: Initializing...\", log_stream.getvalue(), None, dropdown_choices\n",
    "        config = LinkPredictionConfig(\n",
    "            edge_csv_path=csv_path,\n",
    "            embeddings_dir_path=embeddings_path,\n",
    "            hidden_channels=int(hidden_channels),\n",
    "            out_channels=int(out_channels),\n",
    "            learning_rate=lr,\n",
    "            epochs=int(epochs),\n",
    "        )\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "        # --- ARTIFACT EXISTENCE CHECK ---\n",
    "        all_artifacts_exist = (\n",
    "            os.path.exists(config.model_state_path)\n",
    "            and os.path.exists(config.node_embeddings_path)\n",
    "            and os.path.exists(config.node_mapping_path)\n",
    "            and os.path.exists(config.edge_index_path)\n",
    "        )\n",
    "\n",
    "        if all_artifacts_exist:\n",
    "            status_message = (\n",
    "                \"âœ… All artifacts already exist. Skipping training and saving.\"\n",
    "            )\n",
    "            logger.info(status_message)\n",
    "            # Fetch the latest dropdown choices *after* confirming artifacts exist\n",
    "            # This ensures the dropdown is populated if artifacts were just created externally\n",
    "            dropdown_choices = get_all_nodes_for_dropdown()\n",
    "            # Ensure the DataFrame passed to Gradio is a standard pandas DataFrame\n",
    "            yield (\n",
    "                status_message,\n",
    "                log_stream.getvalue(),\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Message\": [\n",
    "                            \"Artifacts found. You can now use the recommendation tab.\"\n",
    "                        ]\n",
    "                    },\n",
    "                    columns=[\"Message\"],\n",
    "                ),\n",
    "                dropdown_choices,\n",
    "            )\n",
    "            return  # Exit the function early\n",
    "\n",
    "        yield (\n",
    "            \"Step 2/5: Loading & processing data...\",\n",
    "            log_stream.getvalue(),\n",
    "            None,\n",
    "            dropdown_choices,\n",
    "        )\n",
    "        loader = GraphDataLoader(config, logger)\n",
    "        node_features_df, edge_list_df = (\n",
    "            loader.load_data()\n",
    "        )  # These are already standard pandas DataFrames\n",
    "        processor = GraphDataProcessor(logger)\n",
    "        data, url_to_idx = processor.process(node_features_df, edge_list_df)\n",
    "\n",
    "        yield (\n",
    "            \"Step 3/5: Initializing model...\",\n",
    "            log_stream.getvalue(),\n",
    "            None,\n",
    "            dropdown_choices,\n",
    "        )\n",
    "        model = GraphSAGEModel(\n",
    "            in_channels=data.num_node_features,\n",
    "            hidden_channels=config.hidden_channels,\n",
    "            out_channels=config.out_channels,\n",
    "        )\n",
    "        trainer = LinkPredictionTrainer(model, data, config, logger)\n",
    "\n",
    "        yield (\n",
    "            \"Step 4/5: Training model...\",\n",
    "            log_stream.getvalue(),\n",
    "            None,\n",
    "            dropdown_choices,\n",
    "        )\n",
    "        for epoch, loss in progress.tqdm(\n",
    "            trainer.train(), total=config.epochs, desc=\"Training Model\"\n",
    "        ):\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                logger.info(f\"Epoch {epoch}/{config.epochs}, Loss: {loss:.4f}\")\n",
    "            # Potentially yield intermediate progress if needed, but not necessarily dropdown_choices here\n",
    "\n",
    "        yield (\n",
    "            \"Step 5/5: Evaluating and saving artifacts...\",\n",
    "            log_stream.getvalue(),\n",
    "            None,\n",
    "            dropdown_choices,\n",
    "        )\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            final_node_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "        # Save the model's architecture metadata along with the URL mapping\n",
    "        logger.info(f\"Saving model metadata to {config.node_mapping_path}\")\n",
    "        model_metadata = {\n",
    "            \"url_to_idx\": url_to_idx,\n",
    "            \"in_channels\": data.num_node_features,\n",
    "            \"hidden_channels\": config.hidden_channels,\n",
    "            \"out_channels\": config.out_channels,\n",
    "        }\n",
    "        with open(config.node_mapping_path, \"w\") as f:\n",
    "            json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Saving model weights to {config.model_state_path}\")\n",
    "        torch.save(model.state_dict(), config.model_state_path)\n",
    "        logger.info(f\"Saving final node embeddings to {config.node_embeddings_path}\")\n",
    "        torch.save(final_node_embeddings, config.node_embeddings_path)\n",
    "        logger.info(f\"Saving edge index to {config.edge_index_path}\")\n",
    "        torch.save(data.edge_index, config.edge_index_path)\n",
    "\n",
    "        final_status = \"âœ… Pipeline Finished Successfully!\"\n",
    "        logger.info(final_status)\n",
    "        # Fetch the latest dropdown choices after saving artifacts\n",
    "        dropdown_choices = get_all_nodes_for_dropdown()\n",
    "        # Ensure the DataFrame passed to Gradio is a standard pandas DataFrame\n",
    "        yield (\n",
    "            final_status,\n",
    "            log_stream.getvalue(),\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Message\": [\n",
    "                        \"Artifacts saved successfully. You can now use the recommendation tab.\"\n",
    "                    ]\n",
    "                },\n",
    "                columns=[\"Message\"],\n",
    "            ),\n",
    "            dropdown_choices,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"A critical error occurred: {e}\")\n",
    "        # Ensure the DataFrame passed to Gradio is a standard pandas DataFrame\n",
    "        # Also, ensure dropdown choices are provided even in case of error\n",
    "        yield (\n",
    "            \"Pipeline Failed\",\n",
    "            log_stream.getvalue(),\n",
    "            pd.DataFrame({\"Error\": [str(e)]}),\n",
    "            dropdown_choices,\n",
    "        )\n",
    "\n",
    "\n",
    "def get_all_nodes_for_dropdown():\n",
    "    \"\"\"\n",
    "    Dynamically loads node URLs from the saved model metadata (artifacts).\n",
    "    If artifacts are not found, it returns a message indicating training is needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = LinkPredictionConfig()  # Use default paths from config\n",
    "        model_metadata_path = config.node_mapping_path\n",
    "\n",
    "        if os.path.exists(model_metadata_path):\n",
    "            with open(model_metadata_path, \"r\") as f:\n",
    "                model_metadata = json.load(f)\n",
    "            if \"url_to_idx\" in model_metadata:\n",
    "                url_to_idx = model_metadata[\"url_to_idx\"]\n",
    "                return sorted(list(url_to_idx.keys()))\n",
    "            else:\n",
    "                return [\"Error: Model metadata is incomplete (missing url_to_idx).\"]\n",
    "        else:\n",
    "            return [\"Run training first to generate artifacts.\"]\n",
    "    except Exception as e:\n",
    "        # Catch any other errors during file reading or JSON parsing\n",
    "        return [\n",
    "            f\"Could not load URLs from artifacts: {e}. Ensure Google Drive is mounted and artifacts exist.\"\n",
    "        ]\n",
    "\n",
    "\n",
    "def run_recommendation_interface(source_url: str, min_depth: int, max_depth: int):\n",
    "    if not source_url:\n",
    "        return None, \"Please select a source URL from the dropdown.\"\n",
    "    log_stream = io.StringIO()\n",
    "    logger = ConsoleAndGradioLogger(log_stream)\n",
    "    config = LinkPredictionConfig()\n",
    "    url_processor = URLProcessor()\n",
    "    engine = RecommendationEngine(config, logger, url_processor)\n",
    "\n",
    "    # Validate depth inputs\n",
    "    if min_depth is None:\n",
    "        min_depth = 0\n",
    "    if max_depth is None:\n",
    "        max_depth = 10  # A reasonable default\n",
    "    if min_depth > max_depth:\n",
    "        return (\n",
    "            None,\n",
    "            \"Error: Minimum folder depth cannot be greater than maximum folder depth.\",\n",
    "        )\n",
    "\n",
    "    recommendations_df, error_msg = engine.get_recommendations(\n",
    "        source_url, top_n=20, min_folder_depth=min_depth, max_folder_depth=max_depth\n",
    "    )\n",
    "    if error_msg:\n",
    "        logger.error(error_msg)\n",
    "        return (\n",
    "            None,\n",
    "            log_stream.getvalue(),\n",
    "        )  # Return None for DataFrame and the error in logs\n",
    "\n",
    "    if recommendations_df is None or recommendations_df.empty:\n",
    "        logger.info(\"No recommendations found matching the specified filters.\")\n",
    "        # Ensure the DataFrame passed to Gradio is a standard pandas DataFrame\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"Message\": [\n",
    "                    \"No recommendations found matching the specified filters. Try adjusting your depth range.\"\n",
    "                ]\n",
    "            },\n",
    "            columns=[\"Message\"],\n",
    "        ), log_stream.getvalue()\n",
    "\n",
    "    return recommendations_df, log_stream.getvalue()\n",
    "\n",
    "\n",
    "#\n",
    "# --- Gradio UI Definition ---\n",
    "#\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ðŸ“ˆ GNN Link Prediction & Recommendation Engine\")\n",
    "    gr.Markdown(\n",
    "        \"First, use the 'Train Model' tab to process your data. Then, use the 'Get Link Recommendations' tab to get predictions for new, non-existent links.\"\n",
    "    )\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Train Model\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"## 1. Configuration\")\n",
    "                    with gr.Accordion(\"Data Paths\", open=True):\n",
    "                        edge_csv_path_input = gr.Textbox(\n",
    "                            label=\"Edge List CSV Path\",\n",
    "                            value=LinkPredictionConfig.edge_csv_path,\n",
    "                        )\n",
    "                        embeddings_dir_path_input = gr.Textbox(\n",
    "                            label=\"Embeddings Directory Path\",\n",
    "                            value=LinkPredictionConfig.embeddings_dir_path,\n",
    "                        )\n",
    "                    with gr.Accordion(\"Model Hyperparameters\", open=True):\n",
    "                        hidden_channels_input = gr.Number(\n",
    "                            label=\"Hidden Channels\",\n",
    "                            value=LinkPredictionConfig.hidden_channels,\n",
    "                        )\n",
    "                        out_channels_input = gr.Number(\n",
    "                            label=\"Output Embedding Size\",\n",
    "                            value=LinkPredictionConfig.out_channels,\n",
    "                        )\n",
    "                    with gr.Accordion(\"Training Parameters\", open=True):\n",
    "                        learning_rate_input = gr.Number(\n",
    "                            label=\"Learning Rate\",\n",
    "                            value=LinkPredictionConfig.learning_rate,\n",
    "                        )\n",
    "                        epochs_input = gr.Number(\n",
    "                            label=\"Training Epochs\", value=LinkPredictionConfig.epochs\n",
    "                        )\n",
    "                    start_button = gr.Button(\n",
    "                        \"Train Link Prediction Model\", variant=\"primary\"\n",
    "                    )\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"## 2. Training Status\")\n",
    "                    train_status_output = gr.Textbox(\n",
    "                        label=\"Current Status\", interactive=False\n",
    "                    )\n",
    "                    train_log_output = gr.Textbox(\n",
    "                        label=\"Pipeline Logs\", interactive=False, lines=15\n",
    "                    )\n",
    "                    train_results_output = gr.DataFrame(\n",
    "                        label=\"Training Completion Status\"\n",
    "                    )\n",
    "\n",
    "        with gr.TabItem(\"Get Link Recommendations\"):\n",
    "            gr.Markdown(\"## 1. Select a Source Page & Filters\")\n",
    "            gr.Markdown(\n",
    "                \"Choose a URL and the model will recommend top pages it should link to. (You must train the model on the tab to the left first).\"\n",
    "            )\n",
    "            with gr.Row():\n",
    "                # Pass the choices argument directly to the Dropdown at initialization\n",
    "                source_url_dropdown = gr.Dropdown(\n",
    "                    label=\"Source URL\",\n",
    "                    choices=get_all_nodes_for_dropdown(),\n",
    "                    interactive=True,\n",
    "                )\n",
    "            with gr.Row():\n",
    "                min_folder_depth_input = gr.Number(\n",
    "                    label=\"Minimum Folder Depth\", value=0, precision=0\n",
    "                )  # precision=0 for integers\n",
    "                max_folder_depth_input = gr.Number(\n",
    "                    label=\"Maximum Folder Depth\", value=100, precision=0\n",
    "                )  # precision=0 for integers\n",
    "\n",
    "            recommend_button = gr.Button(\"Get Recommendations\", variant=\"primary\")\n",
    "            gr.Markdown(\"## 2. Results: High-Potential Missing Links\")\n",
    "            recommend_results_output = gr.DataFrame(\n",
    "                label=\"Top Link Recommendations\",\n",
    "                headers=[\"RECOMMENDED_URL\", \"SCORE\", \"FOLDER_DEPTH\"],\n",
    "            )\n",
    "            recommend_log_output = gr.Textbox(label=\"Logs\", interactive=False, lines=4)\n",
    "\n",
    "    start_button.click(\n",
    "        fn=run_training_pipeline,\n",
    "        inputs=[\n",
    "            edge_csv_path_input,\n",
    "            embeddings_dir_path_input,\n",
    "            hidden_channels_input,\n",
    "            out_channels_input,\n",
    "            learning_rate_input,\n",
    "            epochs_input,\n",
    "        ],\n",
    "        # Add source_url_dropdown as an output, so it gets updated\n",
    "        outputs=[\n",
    "            train_status_output,\n",
    "            train_log_output,\n",
    "            train_results_output,\n",
    "            source_url_dropdown,\n",
    "        ],\n",
    "    )\n",
    "    # The .then() chain is no longer needed here, as the dropdown is updated directly by run_training_pipeline.\n",
    "\n",
    "    recommend_button.click(\n",
    "        fn=run_recommendation_interface,\n",
    "        inputs=[source_url_dropdown, min_folder_depth_input, max_folder_depth_input],\n",
    "        outputs=[recommend_results_output, recommend_log_output],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Assuming this is run in Colab, mount drive first.\n",
    "        # If running locally, you might comment this out and ensure paths are correct.\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive/\", force_remount=True)\n",
    "        demo.launch(debug=True, share=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not launch Gradio demo in this environment: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}