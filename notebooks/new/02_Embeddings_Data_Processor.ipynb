{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wx5YXxqgrbEH"
   },
   "outputs": [],
   "source": [
    "!pip install trafilatura sentence-transformers torch pandas pyarrow duckdb scipy -q\n",
    "!pip install fireducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QO9Ir6L3r1Dd"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# --- Step 2: Import Libraries ---\n",
    "#\n",
    "import gradio as gr\n",
    "import duckdb\n",
    "\n",
    "# import pandas as pd\n",
    "import fireducks.pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "import trafilatura\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "# Suppress a common warning from the sentence-transformers library\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "# --- Step 3: Configuration & Core Interfaces ---\n",
    "#\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Holds all configuration settings for the pipeline.\"\"\"\n",
    "\n",
    "    input_path: str = (\n",
    "        \"/content/drive/My Drive/master_july_2025/data/crawled_data_parquet/\"\n",
    "    )\n",
    "    output_path: str = \"/content/drive/My Drive/master_july_2025/data/url_embeddings/\"\n",
    "    model_name: str = \"all-MiniLM-L6-v2\"  # should be changed to other/multilingual content if the content is not in English\n",
    "    batch_size: int = 10\n",
    "\n",
    "\n",
    "class ILogger(ABC):\n",
    "    \"\"\"Interface for logging messages.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def info(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def error(self, message: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def exception(self, message: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ConsoleAndGradioLogger(ILogger):\n",
    "    \"\"\"Logs messages to the console and a Gradio UI component.\"\"\"\n",
    "\n",
    "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
    "        self._logger = logging.getLogger(\"EmbeddingLogger\")\n",
    "        self._logger.setLevel(level)\n",
    "        if self._logger.hasHandlers():\n",
    "            self._logger.handlers.clear()\n",
    "\n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        self._logger.addHandler(console_handler)\n",
    "\n",
    "        # Gradio handler\n",
    "        gradio_handler = logging.StreamHandler(log_output_stream)\n",
    "        gradio_handler.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        self._logger.addHandler(gradio_handler)\n",
    "\n",
    "    def info(self, message: str):\n",
    "        self._logger.info(message)\n",
    "\n",
    "    def error(self, message: str):\n",
    "        self._logger.error(message)\n",
    "\n",
    "    def exception(self, message: str):\n",
    "        self._logger.exception(message)\n",
    "\n",
    "\n",
    "#\n",
    "# --- Step 4: Component Classes (Single Responsibility Principle) ---\n",
    "#\n",
    "\n",
    "\n",
    "class EmbeddingStateManager:\n",
    "    \"\"\"Manages the state of the embedding process, enabling resumes.\"\"\"\n",
    "\n",
    "    def __init__(self, output_path: str, logger: ILogger):\n",
    "        self.output_path = output_path\n",
    "        self.logger = logger\n",
    "\n",
    "    def get_processed_urls(self) -> set:\n",
    "        \"\"\"Scans the output directory to find URLs that have already been embedded.\"\"\"\n",
    "        processed_urls = set()\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "            self.logger.info(\"Output directory created.\")\n",
    "            return processed_urls\n",
    "\n",
    "        try:\n",
    "            output_glob_path = os.path.join(self.output_path, \"*.parquet\")\n",
    "            # Use DuckDB for efficient scanning of existing results\n",
    "            processed_df = duckdb.query(\n",
    "                f\"SELECT DISTINCT URL FROM read_parquet('{output_glob_path}')\"\n",
    "            ).to_df()\n",
    "            processed_urls = set(processed_df[\"URL\"])\n",
    "            if processed_urls:\n",
    "                self.logger.info(\n",
    "                    f\"Found {len(processed_urls)} URLs that have already been processed. They will be skipped.\"\n",
    "                )\n",
    "        except Exception:\n",
    "            self.logger.info(\n",
    "                \"No previously processed embeddings found. Starting fresh.\"\n",
    "            )\n",
    "        return processed_urls\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Responsible for loading unprocessed data in batches.\"\"\"\n",
    "\n",
    "    def __init__(self, input_path: str, logger: ILogger):\n",
    "        self.input_path = input_path\n",
    "        self.logger = logger\n",
    "        self.con = duckdb.connect()\n",
    "\n",
    "    def stream_unprocessed_data(self, processed_urls: set, batch_size: int):\n",
    "        \"\"\"A generator that yields batches of new data to be processed.\"\"\"\n",
    "        input_glob_path = os.path.join(self.input_path, \"**\", \"*.parquet\")\n",
    "        base_query = f\"SELECT URL, Content FROM read_parquet('{input_glob_path}') WHERE Status_Code >= 200 AND Status_Code < 300 AND Content IS NOT NULL AND Content != ''\"\n",
    "\n",
    "        if processed_urls:\n",
    "            processed_urls_df = pd.DataFrame(list(processed_urls), columns=[\"URL\"])\n",
    "\n",
    "            # --- THIS IS THE FIX ---\n",
    "            # We replace the non-standard \"LEFT ANTI JOIN\" with a standard\n",
    "            # \"LEFT JOIN\" and a \"WHERE ... IS NULL\" check. This achieves the same goal.\n",
    "            final_query = f\"\"\"\n",
    "                SELECT t1.URL, t1.Content\n",
    "                FROM ({base_query}) AS t1\n",
    "                LEFT JOIN processed_urls_df AS t2 ON t1.URL = t2.URL\n",
    "                WHERE t2.URL IS NULL\n",
    "            \"\"\"\n",
    "            # --- END OF FIX ---\n",
    "        else:\n",
    "            final_query = base_query\n",
    "\n",
    "        self.logger.info(\"Querying for new pages to process...\")\n",
    "        try:\n",
    "            # Use fetch_record_batch for memory-efficient iteration\n",
    "            for batch in self.con.execute(final_query).fetch_record_batch(batch_size):\n",
    "                yield batch.to_pandas()\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Could not query Parquet files. Please check the input path: {e}\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "\n",
    "class TextExtractor:\n",
    "    \"\"\"Extracts clean text from raw HTML.\"\"\"\n",
    "\n",
    "    def extract(self, html_content: str) -> str:\n",
    "        if not html_content or not isinstance(html_content, str):\n",
    "            return \"\"\n",
    "        text = trafilatura.extract(\n",
    "            html_content, include_comments=False, include_tables=False, deduplicate=True\n",
    "        )\n",
    "        if text:\n",
    "            text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "            return text.strip()\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generates embeddings for a list of texts.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, logger: ILogger):\n",
    "        self.logger = logger\n",
    "        self.logger.info(f\"Loading embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.logger.info(\"Model loaded successfully.\")\n",
    "\n",
    "    def generate(self, texts: list[str]) -> np.ndarray:\n",
    "        self.logger.info(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "class DataSaver:\n",
    "    \"\"\"Saves a batch of embeddings to a Parquet file.\"\"\"\n",
    "\n",
    "    def __init__(self, output_path: str, logger: ILogger):\n",
    "        self.output_path = output_path\n",
    "        self.logger = logger\n",
    "\n",
    "    def save_batch(self, df_batch: pd.DataFrame, batch_num: int):\n",
    "        \"\"\"Saves a DataFrame of URLs and embeddings to a uniquely named file.\"\"\"\n",
    "        batch_filename = f\"embeddings_batch_{int(time.time())}_{batch_num}.parquet\"\n",
    "        batch_output_path = os.path.join(self.output_path, batch_filename)\n",
    "        df_batch.to_parquet(batch_output_path, index=False)\n",
    "        self.logger.info(f\"âœ… Saved batch {batch_num} to {batch_filename}\")\n",
    "\n",
    "\n",
    "#\n",
    "# --- Step 5: The Main Pipeline Orchestrator ---\n",
    "#\n",
    "\n",
    "\n",
    "class EmbeddingPipeline:\n",
    "    \"\"\"Orchestrates the entire embedding generation process.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: EmbeddingConfig,\n",
    "        logger: ILogger,\n",
    "        state_manager: EmbeddingStateManager,\n",
    "        data_loader: DataLoader,\n",
    "        text_extractor: TextExtractor,\n",
    "        embedding_generator: EmbeddingGenerator,\n",
    "        data_saver: DataSaver,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.state_manager = state_manager\n",
    "        self.data_loader = data_loader\n",
    "        self.text_extractor = text_extractor\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.data_saver = data_saver\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"A generator that executes the pipeline and yields status updates.\"\"\"\n",
    "        try:\n",
    "            yield \"Initializing...\"\n",
    "            processed_urls = self.state_manager.get_processed_urls()\n",
    "\n",
    "            yield \"Loading model and querying data...\"\n",
    "            data_stream = self.data_loader.stream_unprocessed_data(\n",
    "                processed_urls, self.config.batch_size\n",
    "            )\n",
    "\n",
    "            batch_num = 1\n",
    "            processed_in_this_session = False\n",
    "            for df_batch in data_stream:\n",
    "                processed_in_this_session = True\n",
    "                status_msg = f\"Processing Batch {batch_num} ({len(df_batch)} pages)...\"\n",
    "                self.logger.info(status_msg)\n",
    "                yield status_msg\n",
    "\n",
    "                # Extract Text\n",
    "                df_batch[\"clean_text\"] = [\n",
    "                    self.text_extractor.extract(html)\n",
    "                    for html in tqdm(df_batch[\"Content\"], desc=\"Extracting Text\")\n",
    "                ]\n",
    "                df_batch = df_batch[df_batch[\"clean_text\"].str.len() > 100]\n",
    "\n",
    "                if df_batch.empty:\n",
    "                    self.logger.info(\n",
    "                        \"Batch had no pages with sufficient text after cleaning.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Generate Embeddings\n",
    "                embeddings = self.embedding_generator.generate(\n",
    "                    df_batch[\"clean_text\"].tolist()\n",
    "                )\n",
    "\n",
    "                # Save Batch\n",
    "                output_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"URL\": df_batch[\"URL\"],\n",
    "                        \"Embedding\": [e.tolist() for e in embeddings],\n",
    "                    }\n",
    "                )\n",
    "                self.data_saver.save_batch(output_df, batch_num)\n",
    "                batch_num += 1\n",
    "\n",
    "            if not processed_in_this_session:\n",
    "                self.logger.info(\n",
    "                    \"No new pages to process. The dataset is already up to date.\"\n",
    "                )\n",
    "                yield \"Already up to date.\"\n",
    "            else:\n",
    "                self.logger.info(\"All new batches processed successfully.\")\n",
    "                yield \"Finished\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"A critical pipeline error occurred: {e}\")\n",
    "            yield f\"Error: {e}\"\n",
    "\n",
    "\n",
    "#\n",
    "# --- Step 6: Gradio UI and Main Execution Logic ---\n",
    "#\n",
    "\n",
    "\n",
    "def run_gradio_interface(input_path: str, output_path: str, batch_size: int):\n",
    "    \"\"\"Wires up all components and runs the pipeline, yielding UI updates.\"\"\"\n",
    "    log_stream = io.StringIO()\n",
    "    logger = ConsoleAndGradioLogger(log_stream)\n",
    "\n",
    "    config = EmbeddingConfig(\n",
    "        input_path=input_path, output_path=output_path, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Instantiate all our components\n",
    "    state_manager = EmbeddingStateManager(config.output_path, logger)\n",
    "    data_loader = DataLoader(config.input_path, logger)\n",
    "    text_extractor = TextExtractor()\n",
    "    embedding_generator = EmbeddingGenerator(config.model_name, logger)\n",
    "    data_saver = DataSaver(config.output_path, logger)\n",
    "\n",
    "    pipeline = EmbeddingPipeline(\n",
    "        config,\n",
    "        logger,\n",
    "        state_manager,\n",
    "        data_loader,\n",
    "        text_extractor,\n",
    "        embedding_generator,\n",
    "        data_saver,\n",
    "    )\n",
    "\n",
    "    final_status = \"Initializing...\"\n",
    "    for status in pipeline.run():\n",
    "        final_status = status\n",
    "        # Yield the current status and the full log content\n",
    "        yield status, log_stream.getvalue(), \"\"\n",
    "\n",
    "    # Generate final summary after the pipeline finishes\n",
    "    try:\n",
    "        output_glob_path = os.path.join(output_path, \"*.parquet\")\n",
    "        total_embeddings = duckdb.query(\n",
    "            f\"SELECT COUNT(URL) FROM read_parquet('{output_glob_path}')\"\n",
    "        ).fetchone()[0]\n",
    "        summary_md = f\"### âœ… Pipeline Finished\\n\\n- **Final Status:** {final_status}\\n- **Total embeddings saved:** {total_embeddings}\\n- **Output location:** `{output_path}`\"\n",
    "    except Exception as e:\n",
    "        summary_md = (\n",
    "            f\"### Pipeline Finished\\n\\n- Could not generate summary. Error: {e}\"\n",
    "        )\n",
    "\n",
    "    yield final_status, log_stream.getvalue(), summary_md\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– Resumable Embedding Pipeline\")\n",
    "    gr.Markdown(\n",
    "        \"This tool reads HTML from Parquet files, cleans it, generates embeddings, and saves the results in batches. It can be stopped and resumed at any time.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## 1. Configuration\")\n",
    "            input_path_box = gr.Textbox(\n",
    "                label=\"Input Parquet Folder Path\", value=EmbeddingConfig.input_path\n",
    "            )\n",
    "            output_path_box = gr.Textbox(\n",
    "                label=\"Output Embeddings Directory Path\",\n",
    "                value=EmbeddingConfig.output_path,\n",
    "            )\n",
    "            batch_size_slider = gr.Slider(\n",
    "                minimum=10,\n",
    "                maximum=50,\n",
    "                value=EmbeddingConfig.batch_size,\n",
    "                step=10,\n",
    "                label=\"Batch Size\",\n",
    "                info=\"How many pages to process in memory at a time.\",\n",
    "            )\n",
    "            start_button = gr.Button(\n",
    "                \"ðŸš€ Start/Resume Embedding Generation\", variant=\"primary\"\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"## 2. Status & Results\")\n",
    "            status_output = gr.Textbox(label=\"Current Status\", interactive=False)\n",
    "            log_output = gr.Textbox(\n",
    "                label=\"Detailed Logs\", interactive=False, lines=10, max_lines=20\n",
    "            )\n",
    "            summary_output = gr.Markdown(\"---\")\n",
    "\n",
    "    start_button.click(\n",
    "        fn=run_gradio_interface,\n",
    "        inputs=[input_path_box, output_path_box, batch_size_slider],\n",
    "        outputs=[status_output, log_output, summary_output],\n",
    "    )\n",
    "\n",
    "#\n",
    "# --- Launch the Application ---\n",
    "#\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive/\", force_remount=True)\n",
    "        demo.launch(debug=True, share=True)\n",
    "    except Exception as e:\n",
    "        print(\"Could not launch Gradio demo in this environment.\")\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}