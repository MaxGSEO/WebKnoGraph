{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Always pefrorm Runtime -> Restart session"
      ],
      "metadata": {
        "id": "RydrBsWi3GyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary packages\n",
        "!pip install pandas pyarrow duckdb gradio requests beautifulsoup4 lxml tqdm -q\n",
        "!pip install fireducks\n",
        "\n",
        "from urllib.parse import urlparse, parse_qs"
      ],
      "metadata": {
        "id": "q-ts3S8AJc2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#\n",
        "# --- Imports and Setup ---\n",
        "#\n",
        "#\n",
        "\n",
        "import gradio as gr\n",
        "import sqlite3\n",
        "import fireducks.pandas as pd\n",
        "# import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "import duckdb\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse, urljoin, parse_qs, urlencode\n",
        "from collections import deque\n",
        "from abc import ABC, abstractmethod\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Google Colab Drive Mount ---\n",
        "#\n",
        "#\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Configuration ---\n",
        "#\n",
        "#\n",
        "\n",
        "@dataclass\n",
        "class CrawlerConfig:\n",
        "    state_db_path: str = \"/content/drive/My Drive/master_july_2025/data/crawler_state.db\"\n",
        "    parquet_path: str = \"/content/drive/My Drive/master_july_2025/data/crawled_data_parquet/\"\n",
        "    min_request_delay: float = 1.0\n",
        "    max_request_delay: float = 30.0\n",
        "    max_pages_to_crawl: int = 700\n",
        "    save_interval_pages: int = 10\n",
        "    max_retries_request: int = 3\n",
        "    max_redirects: int = 2\n",
        "    request_timeout: int = 15\n",
        "    allowed_path_segment: str = \"/blog/\"\n",
        "    initial_start_url: str = 'https://example.com/blog'\n",
        "    user_agents: list[str] = field(default_factory=lambda: [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n",
        "    ])\n",
        "    # --- NEW: Whitelist of query parameters to keep ---\n",
        "    allowed_query_params: list[str] = field(default_factory=lambda: ['page', 'p', 'q', 'id', 'post'])\n",
        "    base_domain: str = \"\"\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Logging and Core Interfaces (Unchanged) ---\n",
        "#\n",
        "#\n",
        "class ILogger(ABC):\n",
        "    @abstractmethod\n",
        "    def debug(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def info(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def warning(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def error(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def exception(self, message: str): pass\n",
        "\n",
        "class GradioLogHandler(logging.Handler):\n",
        "    def __init__(self, log_output_stream: io.StringIO):\n",
        "        super().__init__()\n",
        "        self.log_output_stream = log_output_stream\n",
        "        self.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "\n",
        "    def emit(self, record):\n",
        "        log_entry = self.format(record)\n",
        "        self.log_output_stream.write(log_entry + '\\n')\n",
        "        self.log_output_stream.flush()\n",
        "\n",
        "class ConsoleAndGradioLogger(ILogger):\n",
        "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
        "        self._logger = logging.getLogger(\"CrawlerLogger\")\n",
        "        self._logger.setLevel(level)\n",
        "        if self._logger.hasHandlers():\n",
        "            self._logger.handlers.clear()\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        self._logger.addHandler(console_handler)\n",
        "        gradio_handler = GradioLogHandler(log_output_stream)\n",
        "        self._logger.addHandler(gradio_handler)\n",
        "\n",
        "    def debug(self, message: str): self._logger.debug(message)\n",
        "    def info(self, message: str): self._logger.info(message)\n",
        "    def warning(self, message: str): self._logger.warning(message)\n",
        "    def error(self, message: str): self._logger.error(message)\n",
        "    def exception(self, message: str): self._logger.exception(message)\n",
        "\n",
        "class VisitedUrlManager:\n",
        "    def __init__(self): self.visited = set()\n",
        "    def add(self, url: str): self.visited.add(url)\n",
        "    def contains(self, url: str) -> bool: return url in self.visited\n",
        "    def size(self) -> int: return len(self.visited)\n",
        "\n",
        "class CrawlingStrategy(ABC):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        self.visited = visited_manager\n",
        "        self.logger = logger\n",
        "    @abstractmethod\n",
        "    def add_links(self, links_info: list[tuple[str, int]]): pass\n",
        "    @abstractmethod\n",
        "    def get_next(self) -> tuple[str, int]: pass\n",
        "    @abstractmethod\n",
        "    def has_next(self) -> bool: pass\n",
        "    @abstractmethod\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): pass\n",
        "\n",
        "class BFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        super().__init__(visited_manager, logger)\n",
        "        self.queue = deque()\n",
        "    def add_links(self, links_info: list[tuple[str, int]]):\n",
        "        new_links = [link_info for link_info in links_info if not self.visited.contains(link_info[0])]\n",
        "        for link_url, _ in new_links: self.visited.add(link_url)\n",
        "        self.queue.extend(new_links)\n",
        "    def get_next(self) -> tuple[str, int]: return self.queue.popleft()\n",
        "    def has_next(self) -> bool: return len(self.queue) > 0\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): self.queue.extend(frontier_urls_info)\n",
        "\n",
        "class DFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        super().__init__(visited_manager, logger)\n",
        "        self.stack = []\n",
        "    def add_links(self, links_info: list[tuple[str, int]]):\n",
        "        new_links = [link_info for link_info in links_info if not self.visited.contains(link_info[0])]\n",
        "        for link_url, _ in new_links: self.visited.add(link_url)\n",
        "        self.stack.extend(new_links)\n",
        "    def get_next(self) -> tuple[str, int]: return self.stack.pop()\n",
        "    def has_next(self) -> bool: return len(self.stack) > 0\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): self.stack.extend(frontier_urls_info)\n",
        "\n",
        "class StateManager:\n",
        "    def __init__(self, db_path: str, logger: ILogger):\n",
        "        self.db_path = db_path\n",
        "        self.logger = logger\n",
        "        db_dir = os.path.dirname(self.db_path)\n",
        "        if db_dir: os.makedirs(db_dir, exist_ok=True)\n",
        "        self.ensure_frontier_table_exists()\n",
        "\n",
        "    def _execute_query(self, query: str, params=None, fetch=False):\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(query, params or [])\n",
        "                if fetch: return cursor.fetchall()\n",
        "                conn.commit()\n",
        "        except sqlite3.Error as e:\n",
        "            self.logger.error(f\"StateManager DB error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def ensure_frontier_table_exists(self):\n",
        "        self._execute_query(\"CREATE TABLE IF NOT EXISTS crawl_frontier (URL TEXT UNIQUE, Redirects INTEGER)\")\n",
        "        self.logger.info(\"Crawl frontier table ensured to exist.\")\n",
        "\n",
        "    def save_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
        "        self._execute_query(\"DELETE FROM crawl_frontier\")\n",
        "        if not frontier_urls_info: return\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                conn.executemany(\"INSERT OR IGNORE INTO crawl_frontier (URL, Redirects) VALUES (?, ?)\", frontier_urls_info)\n",
        "                conn.commit()\n",
        "            self.logger.info(f\"Saved {len(frontier_urls_info)} URLs to frontier.\")\n",
        "        except sqlite3.Error as e:\n",
        "            self.logger.error(f\"Error saving frontier: {e}\")\n",
        "\n",
        "    def load_frontier(self) -> list[tuple[str, int]]:\n",
        "        result = self._execute_query(\"SELECT URL, Redirects FROM crawl_frontier\", fetch=True)\n",
        "        return result or []\n",
        "\n",
        "class HttpClient:\n",
        "    def __init__(self, config: CrawlerConfig, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.session = self._create_requests_session()\n",
        "        self.current_delay = self.config.min_request_delay\n",
        "\n",
        "    def _create_requests_session(self):\n",
        "        retry = Retry(total=self.config.max_retries_request, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        session = requests.Session()\n",
        "        session.mount(\"http://\", adapter)\n",
        "        session.mount(\"https://\", adapter)\n",
        "        return session\n",
        "\n",
        "    def fetch(self, url: str) -> tuple[int, str, str | None]:\n",
        "        headers = {'User-Agent': random.choice(self.config.user_agents)}\n",
        "        try:\n",
        "            time.sleep(self.current_delay)\n",
        "            response = self.session.get(url, headers=headers, timeout=self.config.request_timeout, allow_redirects=False)\n",
        "            if 300 <= response.status_code < 400:\n",
        "                redirect_url = response.headers.get('Location')\n",
        "                return response.status_code, \"\", urljoin(url, redirect_url)\n",
        "            return response.status_code, response.text, None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.logger.error(f\"Request error for {url}: {e}\")\n",
        "            return -2, f\"Request Error: {e}\", None\n",
        "\n",
        "class UrlFilter:\n",
        "    def __init__(self, allowed_path_segment: str, base_domain: str):\n",
        "        self.allowed_path_segment = allowed_path_segment\n",
        "        self.base_domain = base_domain\n",
        "        self.file_extension_pattern = re.compile(r'\\.(jpg|jpeg|png|gif|pdf|doc|xls|zip|rar|mp3|mp4)$', re.I)\n",
        "\n",
        "    def is_valid(self, url: str) -> bool:\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            return (parsed_url.scheme in ('http', 'https') and\n",
        "                    parsed_url.netloc == self.base_domain and\n",
        "                    self.allowed_path_segment in parsed_url.path and\n",
        "                    not self.file_extension_pattern.search(parsed_url.path))\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "#\n",
        "#\n",
        "# --- LinkExtractor (UPDATED with Normalization Logic) ---\n",
        "#\n",
        "#\n",
        "class LinkExtractor:\n",
        "    def __init__(self, url_filter: UrlFilter, allowed_params: list[str]):\n",
        "        self.url_filter = url_filter\n",
        "        self.allowed_params = allowed_params\n",
        "\n",
        "    def normalize_url(self, url: str) -> str:\n",
        "        \"\"\"Strips fragments and unwanted query parameters from a URL.\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "\n",
        "        # If there are query parameters, filter them\n",
        "        if parsed.query:\n",
        "            query_params = parse_qs(parsed.query)\n",
        "            # Keep only the parameters that are in our whitelist\n",
        "            filtered_params = {k: v for k, v in query_params.items() if k in self.allowed_params}\n",
        "            # Re-encode the query string with only the allowed parameters\n",
        "            clean_query = urlencode(filtered_params, doseq=True)\n",
        "        else:\n",
        "            clean_query = \"\"\n",
        "\n",
        "        # Reconstruct the URL with the clean query and no fragment\n",
        "        normalized = parsed._replace(query=clean_query, fragment=\"\").geturl()\n",
        "        return normalized\n",
        "\n",
        "    def extract_links(self, base_url: str, html_content: str) -> list[str]:\n",
        "        links = set()\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            absolute_link = urljoin(base_url, href)\n",
        "\n",
        "            # Normalize the link before doing anything else\n",
        "            normalized_link = self.normalize_url(absolute_link)\n",
        "\n",
        "            if self.url_filter.is_valid(normalized_link):\n",
        "                links.add(normalized_link)\n",
        "        return list(links)\n",
        "\n",
        "#\n",
        "#\n",
        "# --- WebCrawler and Main UI (Unchanged, but benefits from new LinkExtractor) ---\n",
        "#\n",
        "#\n",
        "class WebCrawler:\n",
        "    def __init__(self, config: CrawlerConfig, crawling_strategy: CrawlingStrategy, state_manager: StateManager,\n",
        "                 http_client: HttpClient, url_filter: UrlFilter, link_extractor: LinkExtractor, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.crawling_strategy = crawling_strategy\n",
        "        self.state_manager = state_manager\n",
        "        self.http_client = http_client\n",
        "        self.url_filter = url_filter\n",
        "        self.link_extractor = link_extractor\n",
        "        self.logger = logger\n",
        "        self.data_buffer = []\n",
        "        self.pages_crawled_session = 0\n",
        "\n",
        "    def _process_url(self, url_info: tuple[str, int]):\n",
        "        url, num_redirects = url_info\n",
        "        if num_redirects >= self.config.max_redirects:\n",
        "            self.data_buffer.append({'URL': url, 'Status_Code': 999, 'Content': \"Max redirects reached\"})\n",
        "            return\n",
        "\n",
        "        status, content, redirect_url = self.http_client.fetch(url)\n",
        "        self.logger.info(f\"Fetched {url} [{status}]\")\n",
        "        self.data_buffer.append({'URL': url, 'Status_Code': status, 'Content': content if 200 <= status < 300 else \"\"})\n",
        "\n",
        "        if 200 <= status < 300:\n",
        "            extracted = self.link_extractor.extract_links(url, content)\n",
        "            self.crawling_strategy.add_links([(link, 0) for link in extracted])\n",
        "            del content\n",
        "        elif redirect_url:\n",
        "            # Also normalize redirect URLs before validation\n",
        "            normalized_redirect = self.link_extractor.normalize_url(redirect_url)\n",
        "            if self.url_filter.is_valid(normalized_redirect):\n",
        "                self.crawling_strategy.add_links([(normalized_redirect, num_redirects + 1)])\n",
        "\n",
        "\n",
        "    def _save_buffer_to_parquet(self) -> str | None:\n",
        "        if not self.data_buffer: return None\n",
        "        num_records = len(self.data_buffer)\n",
        "        df = pd.DataFrame(self.data_buffer)\n",
        "        today = datetime.now().date()\n",
        "        df['crawl_date'] = today\n",
        "        try:\n",
        "            partition_path = os.path.join(self.config.parquet_path, f'crawl_date={today}')\n",
        "            df.to_parquet(path=self.config.parquet_path, engine='pyarrow', compression='snappy', partition_cols=['crawl_date'])\n",
        "            log_message = f\"✅ Saved a batch of **{num_records}** pages to partition `{partition_path}`\"\n",
        "            self.logger.info(log_message)\n",
        "            self.data_buffer = []\n",
        "            gc.collect()\n",
        "            return log_message\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to save to Parquet: {e}\")\n",
        "            return None\n",
        "\n",
        "    def crawl(self):\n",
        "        pbar = tqdm(total=self.config.max_pages_to_crawl, desc=\"Crawling Progress\")\n",
        "        while self.pages_crawled_session < self.config.max_pages_to_crawl:\n",
        "            if not self.crawling_strategy.has_next():\n",
        "                self.logger.info(\"Frontier is empty. Stopping crawl.\")\n",
        "                break\n",
        "\n",
        "            url_data = self.crawling_strategy.get_next()\n",
        "            self._process_url(url_data)\n",
        "            self.pages_crawled_session += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            save_event_msg = None\n",
        "            if len(self.data_buffer) >= self.config.save_interval_pages:\n",
        "                save_event_msg = self._save_buffer_to_parquet()\n",
        "                if hasattr(self.crawling_strategy, 'queue'):\n",
        "                    self.state_manager.save_frontier(list(self.crawling_strategy.queue))\n",
        "                elif hasattr(self.crawling_strategy, 'stack'):\n",
        "                    self.state_manager.save_frontier(list(self.crawling_strategy.stack))\n",
        "\n",
        "            yield {\n",
        "                \"status\": f\"Crawled {self.pages_crawled_session}/{self.config.max_pages_to_crawl} pages.\",\n",
        "                \"save_event\": save_event_msg\n",
        "            }\n",
        "\n",
        "        pbar.close()\n",
        "        self.logger.info(\"Crawl finished. Performing final save...\")\n",
        "        final_save_msg = self._save_buffer_to_parquet()\n",
        "        if hasattr(self.crawling_strategy, 'queue'):\n",
        "            self.state_manager.save_frontier(list(self.crawling_strategy.queue))\n",
        "        elif hasattr(self.crawling_strategy, 'stack'):\n",
        "            self.state_manager.save_frontier(list(self.crawling_strategy.stack))\n",
        "\n",
        "        yield {\n",
        "            \"status\": f\"Crawl finished. Processed {self.pages_crawled_session} pages.\",\n",
        "            \"save_event\": final_save_msg\n",
        "        }\n",
        "\n",
        "\n",
        "def run_gradio_crawler_interface(initial_start_url: str, allowed_path_segment: str, crawling_strategy_type: str,\n",
        "                                 state_db_path_input: str, parquet_path_input: str, max_pages_to_crawl: int):\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(log_stream)\n",
        "\n",
        "    try:\n",
        "        base_domain = urlparse(initial_start_url).netloc\n",
        "        if not base_domain: raise ValueError(\"Invalid Initial Start URL.\")\n",
        "\n",
        "        config = CrawlerConfig(\n",
        "            initial_start_url=initial_start_url, allowed_path_segment=allowed_path_segment,\n",
        "            state_db_path=state_db_path_input, parquet_path=parquet_path_input,\n",
        "            max_pages_to_crawl=max_pages_to_crawl, base_domain=base_domain\n",
        "        )\n",
        "        os.makedirs(config.parquet_path, exist_ok=True)\n",
        "        yield \"Initializing...\", log_stream.getvalue(), \"### Save Events Log\\n\\n- Waiting for first save event...\", \"\"\n",
        "\n",
        "        state_manager = StateManager(config.state_db_path, logger)\n",
        "        visited_manager = VisitedUrlManager()\n",
        "\n",
        "        logger.info(\"Rebuilding visited set from existing Parquet data...\")\n",
        "        try:\n",
        "            parquet_glob_path = os.path.join(config.parquet_path, '**', '*.parquet')\n",
        "            visited_urls_df = duckdb.query(f\"SELECT DISTINCT URL FROM read_parquet('{parquet_glob_path}')\").to_df()\n",
        "            for url in visited_urls_df['URL']:\n",
        "                visited_manager.add(url)\n",
        "            logger.info(f\"Rebuilt visited set with {visited_manager.size()} URLs.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not rebuild visited set from Parquet (may be a new crawl): {e}\")\n",
        "\n",
        "        strategy_class = BFSCrawlingStrategy if crawling_strategy_type == 'BFS' else DFSCrawlingStrategy\n",
        "        crawling_strategy = strategy_class(visited_manager, logger)\n",
        "\n",
        "        loaded_frontier = state_manager.load_frontier()\n",
        "        unvisited_frontier = [info for info in loaded_frontier if not visited_manager.contains(info[0])]\n",
        "\n",
        "        if unvisited_frontier:\n",
        "            crawling_strategy.prime_with_frontier(unvisited_frontier)\n",
        "        elif not visited_manager.contains(config.initial_start_url):\n",
        "             crawling_strategy.add_links([(config.initial_start_url, 0)])\n",
        "\n",
        "        http_client = HttpClient(config, logger)\n",
        "        url_filter = UrlFilter(config.allowed_path_segment, config.base_domain)\n",
        "        # --- Pass the whitelist to the LinkExtractor ---\n",
        "        link_extractor = LinkExtractor(url_filter, config.allowed_query_params)\n",
        "\n",
        "        crawler = WebCrawler(config, crawling_strategy, state_manager, http_client, url_filter, link_extractor, logger)\n",
        "\n",
        "        final_status = \"\"\n",
        "        save_events_log = [\"### Save Events Log\"]\n",
        "\n",
        "        for event in crawler.crawl():\n",
        "            status_msg = event.get(\"status\")\n",
        "            save_event = event.get(\"save_event\")\n",
        "\n",
        "            final_status = status_msg\n",
        "            if save_event:\n",
        "                save_events_log.append(f\"- {save_event}\")\n",
        "            yield status_msg, log_stream.getvalue(), \"\\n\".join(save_events_log), \"\"\n",
        "\n",
        "        logger.info(\"Generating final summary from Parquet data...\")\n",
        "        final_save_events = \"\\n\".join(save_events_log)\n",
        "        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Crawled Data Location**: `{config.parquet_path}`\"\n",
        "        try:\n",
        "            parquet_glob_path = os.path.join(config.parquet_path, '**', '*.parquet')\n",
        "            summary_df = duckdb.query(f\"SELECT CASE WHEN Status_Code >= 200 AND Status_Code < 300 THEN 'Success (Content Saved)' WHEN Status_Code >= 300 AND Status_Code < 400 THEN 'Redirect' ELSE 'Error / Other' END AS Category, COUNT(*) as Total FROM read_parquet('{parquet_glob_path}') GROUP BY Category ORDER BY Total DESC\").to_df()\n",
        "            total_urls = summary_df['Total'].sum()\n",
        "            summary_md += f\"\\n- **Total URLs in Parquet Dataset**: {total_urls}\\n\\n### Crawl Summary by Category\\n\\n\"\n",
        "            summary_md += summary_df.to_markdown(index=False)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Could not generate summary from Parquet: {e}\")\n",
        "            summary_md += \"\\n\\n**Could not generate summary from Parquet data.**\"\n",
        "\n",
        "        yield final_status, log_stream.getvalue(), final_save_events, summary_md\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"A critical error occurred: {e}\")\n",
        "        yield \"Crawl Failed!\", log_stream.getvalue(), \"\", f\"**Error:** {e}\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🕸️ Memory-Optimized Parquet Web Crawler\")\n",
        "    gr.Markdown(\"This crawler saves data to a partitioned Parquet dataset and uses SQLite only to manage the crawl state.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Crawler Configuration\")\n",
        "            initial_url_input = gr.Textbox(label=\"Initial Start URL\", value=CrawlerConfig.initial_start_url)\n",
        "            allowed_path_input = gr.Textbox(label=\"Allowed Path Segment\", value=CrawlerConfig.allowed_path_segment)\n",
        "            crawling_strategy_radio = gr.Radio(choices=['BFS', 'DFS'], label=\"Crawling Strategy\", value='BFS')\n",
        "            max_pages_input = gr.Number(label=\"Maximum Pages to Crawl (per session)\", value=CrawlerConfig.max_pages_to_crawl, minimum=1, step=100)\n",
        "            gr.Markdown(\"### Storage Paths\")\n",
        "            state_db_path_input = gr.Textbox(label=\"Crawl State DB Path (SQLite)\", value=CrawlerConfig.state_db_path)\n",
        "            parquet_path_input = gr.Textbox(label=\"Crawled Data Path (Parquet)\", value=CrawlerConfig.parquet_path)\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## Actions and Status\")\n",
        "            start_button = gr.Button(\"🚀 Start Crawl\", variant=\"primary\")\n",
        "            status_message_output = gr.Textbox(label=\"Status Message\", interactive=False)\n",
        "            logs_output = gr.Textbox(label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20)\n",
        "            with gr.Row():\n",
        "                save_events_output = gr.Markdown(\"### Save Events Log\")\n",
        "                summary_output = gr.Markdown(\"---\")\n",
        "\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_gradio_crawler_interface,\n",
        "        inputs=[initial_url_input, allowed_path_input, crawling_strategy_radio, state_db_path_input, parquet_path_input, max_pages_input],\n",
        "        outputs=[status_message_output, logs_output, save_events_output, summary_output]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "h7FCmsKPKA__"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}