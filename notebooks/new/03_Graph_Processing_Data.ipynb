{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0RiV0JnAG3w"
      },
      "outputs": [],
      "source": [
        "# Installing necessary packages\n",
        "!pip install gradio pandas requests tqdm beautifulsoup4 lxml -q\n",
        "!pip install fireducks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt_ov92UCtrz"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "# --- Imports and Setup ---\n",
        "#\n",
        "#\n",
        "\n",
        "import gradio as gr\n",
        "import sqlite3\n",
        "# import pandas as pd\n",
        "import fireducks.pandas as pd\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from collections import deque\n",
        "from abc import ABC, abstractmethod\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Google Colab Drive Mount ---\n",
        "#\n",
        "#\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Configuration ---\n",
        "#\n",
        "#\n",
        "\n",
        "@dataclass\n",
        "class CrawlerConfig:\n",
        "    state_db_path: str = \"/content/drive/My Drive/master_july_2025/data/link_crawler_state.db\"\n",
        "    edge_list_path: str = \"/content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\"\n",
        "    min_request_delay: float = 1.0\n",
        "    max_pages_to_crawl: int = 1000\n",
        "    save_interval_edges: int = 250\n",
        "    max_retries_request: int = 3\n",
        "    max_redirects: int = 2\n",
        "    request_timeout: int = 15\n",
        "    initial_start_url: str = 'https://kalicube.com/'\n",
        "    # The \"playground\" for the crawler. Set to '/' to explore the whole site.\n",
        "    crawling_scope_path: str = \"/\"\n",
        "    # The rule for what gets saved. Only edges within this path will be recorded.\n",
        "    saving_scope_path: str = \"/blog/\"\n",
        "    user_agents: list[str] = field(default_factory=lambda: [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    ])\n",
        "    base_domain: str = \"\"\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Logging and Core Interfaces ---\n",
        "#\n",
        "#\n",
        "class ILogger(ABC):\n",
        "    @abstractmethod\n",
        "    def info(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def error(self, message: str): pass\n",
        "    @abstractmethod\n",
        "    def exception(self, message: str): pass\n",
        "\n",
        "class GradioLogHandler(logging.Handler):\n",
        "    def __init__(self, log_output_stream: io.StringIO):\n",
        "        super().__init__()\n",
        "        self.log_output_stream = log_output_stream\n",
        "        self.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "    def emit(self, record):\n",
        "        log_entry = self.format(record)\n",
        "        self.log_output_stream.write(log_entry + '\\n')\n",
        "        self.log_output_stream.flush()\n",
        "\n",
        "class ConsoleAndGradioLogger(ILogger):\n",
        "    def __init__(self, log_output_stream: io.StringIO, level=logging.INFO):\n",
        "        self._logger = logging.getLogger(\"EdgeCrawlerLogger\")\n",
        "        self._logger.setLevel(level)\n",
        "        if self._logger.hasHandlers():\n",
        "            self._logger.handlers.clear()\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        self._logger.addHandler(console_handler)\n",
        "        gradio_handler = GradioLogHandler(log_output_stream)\n",
        "        self._logger.addHandler(gradio_handler)\n",
        "\n",
        "    def info(self, message: str): self._logger.info(message)\n",
        "    def error(self, message: str): self._logger.error(message)\n",
        "    def exception(self, message: str): self._logger.exception(message)\n",
        "\n",
        "class VisitedUrlManager:\n",
        "    def __init__(self): self.visited = set()\n",
        "    def add(self, url: str): self.visited.add(url)\n",
        "    def contains(self, url: str) -> bool: return url in self.visited\n",
        "    def size(self) -> int: return len(self.visited)\n",
        "\n",
        "class CrawlingStrategy(ABC):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        self.visited = visited_manager\n",
        "        self.logger = logger\n",
        "    def add_links(self, links_info: list[tuple[str, int]]):\n",
        "        new_links = [link_info for link_info in links_info if not self.visited.contains(link_info[0])]\n",
        "        for link_url, _ in new_links: self.visited.add(link_url)\n",
        "        self._add_to_collection(new_links)\n",
        "    def _add_to_collection(self, links): raise NotImplementedError\n",
        "    def get_next(self) -> tuple[str, int]: raise NotImplementedError\n",
        "    def has_next(self) -> bool: raise NotImplementedError\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): raise NotImplementedError\n",
        "    def get_queue(self) -> list: raise NotImplementedError\n",
        "\n",
        "class BFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        super().__init__(visited_manager, logger)\n",
        "        self.queue = deque()\n",
        "    def _add_to_collection(self, links): self.queue.extend(links)\n",
        "    def get_next(self) -> tuple[str, int]: return self.queue.popleft()\n",
        "    def has_next(self) -> bool: return len(self.queue) > 0\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): self.queue.extend(frontier_urls_info)\n",
        "    def get_queue(self) -> list: return list(self.queue)\n",
        "\n",
        "class DFSCrawlingStrategy(CrawlingStrategy):\n",
        "    def __init__(self, visited_manager: VisitedUrlManager, logger: ILogger):\n",
        "        super().__init__(visited_manager, logger)\n",
        "        self.stack = []\n",
        "    def _add_to_collection(self, links): self.stack.extend(links)\n",
        "    def get_next(self) -> tuple[str, int]: return self.stack.pop()\n",
        "    def has_next(self) -> bool: return len(self.stack) > 0\n",
        "    def prime_with_frontier(self, frontier_urls_info: list[tuple[str, int]]): self.stack.extend(frontier_urls_info)\n",
        "    def get_queue(self) -> list: return list(self.stack)\n",
        "\n",
        "class StateManager:\n",
        "    def __init__(self, db_path: str, logger: ILogger):\n",
        "        self.db_path = db_path\n",
        "        self.logger = logger\n",
        "        db_dir = os.path.dirname(self.db_path)\n",
        "        if db_dir: os.makedirs(db_dir, exist_ok=True)\n",
        "        self._execute_query(\"CREATE TABLE IF NOT EXISTS crawl_frontier (URL TEXT UNIQUE, Redirects INTEGER)\")\n",
        "\n",
        "    def _execute_query(self, query: str, params=None, fetch=False):\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(query, params or [])\n",
        "                if fetch: return cursor.fetchall()\n",
        "                conn.commit()\n",
        "        except sqlite3.Error as e:\n",
        "            self.logger.error(f\"StateManager DB error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_frontier(self, frontier_urls_info: list[tuple[str, int]]):\n",
        "        self._execute_query(\"DELETE FROM crawl_frontier\")\n",
        "        if not frontier_urls_info: return\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                conn.executemany(\"INSERT OR IGNORE INTO crawl_frontier (URL, Redirects) VALUES (?, ?)\", frontier_urls_info)\n",
        "        except sqlite3.Error as e:\n",
        "            self.logger.error(f\"Error saving frontier: {e}\")\n",
        "\n",
        "    def load_frontier(self) -> list[tuple[str, int]]:\n",
        "        return self._execute_query(\"SELECT URL, Redirects FROM crawl_frontier\", fetch=True) or []\n",
        "\n",
        "class HttpClient:\n",
        "    def __init__(self, config: CrawlerConfig, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.session = self._create_requests_session()\n",
        "\n",
        "    def _create_requests_session(self):\n",
        "        retry = Retry(total=self.config.max_retries_request, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        session = requests.Session()\n",
        "        session.mount(\"http://\", adapter)\n",
        "        session.mount(\"https://\", adapter)\n",
        "        return session\n",
        "\n",
        "    def fetch(self, url: str) -> tuple[int, str, str | None]:\n",
        "        headers = {'User-Agent': random.choice(self.config.user_agents)}\n",
        "        try:\n",
        "            time.sleep(self.config.min_request_delay)\n",
        "            response = self.session.get(url, headers=headers, timeout=self.config.request_timeout, allow_redirects=False)\n",
        "            if 300 <= response.status_code < 400:\n",
        "                return response.status_code, \"\", urljoin(url, response.headers.get('Location'))\n",
        "            return response.status_code, response.text, None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.logger.error(f\"Request error for {url}: {e}\")\n",
        "            return -2, f\"Request Error: {e}\", None\n",
        "\n",
        "class UrlFilter:\n",
        "    def __init__(self, crawling_scope_path: str, base_domain: str):\n",
        "        self.crawling_scope_path = crawling_scope_path\n",
        "        self.base_domain = base_domain\n",
        "        self.file_extension_pattern = re.compile(\n",
        "            r'\\.(pdf|jpg|jpeg|png|gif|zip|rar|mp3|mp4|svg|xml|css|js|webp|ico)$',\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "\n",
        "    def is_valid_for_crawling(self, url: str) -> bool:\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            return (parsed_url.scheme in ('http', 'https') and\n",
        "                    parsed_url.netloc == self.base_domain and\n",
        "                    parsed_url.path.startswith(self.crawling_scope_path) and\n",
        "                    not self.file_extension_pattern.search(parsed_url.path))\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "class LinkExtractor:\n",
        "    def __init__(self, url_filter: UrlFilter):\n",
        "        self.url_filter = url_filter\n",
        "\n",
        "    def normalize_url(self, url: str) -> str:\n",
        "        \"\"\"Strips ALL query parameters and fragments from a URL.\"\"\"\n",
        "        return urlparse(url)._replace(query=\"\", fragment=\"\").geturl()\n",
        "\n",
        "    def extract_links(self, base_url: str, html_content: str) -> set[str]:\n",
        "        links = set()\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            absolute_link = urljoin(base_url, a_tag['href'])\n",
        "            normalized_link = self.normalize_url(absolute_link)\n",
        "            if self.url_filter.is_valid_for_crawling(normalized_link):\n",
        "                links.add(normalized_link)\n",
        "        return links\n",
        "\n",
        "class EdgeCrawler:\n",
        "    def __init__(self, config: CrawlerConfig, crawling_strategy: CrawlingStrategy, state_manager: StateManager,\n",
        "                 http_client: HttpClient, link_extractor: LinkExtractor, logger: ILogger):\n",
        "        self.config = config\n",
        "        self.crawling_strategy = crawling_strategy\n",
        "        self.state_manager = state_manager\n",
        "        self.http_client = http_client\n",
        "        self.link_extractor = link_extractor\n",
        "        self.logger = logger\n",
        "        self.edge_buffer = []\n",
        "        self.pages_processed_session = 0\n",
        "\n",
        "    def _process_page_for_edges(self, from_url: str, num_redirects: int):\n",
        "        if num_redirects >= self.config.max_redirects:\n",
        "            self.logger.warning(f\"Max redirects for {from_url}. Skipping.\")\n",
        "            return\n",
        "\n",
        "        status, content, redirect_url = self.http_client.fetch(from_url)\n",
        "        self.logger.info(f\"Processed {from_url} [{status}]\")\n",
        "\n",
        "        if 200 <= status < 300 and content:\n",
        "            linked_urls = self.link_extractor.extract_links(from_url, content)\n",
        "\n",
        "            from_url_path = urlparse(from_url).path\n",
        "            saving_scope = self.config.saving_scope_path\n",
        "\n",
        "            if from_url_path.startswith(saving_scope):\n",
        "                for to_url in linked_urls:\n",
        "                    to_url_path = urlparse(to_url).path\n",
        "                    if to_url_path.startswith(saving_scope):\n",
        "                        self.edge_buffer.append({'FROM': from_url, 'TO': to_url})\n",
        "\n",
        "            self.crawling_strategy.add_links([(link, 0) for link in linked_urls])\n",
        "            del content\n",
        "\n",
        "        elif redirect_url:\n",
        "            normalized_redirect = self.link_extractor.normalize_url(redirect_url)\n",
        "            if self.link_extractor.url_filter.is_valid_for_crawling(normalized_redirect):\n",
        "                self.crawling_strategy.add_links([(normalized_redirect, num_redirects + 1)])\n",
        "\n",
        "    def _save_edges_to_csv(self):\n",
        "        if not self.edge_buffer: return\n",
        "\n",
        "        output_path = self.config.edge_list_path\n",
        "        write_header = not os.path.exists(output_path)\n",
        "\n",
        "        df = pd.DataFrame(self.edge_buffer)\n",
        "        df.to_csv(output_path, mode='a', header=write_header, index=False)\n",
        "\n",
        "        self.logger.info(f\"✅ Saved a batch of {len(self.edge_buffer)} edges to {output_path}\")\n",
        "        self.edge_buffer = []\n",
        "        gc.collect()\n",
        "\n",
        "    def crawl(self):\n",
        "        pbar = tqdm(total=self.config.max_pages_to_crawl, desc=\"Processing Pages\")\n",
        "        while self.pages_processed_session < self.config.max_pages_to_crawl:\n",
        "            if not self.crawling_strategy.has_next():\n",
        "                self.logger.info(\"Frontier is empty. Stopping crawl.\")\n",
        "                break\n",
        "\n",
        "            url_data = self.crawling_strategy.get_next()\n",
        "            self._process_page_for_edges(url_data[0], url_data[1])\n",
        "            self.pages_processed_session += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if len(self.edge_buffer) >= self.config.save_interval_edges:\n",
        "                self._save_edges_to_csv()\n",
        "                self.state_manager.save_frontier(self.crawling_strategy.get_queue())\n",
        "\n",
        "            yield f\"Processed {self.pages_processed_session}/{self.config.max_pages_to_crawl} pages.\"\n",
        "\n",
        "        pbar.close()\n",
        "        self.logger.info(\"Crawl finished. Performing final save...\")\n",
        "        self._save_edges_to_csv()\n",
        "        self.state_manager.save_frontier(self.crawling_strategy.get_queue())\n",
        "        yield f\"Crawl finished. Processed {self.pages_processed_session} pages in this session.\"\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Main Application Logic ---\n",
        "#\n",
        "#\n",
        "\n",
        "def run_edge_crawler_interface(initial_start_url: str, crawling_scope_path: str, saving_scope_path: str,\n",
        "                               crawling_strategy_type: str, state_db_path_input: str,\n",
        "                               edge_list_path_input: str, max_pages_to_crawl: int):\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(log_stream)\n",
        "\n",
        "    try:\n",
        "        base_domain = urlparse(initial_start_url).netloc\n",
        "        if not base_domain: raise ValueError(\"Invalid Initial Start URL.\")\n",
        "\n",
        "        config = CrawlerConfig(\n",
        "            initial_start_url=initial_start_url,\n",
        "            crawling_scope_path=crawling_scope_path,\n",
        "            saving_scope_path=saving_scope_path,\n",
        "            state_db_path=state_db_path_input,\n",
        "            edge_list_path=edge_list_path_input,\n",
        "            max_pages_to_crawl=max_pages_to_crawl,\n",
        "            base_domain=base_domain\n",
        "        )\n",
        "        os.makedirs(os.path.dirname(config.edge_list_path), exist_ok=True)\n",
        "        yield \"Initializing...\", log_stream.getvalue(), \"\"\n",
        "\n",
        "        state_manager = StateManager(config.state_db_path, logger)\n",
        "        visited_manager = VisitedUrlManager()\n",
        "\n",
        "        logger.info(\"Rebuilding visited set from existing edge list CSV...\")\n",
        "        try:\n",
        "            if os.path.exists(config.edge_list_path):\n",
        "                edge_df = pd.read_csv(config.edge_list_path, low_memory=False)\n",
        "                all_urls_in_graph = set(pd.concat([edge_df['FROM'], edge_df['TO']]).unique())\n",
        "                for url in all_urls_in_graph:\n",
        "                    visited_manager.add(url)\n",
        "                logger.info(f\"Rebuilt visited set with {visited_manager.size()} URLs from CSV.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not rebuild visited set from CSV (may be a new crawl): {e}\")\n",
        "\n",
        "        if crawling_strategy_type == 'BFS':\n",
        "            crawling_strategy = BFSCrawlingStrategy(visited_manager, logger)\n",
        "        else:\n",
        "            crawling_strategy = DFSCrawlingStrategy(visited_manager, logger)\n",
        "\n",
        "        loaded_frontier = state_manager.load_frontier()\n",
        "        unvisited_frontier = [info for info in loaded_frontier if not visited_manager.contains(info[0])]\n",
        "\n",
        "        if unvisited_frontier:\n",
        "            crawling_strategy.prime_with_frontier(unvisited_frontier)\n",
        "        elif not visited_manager.contains(config.initial_start_url):\n",
        "             crawling_strategy.add_links([(config.initial_start_url, 0)])\n",
        "\n",
        "        url_filter = UrlFilter(config.crawling_scope_path, config.base_domain)\n",
        "        link_extractor = LinkExtractor(url_filter)\n",
        "        crawler = EdgeCrawler(config, crawling_strategy, state_manager, HttpClient(config, logger), link_extractor, logger)\n",
        "\n",
        "        final_status = \"\"\n",
        "        for status_msg in crawler.crawl():\n",
        "            final_status = status_msg\n",
        "            yield status_msg, log_stream.getvalue(), \"\"\n",
        "\n",
        "        logger.info(\"Generating final summary from CSV file...\")\n",
        "        summary_md = f\"## Crawl Session Finished\\n\\n- **Status**: {final_status}\\n- **Edge List Location**: `{config.edge_list_path}`\"\n",
        "        try:\n",
        "            if os.path.exists(config.edge_list_path):\n",
        "                edge_df = pd.read_csv(config.edge_list_path)\n",
        "                num_edges = len(edge_df)\n",
        "                num_nodes = len(pd.concat([edge_df['FROM'], edge_df['TO']]).unique())\n",
        "                summary_md += f\"\\n- **Total Unique Pages (Nodes):** {num_nodes}\\n- **Total Links (Edges):** {num_edges}\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Could not generate summary from CSV: {e}\")\n",
        "            summary_md += \"\\n\\n**Could not generate summary from CSV file.**\"\n",
        "\n",
        "        yield final_status, log_stream.getvalue(), summary_md\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"A critical error occurred: {e}\")\n",
        "        yield \"Crawl Failed!\", log_stream.getvalue(), f\"**Error:** {e}\"\n",
        "\n",
        "#\n",
        "#\n",
        "# --- Final Gradio UI ---\n",
        "#\n",
        "#\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🕸️ Link Graph Extractor\")\n",
        "    gr.Markdown(\"This tool crawls a website to produce a simple `FROM, TO` list of all hyperlinks, saved as a CSV file.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Configuration\")\n",
        "            initial_url_input = gr.Textbox(label=\"Initial Start URL\", value=CrawlerConfig.initial_start_url)\n",
        "            max_pages_input = gr.Number(label=\"Maximum Pages to Process\", value=CrawlerConfig.max_pages_to_crawl, minimum=1, step=100)\n",
        "            crawling_strategy_radio = gr.Radio(choices=['BFS', 'DFS'], label=\"Crawling Strategy\", value='BFS')\n",
        "\n",
        "            gr.Markdown(\"### 📜 Scopes\")\n",
        "            crawling_scope_path_input = gr.Textbox(label=\"Crawling Scope Path\", value=CrawlerConfig.crawling_scope_path, info=\"The 'playground'. Set to '/' to explore the entire site.\")\n",
        "            saving_scope_path_input = gr.Textbox(label=\"Saving Scope Path\", value=CrawlerConfig.saving_scope_path, info=\"The 'rulebook'. Only save links where FROM and TO are in this path.\")\n",
        "\n",
        "            gr.Markdown(\"### 💾 Storage Paths\")\n",
        "            state_db_path_input = gr.Textbox(label=\"Crawl State DB Path (SQLite)\", value=CrawlerConfig.state_db_path)\n",
        "            edge_list_path_input = gr.Textbox(label=\"Output Edge List Path (CSV)\", value=CrawlerConfig.edge_list_path)\n",
        "\n",
        "            start_button = gr.Button(\"🚀 Start Extraction\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## Status & Results\")\n",
        "            status_message_output = gr.Textbox(label=\"Status Message\", interactive=False)\n",
        "            logs_output = gr.Textbox(label=\"Crawler Logs\", interactive=False, lines=15, max_lines=20)\n",
        "            summary_output = gr.Markdown(\"---\")\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_edge_crawler_interface,\n",
        "        inputs=[\n",
        "            initial_url_input,\n",
        "            crawling_scope_path_input,\n",
        "            saving_scope_path_input,\n",
        "            crawling_strategy_radio,\n",
        "            state_db_path_input,\n",
        "            edge_list_path_input,\n",
        "            max_pages_input\n",
        "        ],\n",
        "        outputs=[status_message_output, logs_output, summary_output]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}