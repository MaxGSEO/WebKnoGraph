{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy2PaM1g18TD"
   },
   "outputs": [],
   "source": [
    "!pip install pandas networkx -q\n",
    "!pip install fireducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz6MSAvPDU-t"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import fireducks.pandas as pd\n",
    "import networkx as nx\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# --- Google Colab Specific: Import and Mount Google Drive ---\n",
    "# If you are running this code in Google Colab, uncomment the following lines\n",
    "# and run them first to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1. Single Responsibility Principle (SRP) for URL processing\n",
    "class URLProcessor:\n",
    "    \"\"\"\n",
    "    Handles URL-related operations, specifically calculating folder depth.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_folder_depth(url: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the folder depth of a given URL.\n",
    "        Example: https://kalicube.com/learning-spaces/faq-list/generative-ai/ -> 2\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        if not path or path == \"/\":\n",
    "            return 0\n",
    "        # Remove leading/trailing slashes and split by '/'\n",
    "        segments = [s for s in path.strip(\"/\").split(\"/\") if s]\n",
    "        return len(segments)\n",
    "\n",
    "\n",
    "# 2. Single Responsibility Principle (SRP) for Graph operations\n",
    "class GraphAnalyzer:\n",
    "    \"\"\"\n",
    "    Handles graph construction and PageRank calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, edges_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the GraphAnalyzer with a DataFrame of graph edges.\n",
    "        \"\"\"\n",
    "        if edges_df.shape[1] < 2:\n",
    "            raise ValueError(\n",
    "                \"Input DataFrame must have at least two columns for source and target URLs.\"\n",
    "            )\n",
    "        self.edges_df = edges_df\n",
    "        self.graph = self._build_graph()\n",
    "\n",
    "    def _build_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"\n",
    "        Builds a directed graph from the provided edges DataFrame.\n",
    "        \"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        # Get all unique URLs from both columns to ensure all nodes are added\n",
    "        all_urls = pd.concat(\n",
    "            [self.edges_df.iloc[:, 0], self.edges_df.iloc[:, 1]]\n",
    "        ).unique()\n",
    "        G.add_nodes_from(all_urls)\n",
    "\n",
    "        for _, row in self.edges_df.iterrows():\n",
    "            source = row.iloc[0]\n",
    "            target = row.iloc[1]\n",
    "            G.add_edge(source, target)\n",
    "        return G\n",
    "\n",
    "    def calculate_pagerank(self) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates PageRank for all nodes in the graph.\n",
    "        Returns a dictionary mapping URLs to their PageRank scores.\n",
    "        \"\"\"\n",
    "        return nx.pagerank(self.graph)\n",
    "\n",
    "    def get_all_nodes(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns a list of all unique nodes (URLs) in the graph.\n",
    "        \"\"\"\n",
    "        return list(self.graph.nodes())\n",
    "\n",
    "\n",
    "# 3. Orchestration and File I/O (SRP, Dependency Inversion Principle)\n",
    "class LinkGraphProcessor:\n",
    "    \"\"\"\n",
    "    Orchestrates the loading of data, analysis, and saving of results.\n",
    "    Depends on abstractions (URLProcessor, GraphAnalyzer) rather than\n",
    "    concrete implementations directly (DIP).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url_processor: URLProcessor, graph_analyzer_class):\n",
    "        \"\"\"\n",
    "        Initializes the processor with an instance of URLProcessor and\n",
    "        the class for GraphAnalyzer (for dependency injection).\n",
    "        \"\"\"\n",
    "        self.url_processor = url_processor\n",
    "        self.graph_analyzer_class = graph_analyzer_class\n",
    "\n",
    "    def process_graph_data(self, input_filepath: str, output_filepath: str):\n",
    "        \"\"\"\n",
    "        Loads graph data, performs analysis, and saves the results.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(input_filepath):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_filepath}\")\n",
    "\n",
    "        print(f\"Loading data from {input_filepath}...\")\n",
    "        df_edges = pd.read_csv(input_filepath)\n",
    "        print(\"Data loaded successfully.\")\n",
    "\n",
    "        graph_analyzer = self.graph_analyzer_class(df_edges)\n",
    "        all_urls = graph_analyzer.get_all_nodes()\n",
    "\n",
    "        print(\"Calculating folder depths...\")\n",
    "        folder_depths = {\n",
    "            url: self.url_processor.get_folder_depth(url) for url in all_urls\n",
    "        }\n",
    "        print(\"Folder depths calculated.\")\n",
    "\n",
    "        print(\"Calculating PageRank scores...\")\n",
    "        pagerank_scores = graph_analyzer.calculate_pagerank()\n",
    "        print(\"PageRank scores calculated.\")\n",
    "\n",
    "        print(\"Compiling results...\")\n",
    "        results_df = pd.DataFrame(\n",
    "            {\n",
    "                \"URL\": all_urls,\n",
    "                \"Folder_Depth\": [folder_depths[url] for url in all_urls],\n",
    "                \"PageRank\": [\n",
    "                    pagerank_scores.get(url, 0.0) for url in all_urls\n",
    "                ],  # Ensure float for PageRank\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results to {output_filepath}...\")\n",
    "        results_df.to_csv(output_filepath, index=False)\n",
    "        print(\"Results saved successfully.\")\n",
    "        print(f\"\\nExample of results (first 5 rows):\\n{results_df.head()}\")\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your input file path on Google Drive\n",
    "    # This path assumes your Google Drive is mounted at '/content/drive'\n",
    "    input_file = \"/content/drive/My Drive/master_july_2025/data/link_graph_edges.csv\"\n",
    "\n",
    "    # Dynamically construct the output file path to be in the same directory\n",
    "    output_directory = os.path.dirname(input_file)\n",
    "    output_file = os.path.join(\n",
    "        output_directory, \"url_analysis_results.csv\"\n",
    "    )  # Changed filename as requested\n",
    "\n",
    "    # Instantiate the processor with its dependencies\n",
    "    processor = LinkGraphProcessor(\n",
    "        url_processor=URLProcessor(), graph_analyzer_class=GraphAnalyzer\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        processor.process_graph_data(input_file, output_file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\n",
    "            \"Please ensure your Google Drive is mounted correctly and the input file path is accurate.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}